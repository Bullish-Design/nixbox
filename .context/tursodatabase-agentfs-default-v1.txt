Repo: https://github.com/tursodatabase/agentfs
Ref: (default)
Generated: 2026-02-13T14:01:38.075Z

Directory Structure:

./
├── examples
│   ├── ai-sdk-just-bash
│   │   ├── agent.ts
│   │   ├── main.ts
│   │   ├── package-lock.json
│   │   ├── package.json
│   │   ├── README.md
│   │   ├── shell.ts
│   │   └── tsconfig.json
│   ├── claude-agent
│   │   └── research-assistant
│   │       ├── src
│   │       │   ├── tools
│   │       │   │   ├── pdf-tool.ts
│   │       │   │   └── semantic-tool.ts
│   │       │   ├── utils
│   │       │   │   ├── agentfs.ts
│   │       │   │   └── pdf.ts
│   │       │   ├── agent.ts
│   │       │   └── cli.ts
│   │       ├── .env.example
│   │       ├── package-lock.json
│   │       ├── package.json
│   │       ├── README.md
│   │       └── tsconfig.json
│   ├── mastra
│   │   └── research-assistant
│   │       ├── cli
│   │       │   └── ask.ts
│   │       ├── src
│   │       │   └── mastra
│   │       │       ├── agents
│   │       │       │   └── research-agent.ts
│   │       │       ├── scorers
│   │       │       │   └── lexical-scorer.ts
│   │       │       ├── tools
│   │       │       │   ├── pdf-tool.ts
│   │       │       │   └── semantic-tool.ts
│   │       │       ├── utils
│   │       │       │   ├── agentfs.ts
│   │       │       │   └── pdf.ts
│   │       │       ├── workflows
│   │       │       │   └── research-workflow.ts
│   │       │       └── index.ts
│   │       ├── package.json
│   │       ├── README.md
│   │       └── tsconfig.json
│   └── openai-agents
│       └── research-assistant
│           ├── src
│           │   ├── tools
│           │   │   ├── pdf-tool.ts
│           │   │   └── semantic-tool.ts
│           │   ├── utils
│           │   │   ├── agentfs.ts
│           │   │   └── pdf.ts
│           │   ├── agent.ts
│           │   └── cli.ts
│           ├── .env.example
│           ├── agentfs.db-wal
│           ├── package-lock.json
│           ├── package.json
│           ├── README.md
│           └── tsconfig.json
├── sdk
│   └── python
│       ├── agentfs_sdk
│       │   ├── __init__.py
│       │   ├── agentfs.py
│       │   ├── constants.py
│       │   ├── errors.py
│       │   ├── filesystem.py
│       │   ├── guards.py
│       │   ├── kvstore.py
│       │   └── toolcalls.py
│       ├── examples
│       │   ├── filesystem_demo.py
│       │   ├── kvstore_demo.py
│       │   └── toolcalls_demo.py
│       ├── tests
│       │   ├── __init__.py
│       │   ├── test_agentfs.py
│       │   ├── test_basic.py
│       │   ├── test_filesystem.py
│       │   ├── test_kvstore.py
│       │   └── test_toolcalls.py
│       ├── pyproject.toml
│       └── README.md
├── MANUAL.md
├── README.md
└── SPEC.md




------------------------------------------
File: examples/ai-sdk-just-bash/agent.ts
------------------------------------------

/**
 * AI agent with persistent AgentFS storage
 *
 * This file contains only the agent logic - see shell.ts for the interactive loop.
 * Uses AgentFs to provide persistent filesystem storage backed by SQLite.
 *
 * The agent starts with the agentfs source code pre-loaded, so you can
 * explore the agentfs codebase using agentfs itself!
 */

import * as nodeFs from "node:fs";
import * as path from "node:path";
import { glob } from "glob";
import { anthropic } from "@ai-sdk/anthropic";
import { streamText, stepCountIs } from "ai";
import { createBashTool } from "bash-tool";
import { Bash } from "just-bash";
import { agentfs } from "agentfs-sdk/just-bash";

export interface AgentRunner {
  chat(
    message: string,
    callbacks: {
      onText: (text: string) => void;
    }
  ): Promise<void>;
}

export interface CreateAgentOptions {
  onToolCall?: (command: string) => void;
  onText?: (text: string) => void;
}

/**
 * Creates an agent runner with persistent filesystem storage
 *
 * Uses AgentFs backed by SQLite - files persist across sessions.
 */
export async function createAgent(
  options: CreateAgentOptions = {}
): Promise<AgentRunner> {
  // Open AgentFS for persistent storage
  const fs = await agentfs({ id: "just-bash-agent" });

  // Seed agentfs source files on first run
  const agentfsRoot = path.resolve(import.meta.dirname, "../..");
  if (!(await fs.exists("/README.md"))) {
    console.log("Seeding AgentFS with agentfs source files...");

    // Find all source files
    const patterns = [
      "**/*.ts",
      "**/*.rs",
      "**/*.toml",
      "**/*.json",
      "**/*.md",
    ];
    const ignorePatterns = [
      "**/node_modules/**",
      "**/dist/**",
      "**/target/**",
      "**/.git/**",
      "**/examples/just_bash/**",
    ];

    // Collect all files first
    const allFiles: string[] = [];
    for (const pattern of patterns) {
      const files = await glob(pattern, {
        cwd: agentfsRoot,
        ignore: ignorePatterns,
        nodir: true,
      });
      allFiles.push(...files);
    }

    // Copy files with progress
    let count = 0;
    const total = allFiles.length;
    for (const file of allFiles) {
      const srcPath = path.join(agentfsRoot, file);
      const destPath = "/" + file;

      // Create parent directories
      const dir = path.dirname(destPath);
      if (dir !== "/") {
        await fs.mkdir(dir, { recursive: true }).catch(() => {});
      }

      // Copy file
      const content = nodeFs.readFileSync(srcPath, "utf-8");
      await fs.writeFile(destPath, content);

      count++;
      if (count % 10 === 0 || count === total) {
        process.stdout.write(`\rSeeding: ${count}/${total} files...`);
      }
    }
    console.log("\nSeeded AgentFS with agentfs source files.");
  }

  // Create a just-bash Bash instance with AgentFS filesystem
  const bash = new Bash({ fs });

  // Create the bash toolkit with the just-bash sandbox
  const bashToolkit = await createBashTool({
    sandbox: bash,
    extraInstructions: `You are exploring the AgentFS codebase - a persistent filesystem for AI agents.
The filesystem is backed by AgentFS itself (SQLite), so files persist across sessions.

Use bash commands to explore:
- ls to see the project structure
- cat README.md to read documentation
- grep -r "pattern" . to search code
- find . -name "*.ts" to find TypeScript files

Key directories:
- /sdk/typescript/src - TypeScript SDK source
- /cli/src - CLI source (Rust)
- /integrations - Framework integrations`,
    onBeforeBashCall: options.onToolCall
      ? ({ command }) => {
          options.onToolCall!(command);
          return { command };
        }
      : undefined,
  });

  const history: Array<{ role: "user" | "assistant"; content: string }> = [];

  return {
    async chat(message, callbacks) {
      history.push({ role: "user", content: message });

      let fullText = "";

      const result = streamText({
        model: anthropic("claude-sonnet-4-20250514"),
        tools: bashToolkit.tools,
        stopWhen: stepCountIs(50),
        messages: history,
      });

      for await (const chunk of result.textStream) {
        options.onText?.(chunk);
        callbacks.onText(chunk);
        fullText += chunk;
      }

      history.push({ role: "assistant", content: fullText });
    },
  };
}




------------------------------------------
File: examples/ai-sdk-just-bash/main.ts
------------------------------------------

#!/usr/bin/env npx tsx
/**
 * just-bash Agent with AgentFS
 *
 * Usage: npx tsx main.ts
 *
 * Requires ANTHROPIC_API_KEY environment variable.
 */

import { createAgent } from "./agent.js";
import { runShell } from "./shell.js";

let lastWasToolCall = false;

const agent = await createAgent({
  onToolCall: (command) => {
    const prefix = lastWasToolCall ? "" : "\n";
    console.log(
      `${prefix}\x1b[34m\x1b[1mExecuting bash tool:\x1b[0m \x1b[36m${command.trim()}\x1b[0m`
    );
    lastWasToolCall = true;
  },
  onText: () => {
    lastWasToolCall = false;
  },
});
runShell(agent);




------------------------------------------
File: examples/ai-sdk-just-bash/package-lock.json
------------------------------------------

{
  "name": "just-bash-agentfs-example",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "just-bash-agentfs-example",
      "version": "1.0.0",
      "dependencies": {
        "@ai-sdk/anthropic": "^3.0.0",
        "agentfs-sdk": "file:../../sdk/typescript",
        "ai": "^6.0.0",
        "bash-tool": "^1.0.0",
        "glob": "^11.0.0",
        "just-bash": "^2.0.0"
      },
      "devDependencies": {
        "@types/node": "^22.0.0",
        "typescript": "^5.0.0"
      }
    },
    "../../sdk/typescript": {
      "name": "agentfs-sdk",
      "version": "0.5.0-pre.1",
      "license": "MIT",
      "dependencies": {
        "@tursodatabase/database": "^0.4.0-pre.18",
        "@tursodatabase/database-common": "^0.4.0-pre.18",
        "buffer": "^6.0.3"
      },
      "devDependencies": {
        "@tursodatabase/database-wasm": "^0.4.0-pre.18",
        "@types/node": "^20.0.0",
        "@vitest/browser": "^4.0.16",
        "@vitest/browser-playwright": "^4.0.16",
        "@vitest/ui": "^4.0.1",
        "just-bash": "^2.0.0",
        "typescript": "^5.3.0",
        "vitest": "^4.0.1"
      },
      "peerDependencies": {
        "just-bash": ">=2.0.0"
      },
      "peerDependenciesMeta": {
        "just-bash": {
          "optional": true
        }
      }
    },
    "node_modules/@ai-sdk/anthropic": {
      "version": "3.0.8",
      "resolved": "https://registry.npmjs.org/@ai-sdk/anthropic/-/anthropic-3.0.8.tgz",
      "integrity": "sha512-x5G07iwQB/Sh+1dFLWyZgdOVRnh9X/o8YaPCr+Ju3ie8Amp1mvHLz+c2i3UKe6/8leIwr/TXjrwyJRJk9mkq2g==",
      "license": "Apache-2.0",
      "dependencies": {
        "@ai-sdk/provider": "3.0.2",
        "@ai-sdk/provider-utils": "4.0.4"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "zod": "^3.25.76 || ^4.1.8"
      }
    },
    "node_modules/@ai-sdk/gateway": {
      "version": "3.0.9",
      "resolved": "https://registry.npmjs.org/@ai-sdk/gateway/-/gateway-3.0.9.tgz",
      "integrity": "sha512-EA5dZIukimwoJ9HIPuuREotAqaTItpdc/yImzVF0XGNg7B0YRJmYI8Uq3aCMr87vjr1YB1cWUfnrTt6OJ9eHiQ==",
      "license": "Apache-2.0",
      "dependencies": {
        "@ai-sdk/provider": "3.0.2",
        "@ai-sdk/provider-utils": "4.0.4",
        "@vercel/oidc": "3.1.0"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "zod": "^3.25.76 || ^4.1.8"
      }
    },
    "node_modules/@ai-sdk/provider": {
      "version": "3.0.2",
      "resolved": "https://registry.npmjs.org/@ai-sdk/provider/-/provider-3.0.2.tgz",
      "integrity": "sha512-HrEmNt/BH/hkQ7zpi2o6N3k1ZR1QTb7z85WYhYygiTxOQuaml4CMtHCWRbric5WPU+RNsYI7r1EpyVQMKO1pYw==",
      "license": "Apache-2.0",
      "dependencies": {
        "json-schema": "^0.4.0"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@ai-sdk/provider-utils": {
      "version": "4.0.4",
      "resolved": "https://registry.npmjs.org/@ai-sdk/provider-utils/-/provider-utils-4.0.4.tgz",
      "integrity": "sha512-VxhX0B/dWGbpNHxrKCWUAJKXIXV015J4e7qYjdIU9lLWeptk0KMLGcqkB4wFxff5Njqur8dt8wRi1MN9lZtDqg==",
      "license": "Apache-2.0",
      "dependencies": {
        "@ai-sdk/provider": "3.0.2",
        "@standard-schema/spec": "^1.1.0",
        "eventsource-parser": "^3.0.6"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "zod": "^3.25.76 || ^4.1.8"
      }
    },
    "node_modules/@borewit/text-codec": {
      "version": "0.2.1",
      "resolved": "https://registry.npmjs.org/@borewit/text-codec/-/text-codec-0.2.1.tgz",
      "integrity": "sha512-k7vvKPbf7J2fZ5klGRD9AeKfUvojuZIQ3BT5u7Jfv+puwXkUBUT5PVyMDfJZpy30CBDXGMgw7fguK/lpOMBvgw==",
      "license": "MIT",
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/Borewit"
      }
    },
    "node_modules/@isaacs/balanced-match": {
      "version": "4.0.1",
      "resolved": "https://registry.npmjs.org/@isaacs/balanced-match/-/balanced-match-4.0.1.tgz",
      "integrity": "sha512-yzMTt9lEb8Gv7zRioUilSglI0c0smZ9k5D65677DLWLtWJaXIS3CqcGyUFByYKlnUj6TkjLVs54fBl6+TiGQDQ==",
      "license": "MIT",
      "engines": {
        "node": "20 || >=22"
      }
    },
    "node_modules/@isaacs/brace-expansion": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/@isaacs/brace-expansion/-/brace-expansion-5.0.0.tgz",
      "integrity": "sha512-ZT55BDLV0yv0RBm2czMiZ+SqCGO7AvmOM3G/w2xhVPH+te0aKgFjmBvGlL1dH+ql2tgGO3MVrbb3jCKyvpgnxA==",
      "license": "MIT",
      "dependencies": {
        "@isaacs/balanced-match": "^4.0.1"
      },
      "engines": {
        "node": "20 || >=22"
      }
    },
    "node_modules/@isaacs/cliui": {
      "version": "8.0.2",
      "resolved": "https://registry.npmjs.org/@isaacs/cliui/-/cliui-8.0.2.tgz",
      "integrity": "sha512-O8jcjabXaleOG9DQ0+ARXWZBTfnP4WNAqzuiJK7ll44AmxGKv/J2M4TPjxjY3znBCfvBXFzucm1twdyFybFqEA==",
      "license": "ISC",
      "dependencies": {
        "string-width": "^5.1.2",
        "string-width-cjs": "npm:string-width@^4.2.0",
        "strip-ansi": "^7.0.1",
        "strip-ansi-cjs": "npm:strip-ansi@^6.0.1",
        "wrap-ansi": "^8.1.0",
        "wrap-ansi-cjs": "npm:wrap-ansi@^7.0.0"
      },
      "engines": {
        "node": ">=12"
      }
    },
    "node_modules/@mixmark-io/domino": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/@mixmark-io/domino/-/domino-2.2.0.tgz",
      "integrity": "sha512-Y28PR25bHXUg88kCV7nivXrP2Nj2RueZ3/l/jdx6J9f8J4nsEGcgX0Qe6lt7Pa+J79+kPiJU3LguR6O/6zrLOw==",
      "license": "BSD-2-Clause"
    },
    "node_modules/@nodelib/fs.scandir": {
      "version": "2.1.5",
      "resolved": "https://registry.npmjs.org/@nodelib/fs.scandir/-/fs.scandir-2.1.5.tgz",
      "integrity": "sha512-vq24Bq3ym5HEQm2NKCr3yXDwjc7vTsEThRDnkp2DK9p1uqLR+DHurm/NOTo0KG7HYHU7eppKZj3MyqYuMBf62g==",
      "license": "MIT",
      "dependencies": {
        "@nodelib/fs.stat": "2.0.5",
        "run-parallel": "^1.1.9"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/@nodelib/fs.stat": {
      "version": "2.0.5",
      "resolved": "https://registry.npmjs.org/@nodelib/fs.stat/-/fs.stat-2.0.5.tgz",
      "integrity": "sha512-RkhPPp2zrqDAQA/2jNhnztcPAlv64XdhIp7a7454A5ovI7Bukxgt7MX7udwAu3zg1DcpPU0rz3VV1SeaqvY4+A==",
      "license": "MIT",
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/@nodelib/fs.walk": {
      "version": "1.2.8",
      "resolved": "https://registry.npmjs.org/@nodelib/fs.walk/-/fs.walk-1.2.8.tgz",
      "integrity": "sha512-oGB+UxlgWcgQkgwo8GcEGwemoTFt3FIO9ababBmaGwXIoBKZ+GTy0pP185beGg7Llih/NSHSV2XAs1lnznocSg==",
      "license": "MIT",
      "dependencies": {
        "@nodelib/fs.scandir": "2.1.5",
        "fastq": "^1.6.0"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/@opentelemetry/api": {
      "version": "1.9.0",
      "resolved": "https://registry.npmjs.org/@opentelemetry/api/-/api-1.9.0.tgz",
      "integrity": "sha512-3giAOQvZiH5F9bMlMiv8+GSPMeqg0dbaeo58/0SlA9sxSqZhnUtxzX9/2FzyhS9sWQf5S0GJE0AKBrFqjpeYcg==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">=8.0.0"
      }
    },
    "node_modules/@standard-schema/spec": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/@standard-schema/spec/-/spec-1.1.0.tgz",
      "integrity": "sha512-l2aFy5jALhniG5HgqrD6jXLi/rUWrKvqN/qJx6yoJsgKhblVd+iqqU4RCXavm/jPityDo5TCvKMnpjKnOriy0w==",
      "license": "MIT"
    },
    "node_modules/@tokenizer/inflate": {
      "version": "0.4.1",
      "resolved": "https://registry.npmjs.org/@tokenizer/inflate/-/inflate-0.4.1.tgz",
      "integrity": "sha512-2mAv+8pkG6GIZiF1kNg1jAjh27IDxEPKwdGul3snfztFerfPGI1LjDezZp3i7BElXompqEtPmoPx6c2wgtWsOA==",
      "license": "MIT",
      "dependencies": {
        "debug": "^4.4.3",
        "token-types": "^6.1.1"
      },
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/Borewit"
      }
    },
    "node_modules/@tokenizer/token": {
      "version": "0.3.0",
      "resolved": "https://registry.npmjs.org/@tokenizer/token/-/token-0.3.0.tgz",
      "integrity": "sha512-OvjF+z51L3ov0OyAU0duzsYuvO01PH7x4t6DJx+guahgTnBHkhJdG7soQeTSFLWN3efnHyibZ4Z8l2EuWwJN3A==",
      "license": "MIT"
    },
    "node_modules/@types/node": {
      "version": "22.19.3",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-22.19.3.tgz",
      "integrity": "sha512-1N9SBnWYOJTrNZCdh/yJE+t910Y128BoyY+zBLWhL3r0TYzlTmFdXrPwHL9DyFZmlEXNQQolTZh3KHV31QDhyA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "undici-types": "~6.21.0"
      }
    },
    "node_modules/@vercel/oidc": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/@vercel/oidc/-/oidc-3.1.0.tgz",
      "integrity": "sha512-Fw28YZpRnA3cAHHDlkt7xQHiJ0fcL+NRcIqsocZQUSmbzeIKRpwttJjik5ZGanXP+vlA4SbTg+AbA3bP363l+w==",
      "license": "Apache-2.0",
      "engines": {
        "node": ">= 20"
      }
    },
    "node_modules/agentfs-sdk": {
      "resolved": "../../sdk/typescript",
      "link": true
    },
    "node_modules/ai": {
      "version": "6.0.16",
      "resolved": "https://registry.npmjs.org/ai/-/ai-6.0.16.tgz",
      "integrity": "sha512-pJDNvgKZJ/rFH3UtNOatJQnR7ajo8I5tSKWpObKmM6TlYADdKL90ustJf36YoaOGSYdP+Wg5GsNL9KJJ5Lm2ng==",
      "license": "Apache-2.0",
      "dependencies": {
        "@ai-sdk/gateway": "3.0.9",
        "@ai-sdk/provider": "3.0.2",
        "@ai-sdk/provider-utils": "4.0.4",
        "@opentelemetry/api": "1.9.0"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "zod": "^3.25.76 || ^4.1.8"
      }
    },
    "node_modules/ansi-regex": {
      "version": "6.2.2",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-6.2.2.tgz",
      "integrity": "sha512-Bq3SmSpyFHaWjPk8If9yc6svM8c56dB5BAtW4Qbw5jHTwwXXcTLoRMkpDJp6VL0XzlWaCHTXrkFURMYmD0sLqg==",
      "license": "MIT",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-regex?sponsor=1"
      }
    },
    "node_modules/ansi-styles": {
      "version": "6.2.3",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-6.2.3.tgz",
      "integrity": "sha512-4Dj6M28JB+oAH8kFkTLUo+a2jwOFkuqb3yucU0CANcRRUbxS0cP0nZYCGjcc3BNXwRIsUVmDGgzawme7zvJHvg==",
      "license": "MIT",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/bash-tool": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/bash-tool/-/bash-tool-1.0.0.tgz",
      "integrity": "sha512-GO8P1dVZnBPqZvjv5lXFtdZGDR9BM0A4UmE3NCfbUhr7nVv2K0qwDN/tPokaG839UcsZocGeU+Z/ELmFpqxRYw==",
      "license": "MIT",
      "dependencies": {
        "fast-glob": "^3.3.2",
        "zod": "^3.23.8"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@vercel/sandbox": "*",
        "ai": "^6.0.0",
        "just-bash": "*"
      },
      "peerDependenciesMeta": {
        "@vercel/sandbox": {
          "optional": true
        },
        "just-bash": {
          "optional": true
        }
      }
    },
    "node_modules/bash-tool/node_modules/zod": {
      "version": "3.25.76",
      "resolved": "https://registry.npmjs.org/zod/-/zod-3.25.76.tgz",
      "integrity": "sha512-gzUt/qt81nXsFGKIFcC3YnfEAx5NkunCfnDlvuBSSFS02bcXu4Lmea0AFIUwbLWxWPx3d9p8S5QoaujKcNQxcQ==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    },
    "node_modules/braces": {
      "version": "3.0.3",
      "resolved": "https://registry.npmjs.org/braces/-/braces-3.0.3.tgz",
      "integrity": "sha512-yQbXgO/OSZVD2IsiLlro+7Hf6Q18EJrKSEsdoMzKePKXct3gvD8oLcOQdIzGupr5Fj+EDe8gO/lxc1BzfMpxvA==",
      "license": "MIT",
      "dependencies": {
        "fill-range": "^7.1.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/color-convert": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/color-convert/-/color-convert-2.0.1.tgz",
      "integrity": "sha512-RRECPsj7iu/xb5oKYcsFHSppFNnsj/52OVTRKb4zP5onXwVF3zVmmToNcOfGC+CRDpfK/U584fMg38ZHCaElKQ==",
      "license": "MIT",
      "dependencies": {
        "color-name": "~1.1.4"
      },
      "engines": {
        "node": ">=7.0.0"
      }
    },
    "node_modules/color-name": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/color-name/-/color-name-1.1.4.tgz",
      "integrity": "sha512-dOy+3AuW3a2wNbZHIuMZpTcgjGuLU/uBL/ubcZF9OXbDo8ff4O8yVp5Bf0efS8uEoYo5q4Fx7dY9OgQGXgAsQA==",
      "license": "MIT"
    },
    "node_modules/cross-spawn": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz",
      "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==",
      "license": "MIT",
      "dependencies": {
        "path-key": "^3.1.0",
        "shebang-command": "^2.0.0",
        "which": "^2.0.1"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/diff": {
      "version": "8.0.2",
      "resolved": "https://registry.npmjs.org/diff/-/diff-8.0.2.tgz",
      "integrity": "sha512-sSuxWU5j5SR9QQji/o2qMvqRNYRDOcBTgsJ/DeCf4iSN4gW+gNMXM7wFIP+fdXZxoNiAnHUTGjCr+TSWXdRDKg==",
      "license": "BSD-3-Clause",
      "engines": {
        "node": ">=0.3.1"
      }
    },
    "node_modules/eastasianwidth": {
      "version": "0.2.0",
      "resolved": "https://registry.npmjs.org/eastasianwidth/-/eastasianwidth-0.2.0.tgz",
      "integrity": "sha512-I88TYZWc9XiYHRQ4/3c5rjjfgkjhLyW2luGIheGERbNQ6OY7yTybanSpDXZa8y7VUP9YmDcYa+eyq4ca7iLqWA==",
      "license": "MIT"
    },
    "node_modules/emoji-regex": {
      "version": "9.2.2",
      "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-9.2.2.tgz",
      "integrity": "sha512-L18DaJsXSUk2+42pv8mLs5jJT2hqFkFE4j21wOmgbUqsZ2hL72NsUU785g9RXgo3s0ZNgVl42TiHp3ZtOv/Vyg==",
      "license": "MIT"
    },
    "node_modules/eventsource-parser": {
      "version": "3.0.6",
      "resolved": "https://registry.npmjs.org/eventsource-parser/-/eventsource-parser-3.0.6.tgz",
      "integrity": "sha512-Vo1ab+QXPzZ4tCa8SwIHJFaSzy4R6SHf7BY79rFBDf0idraZWAkYrDjDj8uWaSm3S2TK+hJ7/t1CEmZ7jXw+pg==",
      "license": "MIT",
      "engines": {
        "node": ">=18.0.0"
      }
    },
    "node_modules/fast-glob": {
      "version": "3.3.3",
      "resolved": "https://registry.npmjs.org/fast-glob/-/fast-glob-3.3.3.tgz",
      "integrity": "sha512-7MptL8U0cqcFdzIzwOTHoilX9x5BrNqye7Z/LuC7kCMRio1EMSyqRK3BEAUD7sXRq4iT4AzTVuZdhgQ2TCvYLg==",
      "license": "MIT",
      "dependencies": {
        "@nodelib/fs.stat": "^2.0.2",
        "@nodelib/fs.walk": "^1.2.3",
        "glob-parent": "^5.1.2",
        "merge2": "^1.3.0",
        "micromatch": "^4.0.8"
      },
      "engines": {
        "node": ">=8.6.0"
      }
    },
    "node_modules/fastq": {
      "version": "1.20.1",
      "resolved": "https://registry.npmjs.org/fastq/-/fastq-1.20.1.tgz",
      "integrity": "sha512-GGToxJ/w1x32s/D2EKND7kTil4n8OVk/9mycTc4VDza13lOvpUZTGX3mFSCtV9ksdGBVzvsyAVLM6mHFThxXxw==",
      "license": "ISC",
      "dependencies": {
        "reusify": "^1.0.4"
      }
    },
    "node_modules/file-type": {
      "version": "21.3.0",
      "resolved": "https://registry.npmjs.org/file-type/-/file-type-21.3.0.tgz",
      "integrity": "sha512-8kPJMIGz1Yt/aPEwOsrR97ZyZaD1Iqm8PClb1nYFclUCkBi0Ma5IsYNQzvSFS9ib51lWyIw5mIT9rWzI/xjpzA==",
      "license": "MIT",
      "dependencies": {
        "@tokenizer/inflate": "^0.4.1",
        "strtok3": "^10.3.4",
        "token-types": "^6.1.1",
        "uint8array-extras": "^1.4.0"
      },
      "engines": {
        "node": ">=20"
      },
      "funding": {
        "url": "https://github.com/sindresorhus/file-type?sponsor=1"
      }
    },
    "node_modules/fill-range": {
      "version": "7.1.1",
      "resolved": "https://registry.npmjs.org/fill-range/-/fill-range-7.1.1.tgz",
      "integrity": "sha512-YsGpe3WHLK8ZYi4tWDg2Jy3ebRz2rXowDxnld4bkQB00cc/1Zw9AWnC0i9ztDJitivtQvaI9KaLyKrc+hBW0yg==",
      "license": "MIT",
      "dependencies": {
        "to-regex-range": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/foreground-child": {
      "version": "3.3.1",
      "resolved": "https://registry.npmjs.org/foreground-child/-/foreground-child-3.3.1.tgz",
      "integrity": "sha512-gIXjKqtFuWEgzFRJA9WCQeSJLZDjgJUOMCMzxtvFq/37KojM1BFGufqsCy0r4qSQmYLsZYMeyRqzIWOMup03sw==",
      "license": "ISC",
      "dependencies": {
        "cross-spawn": "^7.0.6",
        "signal-exit": "^4.0.1"
      },
      "engines": {
        "node": ">=14"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/glob": {
      "version": "11.1.0",
      "resolved": "https://registry.npmjs.org/glob/-/glob-11.1.0.tgz",
      "integrity": "sha512-vuNwKSaKiqm7g0THUBu2x7ckSs3XJLXE+2ssL7/MfTGPLLcrJQ/4Uq1CjPTtO5cCIiRxqvN6Twy1qOwhL0Xjcw==",
      "license": "BlueOak-1.0.0",
      "dependencies": {
        "foreground-child": "^3.3.1",
        "jackspeak": "^4.1.1",
        "minimatch": "^10.1.1",
        "minipass": "^7.1.2",
        "package-json-from-dist": "^1.0.0",
        "path-scurry": "^2.0.0"
      },
      "bin": {
        "glob": "dist/esm/bin.mjs"
      },
      "engines": {
        "node": "20 || >=22"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/glob-parent": {
      "version": "5.1.2",
      "resolved": "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz",
      "integrity": "sha512-AOIgSQCepiJYwP3ARnGx+5VnTu2HBYdzbGP45eLw1vr3zB3vZLeyed1sC9hnbcOc9/SrMyM5RPQrkGz4aS9Zow==",
      "license": "ISC",
      "dependencies": {
        "is-glob": "^4.0.1"
      },
      "engines": {
        "node": ">= 6"
      }
    },
    "node_modules/ieee754": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/ieee754/-/ieee754-1.2.1.tgz",
      "integrity": "sha512-dcyqhDvX1C46lXZcVqCpK+FtMRQVdIMN6/Df5js2zouUsqG7I6sFxitIC+7KYK29KdXOLHdu9zL4sFnoVQnqaA==",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/feross"
        },
        {
          "type": "patreon",
          "url": "https://www.patreon.com/feross"
        },
        {
          "type": "consulting",
          "url": "https://feross.org/support"
        }
      ],
      "license": "BSD-3-Clause"
    },
    "node_modules/is-extglob": {
      "version": "2.1.1",
      "resolved": "https://registry.npmjs.org/is-extglob/-/is-extglob-2.1.1.tgz",
      "integrity": "sha512-SbKbANkN603Vi4jEZv49LeVJMn4yGwsbzZworEoyEiutsN3nJYdbO36zfhGJ6QEDpOZIFkDtnq5JRxmvl3jsoQ==",
      "license": "MIT",
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-fullwidth-code-point": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/is-fullwidth-code-point/-/is-fullwidth-code-point-3.0.0.tgz",
      "integrity": "sha512-zymm5+u+sCsSWyD9qNaejV3DFvhCKclKdizYaJUuHA83RLjb7nSuGnddCHGv0hk+KY7BMAlsWeK4Ueg6EV6XQg==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/is-glob": {
      "version": "4.0.3",
      "resolved": "https://registry.npmjs.org/is-glob/-/is-glob-4.0.3.tgz",
      "integrity": "sha512-xelSayHH36ZgE7ZWhli7pW34hNbNl8Ojv5KVmkJD4hBdD3th8Tfk9vYasLM+mXWOZhFkgZfxhLSnrwRr4elSSg==",
      "license": "MIT",
      "dependencies": {
        "is-extglob": "^2.1.1"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/is-number": {
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/is-number/-/is-number-7.0.0.tgz",
      "integrity": "sha512-41Cifkg6e8TylSpdtTpeLVMqvSBEVzTttHvERD741+pnZ8ANv0004MRL43QKPDlK9cGvNp6NZWZUBlbGXYxxng==",
      "license": "MIT",
      "engines": {
        "node": ">=0.12.0"
      }
    },
    "node_modules/isexe": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz",
      "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==",
      "license": "ISC"
    },
    "node_modules/jackspeak": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/jackspeak/-/jackspeak-4.1.1.tgz",
      "integrity": "sha512-zptv57P3GpL+O0I7VdMJNBZCu+BPHVQUk55Ft8/QCJjTVxrnJHuVuX/0Bl2A6/+2oyR/ZMEuFKwmzqqZ/U5nPQ==",
      "license": "BlueOak-1.0.0",
      "dependencies": {
        "@isaacs/cliui": "^8.0.2"
      },
      "engines": {
        "node": "20 || >=22"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/json-schema": {
      "version": "0.4.0",
      "resolved": "https://registry.npmjs.org/json-schema/-/json-schema-0.4.0.tgz",
      "integrity": "sha512-es94M3nTIfsEPisRafak+HDLfHXnKBhV3vU5eqPcS3flIWqcxJWgXHXiey3YrpaNsanY5ei1VoYEbOzijuq9BA==",
      "license": "(AFL-2.1 OR BSD-3-Clause)"
    },
    "node_modules/just-bash": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/just-bash/-/just-bash-2.0.0.tgz",
      "integrity": "sha512-dIxmIwoHMZysQfjOm5pLlpE75gb0QNMTzuhUSaip/YbSSM4x8jA77EdVEex2TH7pIE5PocApEv3WxavsnVQLEA==",
      "license": "Apache-2.0",
      "dependencies": {
        "diff": "^8.0.2",
        "file-type": "^21.2.0",
        "minimatch": "^10.1.1",
        "sprintf-js": "^1.1.3",
        "turndown": "^7.2.2"
      },
      "bin": {
        "just-bash": "dist/bin/just-bash.js",
        "just-bash-shell": "dist/bin/shell/shell.js"
      }
    },
    "node_modules/lru-cache": {
      "version": "11.2.4",
      "resolved": "https://registry.npmjs.org/lru-cache/-/lru-cache-11.2.4.tgz",
      "integrity": "sha512-B5Y16Jr9LB9dHVkh6ZevG+vAbOsNOYCX+sXvFWFu7B3Iz5mijW3zdbMyhsh8ANd2mSWBYdJgnqi+mL7/LrOPYg==",
      "license": "BlueOak-1.0.0",
      "engines": {
        "node": "20 || >=22"
      }
    },
    "node_modules/merge2": {
      "version": "1.4.1",
      "resolved": "https://registry.npmjs.org/merge2/-/merge2-1.4.1.tgz",
      "integrity": "sha512-8q7VEgMJW4J8tcfVPy8g09NcQwZdbwFEqhe/WZkoIzjn/3TGDwtOCYtXGxA3O8tPzpczCCDgv+P2P5y00ZJOOg==",
      "license": "MIT",
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/micromatch": {
      "version": "4.0.8",
      "resolved": "https://registry.npmjs.org/micromatch/-/micromatch-4.0.8.tgz",
      "integrity": "sha512-PXwfBhYu0hBCPw8Dn0E+WDYb7af3dSLVWKi3HGv84IdF4TyFoC0ysxFd0Goxw7nSv4T/PzEJQxsYsEiFCKo2BA==",
      "license": "MIT",
      "dependencies": {
        "braces": "^3.0.3",
        "picomatch": "^2.3.1"
      },
      "engines": {
        "node": ">=8.6"
      }
    },
    "node_modules/minimatch": {
      "version": "10.1.1",
      "resolved": "https://registry.npmjs.org/minimatch/-/minimatch-10.1.1.tgz",
      "integrity": "sha512-enIvLvRAFZYXJzkCYG5RKmPfrFArdLv+R+lbQ53BmIMLIry74bjKzX6iHAm8WYamJkhSSEabrWN5D97XnKObjQ==",
      "license": "BlueOak-1.0.0",
      "dependencies": {
        "@isaacs/brace-expansion": "^5.0.0"
      },
      "engines": {
        "node": "20 || >=22"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/minipass": {
      "version": "7.1.2",
      "resolved": "https://registry.npmjs.org/minipass/-/minipass-7.1.2.tgz",
      "integrity": "sha512-qOOzS1cBTWYF4BH8fVePDBOO9iptMnGUEZwNc/cMWnTV2nVLZ7VoNWEPHkYczZA0pdoA7dl6e7FL659nX9S2aw==",
      "license": "ISC",
      "engines": {
        "node": ">=16 || 14 >=14.17"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "license": "MIT"
    },
    "node_modules/package-json-from-dist": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/package-json-from-dist/-/package-json-from-dist-1.0.1.tgz",
      "integrity": "sha512-UEZIS3/by4OC8vL3P2dTXRETpebLI2NiI5vIrjaD/5UtrkFX/tNbwjTSRAGC/+7CAo2pIcBaRgWmcBBHcsaCIw==",
      "license": "BlueOak-1.0.0"
    },
    "node_modules/path-key": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz",
      "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-scurry": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/path-scurry/-/path-scurry-2.0.1.tgz",
      "integrity": "sha512-oWyT4gICAu+kaA7QWk/jvCHWarMKNs6pXOGWKDTr7cw4IGcUbW+PeTfbaQiLGheFRpjo6O9J0PmyMfQPjH71oA==",
      "license": "BlueOak-1.0.0",
      "dependencies": {
        "lru-cache": "^11.0.0",
        "minipass": "^7.1.2"
      },
      "engines": {
        "node": "20 || >=22"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/picomatch": {
      "version": "2.3.1",
      "resolved": "https://registry.npmjs.org/picomatch/-/picomatch-2.3.1.tgz",
      "integrity": "sha512-JU3teHTNjmE2VCGFzuY8EXzCDVwEqB2a8fsIvwaStHhAWJEeVd1o1QD80CU6+ZdEXXSLbSsuLwJjkCBWqRQUVA==",
      "license": "MIT",
      "engines": {
        "node": ">=8.6"
      },
      "funding": {
        "url": "https://github.com/sponsors/jonschlinkert"
      }
    },
    "node_modules/queue-microtask": {
      "version": "1.2.3",
      "resolved": "https://registry.npmjs.org/queue-microtask/-/queue-microtask-1.2.3.tgz",
      "integrity": "sha512-NuaNSa6flKT5JaSYQzJok04JzTL1CA6aGhv5rfLW3PgqA+M2ChpZQnAC8h8i4ZFkBS8X5RqkDBHA7r4hej3K9A==",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/feross"
        },
        {
          "type": "patreon",
          "url": "https://www.patreon.com/feross"
        },
        {
          "type": "consulting",
          "url": "https://feross.org/support"
        }
      ],
      "license": "MIT"
    },
    "node_modules/reusify": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/reusify/-/reusify-1.1.0.tgz",
      "integrity": "sha512-g6QUff04oZpHs0eG5p83rFLhHeV00ug/Yf9nZM6fLeUrPguBTkTQOdpAWWspMh55TZfVQDPaN3NQJfbVRAxdIw==",
      "license": "MIT",
      "engines": {
        "iojs": ">=1.0.0",
        "node": ">=0.10.0"
      }
    },
    "node_modules/run-parallel": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/run-parallel/-/run-parallel-1.2.0.tgz",
      "integrity": "sha512-5l4VyZR86LZ/lDxZTR6jqL8AFE2S0IFLMP26AbjsLVADxHdhB/c0GUsH+y39UfCi3dzz8OlQuPmnaJOMoDHQBA==",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/feross"
        },
        {
          "type": "patreon",
          "url": "https://www.patreon.com/feross"
        },
        {
          "type": "consulting",
          "url": "https://feross.org/support"
        }
      ],
      "license": "MIT",
      "dependencies": {
        "queue-microtask": "^1.2.2"
      }
    },
    "node_modules/shebang-command": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz",
      "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==",
      "license": "MIT",
      "dependencies": {
        "shebang-regex": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/shebang-regex": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz",
      "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/signal-exit": {
      "version": "4.1.0",
      "resolved": "https://registry.npmjs.org/signal-exit/-/signal-exit-4.1.0.tgz",
      "integrity": "sha512-bzyZ1e88w9O1iNJbKnOlvYTrWPDl46O1bG0D3XInv+9tkPrxrN8jUUTiFlDkkmKWgn1M6CfIA13SuGqOa9Korw==",
      "license": "ISC",
      "engines": {
        "node": ">=14"
      },
      "funding": {
        "url": "https://github.com/sponsors/isaacs"
      }
    },
    "node_modules/sprintf-js": {
      "version": "1.1.3",
      "resolved": "https://registry.npmjs.org/sprintf-js/-/sprintf-js-1.1.3.tgz",
      "integrity": "sha512-Oo+0REFV59/rz3gfJNKQiBlwfHaSESl1pcGyABQsnnIfWOFt6JNj5gCog2U6MLZ//IGYD+nA8nI+mTShREReaA==",
      "license": "BSD-3-Clause"
    },
    "node_modules/string-width": {
      "version": "5.1.2",
      "resolved": "https://registry.npmjs.org/string-width/-/string-width-5.1.2.tgz",
      "integrity": "sha512-HnLOCR3vjcY8beoNLtcjZ5/nxn2afmME6lhrDrebokqMap+XbeW8n9TXpPDOqdGK5qcI3oT0GKTW6wC7EMiVqA==",
      "license": "MIT",
      "dependencies": {
        "eastasianwidth": "^0.2.0",
        "emoji-regex": "^9.2.2",
        "strip-ansi": "^7.0.1"
      },
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/string-width-cjs": {
      "name": "string-width",
      "version": "4.2.3",
      "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz",
      "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==",
      "license": "MIT",
      "dependencies": {
        "emoji-regex": "^8.0.0",
        "is-fullwidth-code-point": "^3.0.0",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/string-width-cjs/node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/string-width-cjs/node_modules/emoji-regex": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz",
      "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==",
      "license": "MIT"
    },
    "node_modules/string-width-cjs/node_modules/strip-ansi": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-ansi": {
      "version": "7.1.2",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-7.1.2.tgz",
      "integrity": "sha512-gmBGslpoQJtgnMAvOVqGZpEz9dyoKTCzy2nfz/n8aIFhN/jCE/rCmcxabB6jOOHV+0WNnylOxaxBQPSvcWklhA==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^6.0.1"
      },
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/chalk/strip-ansi?sponsor=1"
      }
    },
    "node_modules/strip-ansi-cjs": {
      "name": "strip-ansi",
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strip-ansi-cjs/node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/strtok3": {
      "version": "10.3.4",
      "resolved": "https://registry.npmjs.org/strtok3/-/strtok3-10.3.4.tgz",
      "integrity": "sha512-KIy5nylvC5le1OdaaoCJ07L+8iQzJHGH6pWDuzS+d07Cu7n1MZ2x26P8ZKIWfbK02+XIL8Mp4RkWeqdUCrDMfg==",
      "license": "MIT",
      "dependencies": {
        "@tokenizer/token": "^0.3.0"
      },
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/Borewit"
      }
    },
    "node_modules/to-regex-range": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/to-regex-range/-/to-regex-range-5.0.1.tgz",
      "integrity": "sha512-65P7iz6X5yEr1cwcgvQxbbIw7Uk3gOy5dIdtZ4rDveLqhrdJP+Li/Hx6tyK0NEb+2GCyneCMJiGqrADCSNk8sQ==",
      "license": "MIT",
      "dependencies": {
        "is-number": "^7.0.0"
      },
      "engines": {
        "node": ">=8.0"
      }
    },
    "node_modules/token-types": {
      "version": "6.1.2",
      "resolved": "https://registry.npmjs.org/token-types/-/token-types-6.1.2.tgz",
      "integrity": "sha512-dRXchy+C0IgK8WPC6xvCHFRIWYUbqqdEIKPaKo/AcTUNzwLTK6AH7RjdLWsEZcAN/TBdtfUw3PYEgPr5VPr6ww==",
      "license": "MIT",
      "dependencies": {
        "@borewit/text-codec": "^0.2.1",
        "@tokenizer/token": "^0.3.0",
        "ieee754": "^1.2.1"
      },
      "engines": {
        "node": ">=14.16"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/Borewit"
      }
    },
    "node_modules/turndown": {
      "version": "7.2.2",
      "resolved": "https://registry.npmjs.org/turndown/-/turndown-7.2.2.tgz",
      "integrity": "sha512-1F7db8BiExOKxjSMU2b7if62D/XOyQyZbPKq/nUwopfgnHlqXHqQ0lvfUTeUIr1lZJzOPFn43dODyMSIfvWRKQ==",
      "license": "MIT",
      "dependencies": {
        "@mixmark-io/domino": "^2.2.0"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.3",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.9.3.tgz",
      "integrity": "sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/uint8array-extras": {
      "version": "1.5.0",
      "resolved": "https://registry.npmjs.org/uint8array-extras/-/uint8array-extras-1.5.0.tgz",
      "integrity": "sha512-rvKSBiC5zqCCiDZ9kAOszZcDvdAHwwIKJG33Ykj43OKcWsnmcBRL09YTU4nOeHZ8Y2a7l1MgTd08SBe9A8Qj6A==",
      "license": "MIT",
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/undici-types": {
      "version": "6.21.0",
      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-6.21.0.tgz",
      "integrity": "sha512-iwDZqg0QAGrg9Rav5H4n0M64c3mkR59cJ6wQp+7C4nI0gsmExaedaYLNO44eT4AtBBwjbTiGPMlt2Md0T9H9JQ==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "license": "ISC",
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/wrap-ansi": {
      "version": "8.1.0",
      "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-8.1.0.tgz",
      "integrity": "sha512-si7QWI6zUMq56bESFvagtmzMdGOtoxfR+Sez11Mobfc7tm+VkUckk9bW2UeffTGVUbOksxmSw0AA2gs8g71NCQ==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^6.1.0",
        "string-width": "^5.0.1",
        "strip-ansi": "^7.0.1"
      },
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://github.com/chalk/wrap-ansi?sponsor=1"
      }
    },
    "node_modules/wrap-ansi-cjs": {
      "name": "wrap-ansi",
      "version": "7.0.0",
      "resolved": "https://registry.npmjs.org/wrap-ansi/-/wrap-ansi-7.0.0.tgz",
      "integrity": "sha512-YVGIj2kamLSTxw6NsZjoBxfSwsn0ycdesmc4p+Q21c5zPuZ1pl+NfxVdxPtdHvmNVOQ6XSYG4AUtyt/Fi7D16Q==",
      "license": "MIT",
      "dependencies": {
        "ansi-styles": "^4.0.0",
        "string-width": "^4.1.0",
        "strip-ansi": "^6.0.0"
      },
      "engines": {
        "node": ">=10"
      },
      "funding": {
        "url": "https://github.com/chalk/wrap-ansi?sponsor=1"
      }
    },
    "node_modules/wrap-ansi-cjs/node_modules/ansi-regex": {
      "version": "5.0.1",
      "resolved": "https://registry.npmjs.org/ansi-regex/-/ansi-regex-5.0.1.tgz",
      "integrity": "sha512-quJQXlTSUGL2LH9SUXo8VwsY4soanhgo6LNSm84E1LBcE8s3O0wpdiRzyR9z/ZZJMlMWv37qOOb9pdJlMUEKFQ==",
      "license": "MIT",
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/wrap-ansi-cjs/node_modules/ansi-styles": {
      "version": "4.3.0",
      "resolved": "https://registry.npmjs.org/ansi-styles/-/ansi-styles-4.3.0.tgz",
      "integrity": "sha512-zbB9rCJAT1rbjiVDb2hqKFHNYLxgtk8NURxZ3IZwD3F6NtxbXZQCnnSi1Lkx+IDohdPlFp222wVALIheZJQSEg==",
      "license": "MIT",
      "dependencies": {
        "color-convert": "^2.0.1"
      },
      "engines": {
        "node": ">=8"
      },
      "funding": {
        "url": "https://github.com/chalk/ansi-styles?sponsor=1"
      }
    },
    "node_modules/wrap-ansi-cjs/node_modules/emoji-regex": {
      "version": "8.0.0",
      "resolved": "https://registry.npmjs.org/emoji-regex/-/emoji-regex-8.0.0.tgz",
      "integrity": "sha512-MSjYzcWNOA0ewAHpz0MxpYFvwg6yjy1NG3xteoqz644VCo/RPgnr1/GGt+ic3iJTzQ8Eu3TdM14SawnVUmGE6A==",
      "license": "MIT"
    },
    "node_modules/wrap-ansi-cjs/node_modules/string-width": {
      "version": "4.2.3",
      "resolved": "https://registry.npmjs.org/string-width/-/string-width-4.2.3.tgz",
      "integrity": "sha512-wKyQRQpjJ0sIp62ErSZdGsjMJWsap5oRNihHhu6G7JVO/9jIB6UyevL+tXuOqrng8j/cxKTWyWUwvSTriiZz/g==",
      "license": "MIT",
      "dependencies": {
        "emoji-regex": "^8.0.0",
        "is-fullwidth-code-point": "^3.0.0",
        "strip-ansi": "^6.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/wrap-ansi-cjs/node_modules/strip-ansi": {
      "version": "6.0.1",
      "resolved": "https://registry.npmjs.org/strip-ansi/-/strip-ansi-6.0.1.tgz",
      "integrity": "sha512-Y38VPSHcqkFrCpFnQ9vuSXmquuv5oXOKpGeT6aGrr3o3Gc9AlVa6JBfUSOCnbxGGZF+/0ooI7KrPuUSztUdU5A==",
      "license": "MIT",
      "dependencies": {
        "ansi-regex": "^5.0.1"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/zod": {
      "version": "4.3.5",
      "resolved": "https://registry.npmjs.org/zod/-/zod-4.3.5.tgz",
      "integrity": "sha512-k7Nwx6vuWx1IJ9Bjuf4Zt1PEllcwe7cls3VNzm4CQ1/hgtFUK2bRNG3rvnpPUhFjmqJKAKtjV576KnUkHocg/g==",
      "license": "MIT",
      "peer": true,
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    }
  }
}




------------------------------------------
File: examples/ai-sdk-just-bash/package.json
------------------------------------------

{
  "name": "just-bash-agentfs-example",
  "version": "1.0.0",
  "description": "Interactive AI agent with persistent AgentFS storage",
  "type": "module",
  "scripts": {
    "start": "npx tsx main.ts",
    "typecheck": "npx tsc --noEmit"
  },
  "dependencies": {
    "@ai-sdk/anthropic": "^3.0.0",
    "agentfs-sdk": "file:../../sdk/typescript",
    "ai": "^6.0.0",
    "bash-tool": "^1.0.0",
    "glob": "^11.0.0",
    "just-bash": "^2.0.0"
  },
  "devDependencies": {
    "@types/node": "^22.0.0",
    "typescript": "^5.0.0"
  }
}




------------------------------------------
File: examples/ai-sdk-just-bash/README.md
------------------------------------------

# AI SDK + just-bash Code Explorer Agent

An interactive AI agent that combines [Vercel AI SDK](https://sdk.vercel.ai/), [just-bash](https://github.com/vercel-labs/just-bash) for bash command execution, and [AgentFS](https://github.com/tursodatabase/agentfs) for persistent filesystem storage.

This example is forked from the [just-bash bash-agent example](https://github.com/vercel-labs/just-bash/tree/main/examples/bash-agent) with AgentFS integration added.

## How It Works

- **Vercel AI SDK** - Orchestrates the AI agent with Claude as the model
- **just-bash** - Provides a bash tool that the AI can use to execute shell commands
- **AgentFS** - Backs the virtual filesystem with SQLite for persistence across sessions

## Files

- `main.ts` - Entry point
- `agent.ts` - Agent logic using AI SDK's `streamText` with just-bash's `createBashTool`
- `shell.ts` - Interactive readline shell

## Setup

1. Install dependencies:

   ```bash
   npm install
   ```

2. Set your Anthropic API key:

   ```bash
   export ANTHROPIC_API_KEY=your-key-here
   ```

3. Run:
   ```bash
   npm start
   ```

## Usage

Ask questions like:

- "What commands are available?"
- "How is the grep command implemented?"
- "Show me the Bash class"
- "Find all test files"

Type `exit` to quit.

## Development

```bash
npm run typecheck
```




------------------------------------------
File: examples/ai-sdk-just-bash/shell.ts
------------------------------------------

/**
 * Interactive shell for the just-bash agent with AgentFS
 */

import * as readline from "node:readline";
import type { AgentRunner } from "./agent.js";

const colors = {
  reset: "\x1b[0m",
  bold: "\x1b[1m",
  dim: "\x1b[2m",
  cyan: "\x1b[36m",
  green: "\x1b[32m",
  yellow: "\x1b[33m",
  blue: "\x1b[34m",
};

export function runShell(agent: AgentRunner): void {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout,
  });

  console.log(`${colors.cyan}${colors.bold}╔══════════════════════════════════════════════════════════════╗
║              just-bash Agent with AgentFS                      ║
║         Persistent filesystem backed by SQLite                 ║
╚══════════════════════════════════════════════════════════════╝${colors.reset}
`);
  console.log(
    `${colors.dim}Type your question and press Enter. Type 'exit' to quit.${colors.reset}\n`
  );

  const prompt = (): void => {
    rl.question(`${colors.green}You:${colors.reset} `, async (input: string) => {
      const trimmed = input.trim();

      if (!trimmed) {
        prompt();
        return;
      }

      if (trimmed.toLowerCase() === "exit") {
        console.log("\nGoodbye!");
        rl.close();
        process.exit(0);
      }

      process.stdout.write(
        `\n${colors.blue}${colors.bold}Agent:${colors.reset} `
      );

      try {
        await agent.chat(trimmed, {
          onText: (text) => process.stdout.write(text),
        });
      } catch (error) {
        const message = error instanceof Error ? error.message : String(error);
        console.error(`\n${colors.yellow}Error: ${message}${colors.reset}`);
      }

      console.log("");
      prompt();
    });
  };

  prompt();
}




------------------------------------------
File: examples/ai-sdk-just-bash/tsconfig.json
------------------------------------------

{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "noEmit": true
  },
  "include": ["*.ts", "../../src/**/*.ts"],
  "exclude": ["node_modules"]
}




------------------------------------------
File: examples/claude-agent/research-assistant/.env.example
------------------------------------------

# Anthropic API Key (required)
ANTHROPIC_API_KEY=your-api-key-here

# Optional: Custom AgentFS agent ID
# AGENTFS_ID=research-assistant




------------------------------------------
File: examples/claude-agent/research-assistant/package-lock.json
------------------------------------------

{
  "name": "research-assistant-claude-agent",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "research-assistant-claude-agent",
      "version": "1.0.0",
      "license": "ISC",
      "dependencies": {
        "@anthropic-ai/claude-agent-sdk": "^0.1.0",
        "agentfs-sdk": "file:../../../sdk/typescript",
        "dotenv": "^16.4.7",
        "pdf-parse": "^1.1.1",
        "zod": "^3.24.1"
      },
      "bin": {
        "research-assistant": "dist/cli.js"
      },
      "devDependencies": {
        "@types/node": "^24.10.0",
        "@types/pdf-parse": "^1.1.4",
        "tsx": "^4.19.1",
        "typescript": "^5.9.3"
      },
      "engines": {
        "node": ">=20.9.0"
      }
    },
    "../../../sdk/typescript": {
      "name": "agentfs-sdk",
      "version": "0.1.0",
      "license": "MIT",
      "dependencies": {
        "@tursodatabase/database": "^0.3.2"
      },
      "devDependencies": {
        "@types/node": "^20.0.0",
        "@vitest/ui": "^4.0.1",
        "typescript": "^5.3.0",
        "vitest": "^4.0.1"
      }
    },
    "node_modules/@anthropic-ai/claude-agent-sdk": {
      "version": "0.1.37",
      "resolved": "https://registry.npmjs.org/@anthropic-ai/claude-agent-sdk/-/claude-agent-sdk-0.1.37.tgz",
      "integrity": "sha512-LMfqMIPLTz0vRhpcO7hpPJ5L6Bg24y5/PaqZvwAUNZ/GR3OAl7xmJR7IryIR6m8Pyd/6Hs2yBU8j86Os+wHFQQ==",
      "license": "SEE LICENSE IN README.md",
      "engines": {
        "node": ">=18.0.0"
      },
      "optionalDependencies": {
        "@img/sharp-darwin-arm64": "^0.33.5",
        "@img/sharp-darwin-x64": "^0.33.5",
        "@img/sharp-linux-arm": "^0.33.5",
        "@img/sharp-linux-arm64": "^0.33.5",
        "@img/sharp-linux-x64": "^0.33.5",
        "@img/sharp-win32-x64": "^0.33.5"
      },
      "peerDependencies": {
        "zod": "^3.24.1"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.12.tgz",
      "integrity": "sha512-Hhmwd6CInZ3dwpuGTF8fJG6yoWmsToE+vYgD4nytZVxcu1ulHpUQRAB1UJ8+N1Am3Mz4+xOByoQoSZf4D+CpkA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.12.tgz",
      "integrity": "sha512-VJ+sKvNA/GE7Ccacc9Cha7bpS8nyzVv0jdVgwNDaR4gDMC/2TTRc33Ip8qrNYUcpkOHUT5OZ0bUcNNVZQ9RLlg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.12.tgz",
      "integrity": "sha512-6AAmLG7zwD1Z159jCKPvAxZd4y/VTO0VkprYy+3N2FtJ8+BQWFXU+OxARIwA46c5tdD9SsKGZ/1ocqBS/gAKHg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.12.tgz",
      "integrity": "sha512-5jbb+2hhDHx5phYR2By8GTWEzn6I9UqR11Kwf22iKbNpYrsmRB18aX/9ivc5cabcUiAT/wM+YIZ6SG9QO6a8kg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.12.tgz",
      "integrity": "sha512-N3zl+lxHCifgIlcMUP5016ESkeQjLj/959RxxNYIthIg+CQHInujFuXeWbWMgnTo4cp5XVHqFPmpyu9J65C1Yg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.12.tgz",
      "integrity": "sha512-HQ9ka4Kx21qHXwtlTUVbKJOAnmG1ipXhdWTmNXiPzPfWKpXqASVcWdnf2bnL73wgjNrFXAa3yYvBSd9pzfEIpA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.12.tgz",
      "integrity": "sha512-gA0Bx759+7Jve03K1S0vkOu5Lg/85dou3EseOGUes8flVOGxbhDDh/iZaoek11Y8mtyKPGF3vP8XhnkDEAmzeg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.12.tgz",
      "integrity": "sha512-TGbO26Yw2xsHzxtbVFGEXBFH0FRAP7gtcPE7P5yP7wGy7cXK2oO7RyOhL5NLiqTlBh47XhmIUXuGciXEqYFfBQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.12.tgz",
      "integrity": "sha512-lPDGyC1JPDou8kGcywY0YILzWlhhnRjdof3UlcoqYmS9El818LLfJJc3PXXgZHrHCAKs/Z2SeZtDJr5MrkxtOw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.12.tgz",
      "integrity": "sha512-8bwX7a8FghIgrupcxb4aUmYDLp8pX06rGh5HqDT7bB+8Rdells6mHvrFHHW2JAOPZUbnjUpKTLg6ECyzvas2AQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.12.tgz",
      "integrity": "sha512-0y9KrdVnbMM2/vG8KfU0byhUN+EFCny9+8g202gYqSSVMonbsCfLjUO+rCci7pM0WBEtz+oK/PIwHkzxkyharA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.12.tgz",
      "integrity": "sha512-h///Lr5a9rib/v1GGqXVGzjL4TMvVTv+s1DPoxQdz7l/AYv6LDSxdIwzxkrPW438oUXiDtwM10o9PmwS/6Z0Ng==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.12.tgz",
      "integrity": "sha512-iyRrM1Pzy9GFMDLsXn1iHUm18nhKnNMWscjmp4+hpafcZjrr2WbT//d20xaGljXDBYHqRcl8HnxbX6uaA/eGVw==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.12.tgz",
      "integrity": "sha512-9meM/lRXxMi5PSUqEXRCtVjEZBGwB7P/D4yT8UG/mwIdze2aV4Vo6U5gD3+RsoHXKkHCfSxZKzmDssVlRj1QQA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.12.tgz",
      "integrity": "sha512-Zr7KR4hgKUpWAwb1f3o5ygT04MzqVrGEGXGLnj15YQDJErYu/BGg+wmFlIDOdJp0PmB0lLvxFIOXZgFRrdjR0w==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.12.tgz",
      "integrity": "sha512-MsKncOcgTNvdtiISc/jZs/Zf8d0cl/t3gYWX8J9ubBnVOwlk65UIEEvgBORTiljloIWnBzLs4qhzPkJcitIzIg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.12.tgz",
      "integrity": "sha512-uqZMTLr/zR/ed4jIGnwSLkaHmPjOjJvnm6TVVitAa08SLS9Z0VM8wIRx7gWbJB5/J54YuIMInDquWyYvQLZkgw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-xXwcTq4GhRM7J9A8Gv5boanHhRa/Q9KLVmcyXHCTaM4wKfIpWkdXiMog/KsnxzJ0A1+nD+zoecuzqPmCRyBGjg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.12.tgz",
      "integrity": "sha512-Ld5pTlzPy3YwGec4OuHh1aCVCRvOXdH8DgRjfDy/oumVovmuSzWfnSJg+VtakB9Cm0gxNO9BzWkj6mtO1FMXkQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-fF96T6KsBo/pkQI950FARU9apGNTSlZGsv1jZBAlcLL1MLjLNIWPBkj5NlSz8aAzYKg+eNqknrUJ24QBybeR5A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.12.tgz",
      "integrity": "sha512-MZyXUkZHjQxUvzK7rN8DJ3SRmrVrke8ZyRusHlP+kuwqTcfWLyqMOE3sScPPyeIXN/mDJIfGXvcMqCgYKekoQw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openharmony-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openharmony-arm64/-/openharmony-arm64-0.25.12.tgz",
      "integrity": "sha512-rm0YWsqUSRrjncSXGA7Zv78Nbnw4XL6/dzr20cyrQf7ZmRcsovpcRBdhD43Nuk3y7XIoW2OxMVvwuRvk9XdASg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.12.tgz",
      "integrity": "sha512-3wGSCDyuTHQUzt0nV7bocDy72r2lI33QL3gkDNGkod22EsYl04sMf0qLb8luNKTOmgF/eDEDP5BFNwoBKH441w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.12.tgz",
      "integrity": "sha512-rMmLrur64A7+DKlnSuwqUdRKyd3UE7oPJZmnljqEptesKM8wx9J8gx5u0+9Pq0fQQW8vqeKebwNXdfOyP+8Bsg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.12.tgz",
      "integrity": "sha512-HkqnmmBoCbCwxUKKNPBixiWDGCpQGVsrQfJoVGYLPT41XWF8lHuE5N6WhVia2n4o5QK5M4tYr21827fNhi4byQ==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.12.tgz",
      "integrity": "sha512-alJC0uCZpTFrSL0CCDjcgleBXPnCrEAhTBILpeAp7M/OFgoqtAetfBzX0xM00MUsVVPpVjlPuMbREqnZCXaTnA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@img/sharp-darwin-arm64": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-darwin-arm64/-/sharp-darwin-arm64-0.33.5.tgz",
      "integrity": "sha512-UT4p+iz/2H4twwAoLCqfA9UH5pI6DggwKEGuaPy7nCVQ8ZsiY5PIcrRvD1DzuY3qYL07NtIQcWnBSY/heikIFQ==",
      "cpu": [
        "arm64"
      ],
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      },
      "optionalDependencies": {
        "@img/sharp-libvips-darwin-arm64": "1.0.4"
      }
    },
    "node_modules/@img/sharp-darwin-x64": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-darwin-x64/-/sharp-darwin-x64-0.33.5.tgz",
      "integrity": "sha512-fyHac4jIc1ANYGRDxtiqelIbdWkIuQaI84Mv45KvGRRxSAa7o7d1ZKAOBaYbnepLC1WqxfpimdeWfvqqSGwR2Q==",
      "cpu": [
        "x64"
      ],
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      },
      "optionalDependencies": {
        "@img/sharp-libvips-darwin-x64": "1.0.4"
      }
    },
    "node_modules/@img/sharp-libvips-darwin-arm64": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/@img/sharp-libvips-darwin-arm64/-/sharp-libvips-darwin-arm64-1.0.4.tgz",
      "integrity": "sha512-XblONe153h0O2zuFfTAbQYAX2JhYmDHeWikp1LM9Hul9gVPjFY427k6dFEcOL72O01QxQsWi761svJ/ev9xEDg==",
      "cpu": [
        "arm64"
      ],
      "license": "LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "darwin"
      ],
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@img/sharp-libvips-darwin-x64": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/@img/sharp-libvips-darwin-x64/-/sharp-libvips-darwin-x64-1.0.4.tgz",
      "integrity": "sha512-xnGR8YuZYfJGmWPvmlunFaWJsb9T/AO2ykoP3Fz/0X5XV2aoYBPkX6xqCQvUTKKiLddarLaxpzNe+b1hjeWHAQ==",
      "cpu": [
        "x64"
      ],
      "license": "LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "darwin"
      ],
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@img/sharp-libvips-linux-arm": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-libvips-linux-arm/-/sharp-libvips-linux-arm-1.0.5.tgz",
      "integrity": "sha512-gvcC4ACAOPRNATg/ov8/MnbxFDJqf/pDePbBnuBDcjsI8PssmjoKMAz4LtLaVi+OnSb5FK/yIOamqDwGmXW32g==",
      "cpu": [
        "arm"
      ],
      "license": "LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "linux"
      ],
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@img/sharp-libvips-linux-arm64": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/@img/sharp-libvips-linux-arm64/-/sharp-libvips-linux-arm64-1.0.4.tgz",
      "integrity": "sha512-9B+taZ8DlyyqzZQnoeIvDVR/2F4EbMepXMc/NdVbkzsJbzkUjhXv/70GQJ7tdLA4YJgNP25zukcxpX2/SueNrA==",
      "cpu": [
        "arm64"
      ],
      "license": "LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "linux"
      ],
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@img/sharp-libvips-linux-x64": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/@img/sharp-libvips-linux-x64/-/sharp-libvips-linux-x64-1.0.4.tgz",
      "integrity": "sha512-MmWmQ3iPFZr0Iev+BAgVMb3ZyC4KeFc3jFxnNbEPas60e1cIfevbtuyf9nDGIzOaW9PdnDciJm+wFFaTlj5xYw==",
      "cpu": [
        "x64"
      ],
      "license": "LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "linux"
      ],
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@img/sharp-linux-arm": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-linux-arm/-/sharp-linux-arm-0.33.5.tgz",
      "integrity": "sha512-JTS1eldqZbJxjvKaAkxhZmBqPRGmxgu+qFKSInv8moZ2AmT5Yib3EQ1c6gp493HvrvV8QgdOXdyaIBrhvFhBMQ==",
      "cpu": [
        "arm"
      ],
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      },
      "optionalDependencies": {
        "@img/sharp-libvips-linux-arm": "1.0.5"
      }
    },
    "node_modules/@img/sharp-linux-arm64": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-linux-arm64/-/sharp-linux-arm64-0.33.5.tgz",
      "integrity": "sha512-JMVv+AMRyGOHtO1RFBiJy/MBsgz0x4AWrT6QoEVVTyh1E39TrCUpTRI7mx9VksGX4awWASxqCYLCV4wBZHAYxA==",
      "cpu": [
        "arm64"
      ],
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      },
      "optionalDependencies": {
        "@img/sharp-libvips-linux-arm64": "1.0.4"
      }
    },
    "node_modules/@img/sharp-linux-x64": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-linux-x64/-/sharp-linux-x64-0.33.5.tgz",
      "integrity": "sha512-opC+Ok5pRNAzuvq1AG0ar+1owsu842/Ab+4qvU879ippJBHvyY5n2mxF1izXqkPYlGuP/M556uh53jRLJmzTWA==",
      "cpu": [
        "x64"
      ],
      "license": "Apache-2.0",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      },
      "optionalDependencies": {
        "@img/sharp-libvips-linux-x64": "1.0.4"
      }
    },
    "node_modules/@img/sharp-win32-x64": {
      "version": "0.33.5",
      "resolved": "https://registry.npmjs.org/@img/sharp-win32-x64/-/sharp-win32-x64-0.33.5.tgz",
      "integrity": "sha512-MpY/o8/8kj+EcnxwvrP4aTJSWw/aZ7JIGR4aBeZkZw5B7/Jn+tY9/VNwtcoGmdT7GfggGIU4kygOMSbYnOrAbg==",
      "cpu": [
        "x64"
      ],
      "license": "Apache-2.0 AND LGPL-3.0-or-later",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": "^18.17.0 || ^20.3.0 || >=21.0.0"
      },
      "funding": {
        "url": "https://opencollective.com/libvips"
      }
    },
    "node_modules/@types/node": {
      "version": "24.10.1",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-24.10.1.tgz",
      "integrity": "sha512-GNWcUTRBgIRJD5zj+Tq0fKOJ5XZajIiBroOF0yvj2bSU1WvNdYS/dn9UxwsujGW4JX06dnHyjV2y9rRaybH0iQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "undici-types": "~7.16.0"
      }
    },
    "node_modules/@types/pdf-parse": {
      "version": "1.1.5",
      "resolved": "https://registry.npmjs.org/@types/pdf-parse/-/pdf-parse-1.1.5.tgz",
      "integrity": "sha512-kBfrSXsloMnUJOKi25s3+hRmkycHfLK6A09eRGqF/N8BkQoPUmaCr+q8Cli5FnfohEz/rsv82zAiPz/LXtOGhA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/agentfs-sdk": {
      "resolved": "../../../sdk/typescript",
      "link": true
    },
    "node_modules/dotenv": {
      "version": "16.6.1",
      "resolved": "https://registry.npmjs.org/dotenv/-/dotenv-16.6.1.tgz",
      "integrity": "sha512-uBq4egWHTcTt33a72vpSG0z3HnPuIl6NqYcTrKEg2azoEyl2hpW0zqlxysq2pK9HlDIHyHyakeYaYnSAwd8bow==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://dotenvx.com"
      }
    },
    "node_modules/esbuild": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.12.tgz",
      "integrity": "sha512-bbPBYYrtZbkt6Os6FiTLCTFxvq4tt3JKall1vRwshA3fdVztsLAatFaZobhkBC8/BrPetoa0oksYoKXoG4ryJg==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=18"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.25.12",
        "@esbuild/android-arm": "0.25.12",
        "@esbuild/android-arm64": "0.25.12",
        "@esbuild/android-x64": "0.25.12",
        "@esbuild/darwin-arm64": "0.25.12",
        "@esbuild/darwin-x64": "0.25.12",
        "@esbuild/freebsd-arm64": "0.25.12",
        "@esbuild/freebsd-x64": "0.25.12",
        "@esbuild/linux-arm": "0.25.12",
        "@esbuild/linux-arm64": "0.25.12",
        "@esbuild/linux-ia32": "0.25.12",
        "@esbuild/linux-loong64": "0.25.12",
        "@esbuild/linux-mips64el": "0.25.12",
        "@esbuild/linux-ppc64": "0.25.12",
        "@esbuild/linux-riscv64": "0.25.12",
        "@esbuild/linux-s390x": "0.25.12",
        "@esbuild/linux-x64": "0.25.12",
        "@esbuild/netbsd-arm64": "0.25.12",
        "@esbuild/netbsd-x64": "0.25.12",
        "@esbuild/openbsd-arm64": "0.25.12",
        "@esbuild/openbsd-x64": "0.25.12",
        "@esbuild/openharmony-arm64": "0.25.12",
        "@esbuild/sunos-x64": "0.25.12",
        "@esbuild/win32-arm64": "0.25.12",
        "@esbuild/win32-ia32": "0.25.12",
        "@esbuild/win32-x64": "0.25.12"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/get-tsconfig": {
      "version": "4.13.0",
      "resolved": "https://registry.npmjs.org/get-tsconfig/-/get-tsconfig-4.13.0.tgz",
      "integrity": "sha512-1VKTZJCwBrvbd+Wn3AOgQP/2Av+TfTCOlE4AcRJE72W1ksZXbAx8PPBR9RzgTeSPzlPMHrbANMH3LbltH73wxQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "resolve-pkg-maps": "^1.0.0"
      },
      "funding": {
        "url": "https://github.com/privatenumber/get-tsconfig?sponsor=1"
      }
    },
    "node_modules/node-ensure": {
      "version": "0.0.0",
      "resolved": "https://registry.npmjs.org/node-ensure/-/node-ensure-0.0.0.tgz",
      "integrity": "sha512-DRI60hzo2oKN1ma0ckc6nQWlHU69RH6xN0sjQTjMpChPfTYvKZdcQFfdYK2RWbJcKyUizSIy/l8OTGxMAM1QDw==",
      "license": "MIT"
    },
    "node_modules/pdf-parse": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/pdf-parse/-/pdf-parse-1.1.4.tgz",
      "integrity": "sha512-XRIRcLgk6ZnUbsHsYXExMw+krrPE81hJ6FQPLdBNhhBefqIQKXu/WeTgNBGSwPrfU0v+UCEwn7AoAUOsVKHFvQ==",
      "license": "MIT",
      "dependencies": {
        "node-ensure": "^0.0.0"
      },
      "engines": {
        "node": ">=6.8.1"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/mehmet-kozan"
      }
    },
    "node_modules/resolve-pkg-maps": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/resolve-pkg-maps/-/resolve-pkg-maps-1.0.0.tgz",
      "integrity": "sha512-seS2Tj26TBVOC2NIc2rOe2y2ZO7efxITtLZcGSOnHHNOQ7CkiUBfw0Iw2ck6xkIhPwLhKNLS8BO+hEpngQlqzw==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/privatenumber/resolve-pkg-maps?sponsor=1"
      }
    },
    "node_modules/tsx": {
      "version": "4.20.6",
      "resolved": "https://registry.npmjs.org/tsx/-/tsx-4.20.6.tgz",
      "integrity": "sha512-ytQKuwgmrrkDTFP4LjR0ToE2nqgy886GpvRSpU0JAnrdBYppuY5rLkRUYPU1yCryb24SsKBTL/hlDQAEFVwtZg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "esbuild": "~0.25.0",
        "get-tsconfig": "^4.7.5"
      },
      "bin": {
        "tsx": "dist/cli.mjs"
      },
      "engines": {
        "node": ">=18.0.0"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.3",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.9.3.tgz",
      "integrity": "sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/undici-types": {
      "version": "7.16.0",
      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-7.16.0.tgz",
      "integrity": "sha512-Zz+aZWSj8LE6zoxD+xrjh4VfkIG8Ya6LvYkZqtUQGJPZjYl53ypCaUwWqo7eI0x66KBGeRo+mlBEkMSeSZ38Nw==",
      "dev": true,
      "license": "MIT"
    },
    "node_modules/zod": {
      "version": "3.25.76",
      "resolved": "https://registry.npmjs.org/zod/-/zod-3.25.76.tgz",
      "integrity": "sha512-gzUt/qt81nXsFGKIFcC3YnfEAx5NkunCfnDlvuBSSFS02bcXu4Lmea0AFIUwbLWxWPx3d9p8S5QoaujKcNQxcQ==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    }
  }
}




------------------------------------------
File: examples/claude-agent/research-assistant/package.json
------------------------------------------

{
  "name": "research-assistant-claude-agent",
  "version": "1.0.0",
  "description": "A research assistant built with Claude Agent SDK that answers database research questions",
  "type": "module",
  "main": "dist/cli.js",
  "bin": {
    "research-assistant": "dist/cli.js"
  },
  "scripts": {
    "build": "tsc",
    "ask": "tsx src/cli.ts",
    "dev": "tsx src/cli.ts"
  },
  "keywords": [
    "claude",
    "agent",
    "research",
    "database",
    "ai"
  ],
  "author": "",
  "license": "ISC",
  "engines": {
    "node": ">=20.9.0"
  },
  "dependencies": {
    "@anthropic-ai/claude-agent-sdk": "^0.1.0",
    "agentfs-sdk": "file:../../../sdk/typescript",
    "dotenv": "^16.4.7",
    "pdf-parse": "^1.1.1",
    "zod": "^3.24.1"
  },
  "devDependencies": {
    "@types/node": "^24.10.0",
    "@types/pdf-parse": "^1.1.4",
    "tsx": "^4.19.1",
    "typescript": "^5.9.3"
  }
}




------------------------------------------
File: examples/claude-agent/research-assistant/README.md
------------------------------------------

# Research Assistant (VLDB + SIGMOD)

A minimal research assistant that answers database research questions by finding most relevant papers from the VLDB and SIGMOD conference proceedings.

## Getting Started

Install dependencies:

```console
npm install
```

Ask a research question:

```
npm run ask -- "How do learned indexes compare to B+ trees?"
```




------------------------------------------
File: examples/claude-agent/research-assistant/src/agent.ts
------------------------------------------

import { query, createSdkMcpServer } from '@anthropic-ai/claude-agent-sdk';
import { semanticScholarTool } from './tools/semantic-tool.js';
import { pdfFetchTool, resolvePdfUrlTool } from './tools/pdf-tool.js';
import { getAgentFS } from './utils/agentfs.js';

const AGENT_INSTRUCTIONS = `
You are a database research expert who provides comprehensive, evidence-based analysis.

When a user asks a research question:

1. **Search for papers**: Use the 'search-semanticscholar-vldb-sigmod' tool with the user's query to find relevant papers from VLDB and SIGMOD venues.

2. **Get full paper content**: For the most relevant papers (typically 3-5 papers):
   - If a paper has a pdfUrl, use the 'fetch-pdf-text' tool to get the complete paper text
   - If a paper only has a regular URL, first try 'resolve-pdf-url' to find the PDF link, then fetch it
   - Read the FULL TEXT carefully to extract specific findings, numbers, and comparisons
   - If PDF fetching fails, work with the abstract, but note the limitation

3. **Analyze the content**: Review the full paper texts, focusing on:
   - Key findings and performance metrics (extract specific numbers!)
   - Comparisons and benchmarks between systems
   - Trade-offs and limitations discussed by authors
   - Theoretical and practical insights
   - Methodologies and experimental setups

4. **Synthesize a comprehensive answer** that:
   - Directly answers the research question with specific evidence from the papers
   - Includes concrete performance numbers, comparisons, and trade-offs
   - Cites sources using the format (FirstAuthor, Year) - e.g., (Marcus, 2020), (Liu, 2024)
   - Uses semicolons for multiple citations: (Author1, Year1; Author2, Year2)
   - Highlights areas of agreement or disagreement between papers
   - Organizes findings into clear sections with headers like "Key Findings", "Performance Metrics", "Trade-offs"
   - Mentions when papers show different results or conflicting evidence

5. **Include a references section** at the end with full citations in this format:
   - AuthorLastName et al. (Year). Paper Title — Venue Year
   - URL

CRITICAL RULES:
- DO NOT just list papers. ANSWER the question with evidence!
- DO NOT give superficial summaries. Include SPECIFIC DETAILS from the papers!
- ALWAYS fetch and read full PDFs for thorough analysis - abstracts alone are insufficient!
- Include ACTUAL NUMBERS and metrics from the papers, not vague descriptions!
- Be thorough, accurate, and evidence-based in your analysis!
`;

// Create an MCP server with our custom tools
const toolServer = createSdkMcpServer({
  name: 'research-tools',
  version: '1.0.0',
  tools: [semanticScholarTool, pdfFetchTool, resolvePdfUrlTool],
});

export async function runResearchAgent(question: string): Promise<string> {
  // Initialize AgentFS
  await getAgentFS();

  let fullResponse = '';

  // Run the agent with the Claude Agent SDK query function
  const result = query({
    prompt: question,
    options: {
      model: 'sonnet',
      systemPrompt: AGENT_INSTRUCTIONS,
      mcpServers: {
        'research-tools': toolServer,
      },
      maxTurns: 50,
      permissionMode: 'bypassPermissions',
    },
  });

  // Stream messages and collect the final response
  for await (const message of result) {
    if (message.type === 'assistant') {
      // Collect text from assistant messages
      for (const content of message.message.content) {
        if (content.type === 'text') {
          fullResponse += content.text;
        }
      }
    } else if (message.type === 'result') {
      // Final result message
      break;
    }
  }

  return fullResponse;
}




------------------------------------------
File: examples/claude-agent/research-assistant/src/cli.ts
------------------------------------------

#!/usr/bin/env node
import 'dotenv/config';
import { runResearchAgent } from './agent.js';

async function main() {
  const args = process.argv.slice(2);
  const question = args.join(' ').trim();

  if (!question) {
    console.error('Usage: npm run ask -- "<your research question>"');
    process.exit(1);
  }

  try {
    console.log('Researching your question...\n');

    const response = await runResearchAgent(question);

    console.log(response);
  } catch (err: any) {
    console.error('Error:', err?.message || err);
    process.exit(1);
  }
}

main();




------------------------------------------
File: examples/claude-agent/research-assistant/src/tools/pdf-tool.ts
------------------------------------------

import { tool } from '@anthropic-ai/claude-agent-sdk';
import { z } from 'zod';
import { fetchPdfTextCached, resolvePdfUrl } from '../utils/pdf.js';
import { getAgentFS } from '../utils/agentfs.js';

export const pdfFetchTool = tool(
  'fetch-pdf-text',
  'Fetches and parses full text content from a research paper PDF URL. Use this when you need the complete paper content for detailed analysis beyond the abstract.',
  {
    pdfUrl: z.string().url().describe('Direct URL to the PDF file'),
    paperTitle: z.string().describe('Title of the paper (used for caching)'),
    year: z.number().describe('Publication year (used for caching)'),
  },
  async ({ pdfUrl, paperTitle, year }) => {
    // Generate cache key from title and year
    const cacheKey = `${paperTitle}-${year}`
      .toLowerCase()
      .replace(/[^a-z0-9-_]+/g, '-')
      .replace(/-+/g, '-')
      .replace(/^-|-$/g, '')
      .slice(0, 150);

    const agentfs = await getAgentFS();

    // Check if already cached
    const txtPath = `/papers/${cacheKey}.txt`;
    try {
      const cached = await agentfs.fs.readFile(txtPath);
      if (cached && cached.length > 300) {
        return {
          content: [{
            type: 'text' as const,
            text: JSON.stringify({ text: cached, cached: true }, null, 2),
          }],
        };
      }
    } catch {
      // Not cached, proceed to fetch
    }

    // Fetch and parse PDF
    const text = await fetchPdfTextCached(cacheKey, pdfUrl);

    if (!text || text.length < 300) {
      throw new Error('PDF text extraction failed or returned insufficient content');
    }

    return {
      content: [{
        type: 'text' as const,
        text: JSON.stringify({ text, cached: false }, null, 2),
      }],
    };
  }
);

export const resolvePdfUrlTool = tool(
  'resolve-pdf-url',
  'Attempts to resolve a landing page URL (like a DOI or ACM DL page) to a direct PDF URL. Use this when you have a paper URL but need the PDF link.',
  {
    url: z.string().url().describe('Landing page URL for the paper'),
  },
  async ({ url }) => {
    const pdfUrl = await resolvePdfUrl(url);

    return {
      content: [{
        type: 'text' as const,
        text: JSON.stringify({
          pdfUrl: pdfUrl || undefined,
          success: !!pdfUrl,
        }, null, 2),
      }],
    };
  }
);




------------------------------------------
File: examples/claude-agent/research-assistant/src/tools/semantic-tool.ts
------------------------------------------

import { tool } from '@anthropic-ai/claude-agent-sdk';
import { z } from 'zod';

export const paperSchema = z.object({
  paperId: z.string(),
  title: z.string(),
  authors: z.array(z.string()),
  venue: z.string(),
  year: z.number(),
  url: z.string().url(),
  abstractText: z.string().optional(),
  pdfUrl: z.string().url().optional(),
});
export type Paper = z.infer<typeof paperSchema>;

type S2Author = { name?: string };
type S2Venue = { displayName?: string } | null;
type S2ExternalIds = { DOI?: string | null };
type S2Paper = {
  paperId?: string;
  title?: string;
  year?: number;
  authors?: S2Author[];
  url?: string | null;
  abstract?: string | null;
  venue?: string | null;
  publicationVenue?: S2Venue;
  externalIds?: S2ExternalIds;
  isOpenAccess?: boolean;
  openAccessPdf?: { url?: string | null } | null;
};

type S2SearchResponse = {
  data?: S2Paper[];
  total?: number;
};

function isTargetVenue(venue?: string | null): boolean {
  if (!venue) return false;
  const v = venue.toLowerCase();
  return (
    v.includes('sigmod') ||
    v.includes('acm sigmod') ||
    v.includes('sigmod conference') ||
    v.includes('proceedings of the vldb endowment') ||
    v.includes('pvl db') ||
    v.includes('pvlb') ||
    v.includes('pvlbd') ||
    v.includes('vldb') // keep broad to catch PVLDB; risk: VLDB Journal may appear
  );
}

async function sleep(ms: number) {
  return new Promise((r) => setTimeout(r, ms));
}

async function fetchWithBackoff(url: string, init: RequestInit, maxRetries = 5, initialDelayMs = 500): Promise<Response> {
  let attempt = 0;
  let delay = initialDelayMs;
  while (true) {
    const res = await fetch(url, init);
    if (res.status !== 429 && res.status < 500) {
      return res;
    }
    if (attempt >= maxRetries) {
      return res;
    }
    // Prefer Retry-After header if present
    const retryAfter = res.headers.get('Retry-After');
    let waitMs = delay;
    if (retryAfter) {
      const secs = Number(retryAfter);
      if (!Number.isNaN(secs) && secs > 0) waitMs = secs * 1000;
    }
    await sleep(waitMs);
    attempt += 1;
    delay = Math.min(delay * 2, 8000);
  }
}

export async function searchSemanticScholar(query: string): Promise<Paper[]> {
  const base = 'https://api.semanticscholar.org/graph/v1/paper/search';
  const params = new URLSearchParams({
    query,
    limit: '100',
    fields: 'title,venue,publicationVenue,year,authors,url,abstract,externalIds,isOpenAccess,openAccessPdf',
  });
  const url = `${base}?${params.toString()}`;
  const res = await fetchWithBackoff(
    url,
    {
      headers: {
        Accept: 'application/json',
        'User-Agent': 'research-assistant',
      },
    },
    5,
    500,
  );
  if (!res.ok) {
    throw new Error(`Semantic Scholar request failed with status ${res.status}`);
  }
  const data = (await res.json()) as S2SearchResponse;
  const arr = data.data || [];
  const mapped: Paper[] = arr
    .map((p) => {
      const title = (p.title || '').trim();
      const authors = (p.authors || []).map((a) => (a.name || '').trim()).filter(Boolean);
      const venue =
        (p.publicationVenue?.displayName || p.venue || '').toString().trim();
      const year = Number(p.year || 0);
      const doi = p.externalIds?.DOI || null;
      const rawUrl = (doi ? `https://doi.org/${doi}` : p.url || '').toString().trim();
      const url = rawUrl;
      const paperId = (doi || p.paperId || url || title).toString();
      const abstractText = (p.abstract || undefined)?.toString();
      let pdfUrl =
        (p.openAccessPdf?.url || undefined) ||
        (doi && doi.toLowerCase().startsWith('10.48550/arxiv.')
          ? `https://arxiv.org/pdf/${doi.toLowerCase().split('arxiv.')[1]}.pdf`
          : undefined);
      // Fallback: if the URL is an arXiv abstract, derive the PDF URL
      try {
        const u = new URL(url);
        if (u.host.includes('arxiv.org')) {
          const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
          if (absMatch && absMatch[1]) {
            pdfUrl = `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
          } else {
            const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
            if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
              pdfUrl = `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
            }
          }
        }
      } catch {
        // ignore URL parse errors
      }
      return {
        paperId,
        title,
        authors,
        venue,
        year,
        url,
        abstractText,
        pdfUrl,
      } as Paper;
    })
    .filter((p) => p.title && p.url);
  // Filter to target venues and dedupe by URL
  const filtered = mapped.filter((p) => isTargetVenue(p.venue));
  const seen = new Set<string>();
  const deduped = filtered.filter((p) => {
    const key = p.url;
    if (seen.has(key)) return false;
    seen.add(key);
    return true;
  });
  return deduped;
}

export const semanticScholarTool = tool(
  'search-semanticscholar-vldb-sigmod',
  'Search Semantic Scholar for papers relevant to a query, constrained to VLDB (PVLDB) and SIGMOD venues',
  {
    query: z.string().describe('User research question or keywords'),
  },
  async ({ query }) => {
    const papers = await searchSemanticScholar(query);
    return {
      content: [{
        type: 'text' as const,
        text: JSON.stringify({ papers }, null, 2),
      }],
    };
  }
);




------------------------------------------
File: examples/claude-agent/research-assistant/src/utils/agentfs.ts
------------------------------------------

import { AgentFS } from 'agentfs-sdk';

let instance: AgentFS | null = null;

export async function getAgentFS(): Promise<AgentFS> {
  if (!instance) {
    const id = process.env.AGENTFS_ID || 'research-assistant';
    instance = await AgentFS.open({ id });
  }
  return instance;
}




------------------------------------------
File: examples/claude-agent/research-assistant/src/utils/pdf.ts
------------------------------------------

import pdfParse from 'pdf-parse';
import { getAgentFS } from './agentfs.js';

async function fetchHtmlWithFallback(url: string): Promise<string> {
  const res = await fetch(url, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      'Accept-Language': 'en-US,en;q=0.9',
      Referer: url,
    },
  });
  const text = await res.text();
  if (!res.ok || /Access Denied|Robot Check|unusual traffic/i.test(text)) {
    try {
      const u = new URL(url);
      const proxy = `https://r.jina.ai/http://${u.host}${u.pathname}${u.search || ''}`;
      const proxRes = await fetch(proxy, { headers: { Accept: 'text/plain' } });
      if (proxRes.ok) {
        return await proxRes.text();
      }
    } catch {
      // ignore
    }
  }
  return text;
}

export async function fetchPdfText(pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  const parsed = await pdfParse(buf);
  return parsed.text || '';
}

export async function fetchPdfTextCached(cacheKey: string, pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const fs = (await getAgentFS()).fs;
  const safeKey = cacheKey.replace(/[^a-z0-9-_]/gi, '_').toLowerCase().slice(0, 200);
  const pdfPath = `/papers/${safeKey}.pdf`;
  const txtPath = `/papers/${safeKey}.txt`;
  // Try text cache
  try {
    const text = (await fs.readFile(txtPath, 'utf8')) as string;
    if (text && text.length > 0) return text;
  } catch {
    // not cached
  }
  // Download PDF
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
      Referer: pdfUrl,
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  // Cache PDF
  try {
    await fs.writeFile(pdfPath, buf);
  } catch {
    // ignore cache write error
  }
  const parsed = await pdfParse(buf);
  const text = parsed.text || '';
  // Cache text
  try {
    await fs.writeFile(txtPath, text);
  } catch {
    // ignore
  }
  return text;
}

export function clipText(text: string, maxChars = 8000): string {
  if (text.length <= maxChars) return text;
  return text.slice(0, maxChars);
}

export async function resolvePdfUrl(landingUrl: string): Promise<string | undefined> {
  try {
    const u = new URL(landingUrl);
    // Direct PDF link
    if (/\.(pdf)(\?|#|$)/i.test(u.pathname)) {
      return landingUrl;
    }
    // arXiv: handle abs -> pdf and pdf without extension
    if (u.host.includes('arxiv.org')) {
      const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
      if (absMatch && absMatch[1]) {
        return `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
      }
      const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
      if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
        return `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
      }
    }
    // ACM DL DOI landing page
    if (u.host.includes('dl.acm.org') && /\/doi\//.test(u.pathname)) {
      const html = await fetchHtmlWithFallback(landingUrl);
      // 1) meta citation_pdf_url
      const meta = html.match(/<meta\s+name=["']citation_pdf_url["']\s+content=["']([^"']+)["']/i);
      if (meta?.[1]) {
        const abs = new URL(meta[1], landingUrl).toString();
        return abs;
      }
      // 2) explicit /doi/pdf or /doi/pdfdirect links
      const m = html.match(/href=["'](\/doi\/pdf(?:direct)?\/[^"']+)["']/i);
      if (m?.[1]) {
        const abs = new URL(m[1], landingUrl).toString();
        return abs;
      }
      // 3) try OpenGraph pdf link
      const og = html.match(/<meta\s+property=["']og:url["']\s+content=["']([^"']+)["']/i);
      if (og?.[1] && /\/doi\/pdf/.test(og[1])) {
        return new URL(og[1], landingUrl).toString();
      }
    }
    // DOI resolver: if arXiv DOI, map to arXiv pdf
    if (u.host.includes('doi.org')) {
      const arxivId = landingUrl.match(/10\.48550\/arXiv\.(\d+\.\d+)(v\d+)?/i);
      if (arxivId?.[1]) {
        return `https://arxiv.org/pdf/${arxivId[1]}.pdf`;
      }
    }
  } catch {
    // ignore
  }
  return undefined;
}




------------------------------------------
File: examples/claude-agent/research-assistant/tsconfig.json
------------------------------------------

{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "lib": ["ES2023"],
    "moduleResolution": "node",
    "esModuleInterop": true,
    "skipLibCheck": true,
    "strict": true,
    "resolveJsonModule": true,
    "outDir": "./dist",
    "rootDir": "./src",
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}




------------------------------------------
File: examples/mastra/research-assistant/cli/ask.ts
------------------------------------------

#!/usr/bin/env node
import 'dotenv/config';
import { mastra } from '../src/mastra/index.js';
import { getAgentFS } from '../src/mastra/utils/agentfs.js';

async function main() {
  const args = process.argv.slice(2);
  const question = args.join(' ').trim();

  if (!question) {
    console.error('Usage: npm run ask -- "<your research question>"');
    process.exit(1);
  }

  try {
    // Initialize agentfs
    const agentfs = await getAgentFS();

    // Get the agent
    const agent = mastra.getAgent('researchAgent');
    if (!agent) {
      console.error('Research agent not available.');
      process.exit(1);
    }

    // Simply ask the agent - it has all the logic and tools it needs
    const response = await agent.generate([
      {
        role: 'user',
        content: question,
      },
    ]);

    // Output the agent's response
    console.log(response.text);

  } catch (err: any) {
    console.error('Error:', err?.message || err);
    process.exit(1);
  }
}

main();






------------------------------------------
File: examples/mastra/research-assistant/package.json
------------------------------------------

{
  "name": "research-assistant",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "mastra dev",
    "build": "mastra build",
    "start": "mastra start",
    "ask": "tsx cli/ask.ts"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "type": "module",
  "engines": {
    "node": ">=20.9.0"
  },
  "dependencies": {
    "@mastra/core": "^0.24.0",
    "@mastra/evals": "^0.14.3",
    "@mastra/libsql": "^0.16.2",
    "@mastra/loggers": "^0.10.19",
    "@mastra/memory": "^0.15.11",
    "agentfs-sdk": "file:../../../sdk/typescript",
    "pdf-parse": "^1.1.1",
    "zod": "^4.1.12"
  },
  "devDependencies": {
    "@types/node": "^24.10.0",
    "tsx": "^4.19.1",
    "mastra": "^0.18.0",
    "typescript": "^5.9.3"
  }
}




------------------------------------------
File: examples/mastra/research-assistant/README.md
------------------------------------------

# Research Assistant (VLDB + SIGMOD)

A minimal research assistant that answers database research questions by finding most relevant papers from the VLDB and SIGMOD conference proceedings.

## Getting Started

Install dependencies:

```console
npm install
```

Ask a research question:

```
npm run ask -- "How do learned indexes compare to B+ trees?"
```




------------------------------------------
File: examples/mastra/research-assistant/src/mastra/agents/research-agent.ts
------------------------------------------

import { Agent } from '@mastra/core/agent';
import { Memory } from '@mastra/memory';
import { LibSQLStore } from '@mastra/libsql';
import { semanticScholarTool } from '../tools/semantic-tool';
import { pdfFetchTool, resolvePdfUrlTool } from '../tools/pdf-tool';

export const researchAgent = new Agent({
  name: 'Research Assistant',
  instructions: `
You are a database research expert who provides comprehensive, evidence-based analysis.

When a user asks a research question:

1. **Search for papers**: Use the 'search-semanticscholar-vldb-sigmod' tool with the user's query to find relevant papers from VLDB and SIGMOD venues.

2. **Get full paper content**: For the most relevant papers (typically 3-5 papers):
   - If a paper has a pdfUrl, use the 'fetch-pdf-text' tool to get the complete paper text
   - If a paper only has a regular URL, first try 'resolve-pdf-url' to find the PDF link, then fetch it
   - Read the FULL TEXT carefully to extract specific findings, numbers, and comparisons
   - If PDF fetching fails, work with the abstract, but note the limitation

3. **Analyze the content**: Review the full paper texts, focusing on:
   - Key findings and performance metrics (extract specific numbers!)
   - Comparisons and benchmarks between systems
   - Trade-offs and limitations discussed by authors
   - Theoretical and practical insights
   - Methodologies and experimental setups

4. **Synthesize a comprehensive answer** that:
   - Directly answers the research question with specific evidence from the papers
   - Includes concrete performance numbers, comparisons, and trade-offs
   - Cites sources using the format (FirstAuthor, Year) - e.g., (Marcus, 2020), (Liu, 2024)
   - Uses semicolons for multiple citations: (Author1, Year1; Author2, Year2)
   - Highlights areas of agreement or disagreement between papers
   - Organizes findings into clear sections with headers like "Key Findings", "Performance Metrics", "Trade-offs"
   - Mentions when papers show different results or conflicting evidence

5. **Include a references section** at the end with full citations in this format:
   - AuthorLastName et al. (Year). Paper Title — Venue Year
   - URL

CRITICAL RULES:
- DO NOT just list papers. ANSWER the question with evidence!
- DO NOT give superficial summaries. Include SPECIFIC DETAILS from the papers!
- ALWAYS fetch and read full PDFs for thorough analysis - abstracts alone are insufficient!
- Include ACTUAL NUMBERS and metrics from the papers, not vague descriptions!
- Be thorough, accurate, and evidence-based in your analysis!
`,
  model: 'openai/gpt-4o-mini',
  tools: { semanticScholarTool, pdfFetchTool, resolvePdfUrlTool },
  memory: new Memory({
    storage: new LibSQLStore({
      url: 'file:../mastra.db',
    }),
  }),
});






------------------------------------------
File: examples/mastra/research-assistant/src/mastra/index.ts
------------------------------------------


import { Mastra } from '@mastra/core/mastra';
import { PinoLogger } from '@mastra/loggers';
import { LibSQLStore } from '@mastra/libsql';
import { researchWorkflow } from './workflows/research-workflow';
import { researchAgent } from './agents/research-agent';

export const mastra = new Mastra({
  workflows: { researchWorkflow },
  agents: { researchAgent },
  scorers: {},
  storage: new LibSQLStore({
    // stores observability, scores, ... into memory storage, if it needs to persist, change to file:../mastra.db
    url: ":memory:",
  }),
  logger: new PinoLogger({
    name: 'Mastra',
    level: 'info',
  }),
  telemetry: {
    // Telemetry is deprecated and will be removed in the Nov 4th release
    enabled: false, 
  },
  observability: {
    // Enables DefaultExporter and CloudExporter for AI tracing
    default: { enabled: true }, 
  },
});




------------------------------------------
File: examples/mastra/research-assistant/src/mastra/scorers/lexical-scorer.ts
------------------------------------------

export type RankablePaper = {
  paperId: string;
  title: string;
  authors: string[];
  venue: string;
  year: number;
  url: string;
  abstractText?: string;
};

function tokenize(text: string): string[] {
  return text
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, ' ')
    .split(/\s+/)
    .filter(Boolean);
}

function unique<T>(arr: T[]): T[] {
  return Array.from(new Set(arr));
}

export function computeLexicalScore(query: string, paper: RankablePaper): number {
  const queryTokens = unique(tokenize(query));
  const hay = [paper.title, paper.abstractText || ''].join(' ');
  const docTokens = tokenize(hay);

  if (docTokens.length === 0 || queryTokens.length === 0) return 0;

  // Simple term overlap weighted by term frequency in the document
  const freq: Record<string, number> = {};
  for (const t of docTokens) {
    freq[t] = (freq[t] || 0) + 1;
  }
  let score = 0;
  for (const qt of queryTokens) {
    if (freq[qt]) score += 1 + Math.log(1 + freq[qt]); // mild TF weight
  }

  // Small recency boost
  const currentYear = new Date().getFullYear();
  let recency = 0;
  if (paper.year && paper.year >= 1970 && paper.year <= currentYear + 1) {
    const age = currentYear - paper.year;
    recency = Math.max(0, 1 - Math.min(10, age) / 10); // 0..1
  }
  score += 0.1 * recency;

  return score;
}

export function rankByLexicalScore(query: string, papers: RankablePaper[], k = 5) {
  const withScores = papers.map((p) => ({
    paper: p,
    score: computeLexicalScore(query, p),
  }));
  withScores.sort((a, b) => {
    if (b.score !== a.score) return b.score - a.score;
    if (b.paper.year !== a.paper.year) return b.paper.year - a.paper.year; // recent first
    return a.paper.title.localeCompare(b.paper.title);
  });
  return withScores.slice(0, k);
}






------------------------------------------
File: examples/mastra/research-assistant/src/mastra/tools/pdf-tool.ts
------------------------------------------

import { createTool } from '@mastra/core/tools';
import { z } from 'zod';
import { fetchPdfTextCached, resolvePdfUrl } from '../utils/pdf.js';
import { getAgentFS } from '../utils/agentfs.js';

export const pdfFetchTool = createTool({
  id: 'fetch-pdf-text',
  description:
    'Fetches and parses full text content from a research paper PDF URL. Use this when you need the complete paper content for detailed analysis beyond the abstract.',
  inputSchema: z.object({
    pdfUrl: z.string().url().describe('Direct URL to the PDF file'),
    paperTitle: z.string().describe('Title of the paper (used for caching)'),
    year: z.number().describe('Publication year (used for caching)'),
  }),
  outputSchema: z.object({
    text: z.string().describe('Full text content extracted from the PDF'),
    cached: z.boolean().describe('Whether the content was retrieved from cache'),
  }),
  execute: async ({ context }) => {
    const { pdfUrl, paperTitle, year } = context;

    // Generate cache key from title and year
    const cacheKey = `${paperTitle}-${year}`
      .toLowerCase()
      .replace(/[^a-z0-9-_]+/g, '-')
      .replace(/-+/g, '-')
      .replace(/^-|-$/g, '')
      .slice(0, 150);

    const agentfs = await getAgentFS();

    // Check if already cached
    const txtPath = `/papers/${cacheKey}.txt`;
    try {
      const cached = await agentfs.fs.readFile(txtPath);
      if (cached && cached.length > 300) {
        return { text: cached, cached: true };
      }
    } catch {
      // Not cached, proceed to fetch
    }

    // Fetch and parse PDF
    const text = await fetchPdfTextCached(cacheKey, pdfUrl);

    if (!text || text.length < 300) {
      throw new Error('PDF text extraction failed or returned insufficient content');
    }

    return { text, cached: false };
  },
});

export const resolvePdfUrlTool = createTool({
  id: 'resolve-pdf-url',
  description:
    'Attempts to resolve a landing page URL (like a DOI or ACM DL page) to a direct PDF URL. Use this when you have a paper URL but need the PDF link.',
  inputSchema: z.object({
    url: z.string().url().describe('Landing page URL for the paper'),
  }),
  outputSchema: z.object({
    pdfUrl: z.string().url().optional().describe('Resolved direct PDF URL, if found'),
    success: z.boolean().describe('Whether PDF URL resolution succeeded'),
  }),
  execute: async ({ context }) => {
    const { url } = context;
    const pdfUrl = await resolvePdfUrl(url);

    return {
      pdfUrl: pdfUrl || undefined,
      success: !!pdfUrl,
    };
  },
});




------------------------------------------
File: examples/mastra/research-assistant/src/mastra/tools/semantic-tool.ts
------------------------------------------

import { createTool } from '@mastra/core/tools';
import { z } from 'zod';

export const paperSchema = z.object({
  paperId: z.string(),
  title: z.string(),
  authors: z.array(z.string()),
  venue: z.string(),
  year: z.number(),
  url: z.string().url(),
  abstractText: z.string().optional(),
  pdfUrl: z.string().url().optional(),
});
export type Paper = z.infer<typeof paperSchema>;

type S2Author = { name?: string };
type S2Venue = { displayName?: string } | null;
type S2ExternalIds = { DOI?: string | null };
type S2Paper = {
  paperId?: string;
  title?: string;
  year?: number;
  authors?: S2Author[];
  url?: string | null;
  abstract?: string | null;
  venue?: string | null;
  publicationVenue?: S2Venue;
  externalIds?: S2ExternalIds;
  isOpenAccess?: boolean;
  openAccessPdf?: { url?: string | null } | null;
};

type S2SearchResponse = {
  data?: S2Paper[];
  total?: number;
};

function isTargetVenue(venue?: string | null): boolean {
  if (!venue) return false;
  const v = venue.toLowerCase();
  return (
    v.includes('sigmod') ||
    v.includes('acm sigmod') ||
    v.includes('sigmod conference') ||
    v.includes('proceedings of the vldb endowment') ||
    v.includes('pvl db') ||
    v.includes('pvlb') ||
    v.includes('pvlbd') ||
    v.includes('vldb') // keep broad to catch PVLDB; risk: VLDB Journal may appear
  );
}

async function sleep(ms: number) {
  return new Promise((r) => setTimeout(r, ms));
}

async function fetchWithBackoff(url: string, init: RequestInit, maxRetries = 5, initialDelayMs = 500): Promise<Response> {
  let attempt = 0;
  let delay = initialDelayMs;
  while (true) {
    const res = await fetch(url, init);
    if (res.status !== 429 && res.status < 500) {
      return res;
    }
    if (attempt >= maxRetries) {
      return res;
    }
    // Prefer Retry-After header if present
    const retryAfter = res.headers.get('Retry-After');
    let waitMs = delay;
    if (retryAfter) {
      const secs = Number(retryAfter);
      if (!Number.isNaN(secs) && secs > 0) waitMs = secs * 1000;
    }
    await sleep(waitMs);
    attempt += 1;
    delay = Math.min(delay * 2, 8000);
  }
}

export async function searchSemanticScholar(query: string): Promise<Paper[]> {
  const base = 'https://api.semanticscholar.org/graph/v1/paper/search';
  const params = new URLSearchParams({
    query,
    limit: '100',
    fields: 'title,venue,publicationVenue,year,authors,url,abstract,externalIds,isOpenAccess,openAccessPdf',
  });
  const url = `${base}?${params.toString()}`;
  const res = await fetchWithBackoff(
    url,
    {
      headers: {
        Accept: 'application/json',
        'User-Agent': 'research-assistant',
      },
    },
    5,
    500,
  );
  if (!res.ok) {
    throw new Error(`Semantic Scholar request failed with status ${res.status}`);
  }
  const data = (await res.json()) as S2SearchResponse;
  const arr = data.data || [];
  const mapped: Paper[] = arr
    .map((p) => {
      const title = (p.title || '').trim();
      const authors = (p.authors || []).map((a) => (a.name || '').trim()).filter(Boolean);
      const venue =
        (p.publicationVenue?.displayName || p.venue || '').toString().trim();
      const year = Number(p.year || 0);
      const doi = p.externalIds?.DOI || null;
      const rawUrl = (doi ? `https://doi.org/${doi}` : p.url || '').toString().trim();
      const url = rawUrl;
      const paperId = (doi || p.paperId || url || title).toString();
      const abstractText = (p.abstract || undefined)?.toString();
      let pdfUrl =
        (p.openAccessPdf?.url || undefined) ||
        (doi && doi.toLowerCase().startsWith('10.48550/arxiv.')
          ? `https://arxiv.org/pdf/${doi.toLowerCase().split('arxiv.')[1]}.pdf`
          : undefined);
      // Fallback: if the URL is an arXiv abstract, derive the PDF URL
      try {
        const u = new URL(url);
        if (u.host.includes('arxiv.org')) {
          const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
          if (absMatch && absMatch[1]) {
            pdfUrl = `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
          } else {
            const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
            if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
              pdfUrl = `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
            }
          }
        }
      } catch {
        // ignore URL parse errors
      }
      return {
        paperId,
        title,
        authors,
        venue,
        year,
        url,
        abstractText,
        pdfUrl,
      } as Paper;
    })
    .filter((p) => p.title && p.url);
  // Filter to target venues and dedupe by URL
  const filtered = mapped.filter((p) => isTargetVenue(p.venue));
  const seen = new Set<string>();
  const deduped = filtered.filter((p) => {
    const key = p.url;
    if (seen.has(key)) return false;
    seen.add(key);
    return true;
  });
  return deduped;
}

export const semanticScholarTool = createTool({
  id: 'search-semanticscholar-vldb-sigmod',
  description:
    'Search Semantic Scholar for papers relevant to a query, constrained to VLDB (PVLDB) and SIGMOD venues',
  inputSchema: z.object({
    query: z.string().describe('User research question or keywords'),
  }),
  outputSchema: z.object({
    papers: z.array(paperSchema),
  }),
  execute: async ({ context }) => {
    const papers = await searchSemanticScholar(context.query);
    return { papers };
  },
});






------------------------------------------
File: examples/mastra/research-assistant/src/mastra/utils/agentfs.ts
------------------------------------------

import { AgentFS } from 'agentfs-sdk';

let instance: AgentFS | null = null;

export async function getAgentFS(): Promise<AgentFS> {
  if (!instance) {
    const id = process.env.AGENTFS_ID || 'research-assistant';
    instance = await AgentFS.open({ id });
  }
  return instance;
}






------------------------------------------
File: examples/mastra/research-assistant/src/mastra/utils/pdf.ts
------------------------------------------

import pdfParse from 'pdf-parse';
import { getAgentFS } from './agentfs.js';

async function fetchHtmlWithFallback(url: string): Promise<string> {
  const res = await fetch(url, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      'Accept-Language': 'en-US,en;q=0.9',
      Referer: url,
    },
  });
  const text = await res.text();
  if (!res.ok || /Access Denied|Robot Check|unusual traffic/i.test(text)) {
    try {
      const u = new URL(url);
      const proxy = `https://r.jina.ai/http://${u.host}${u.pathname}${u.search || ''}`;
      const proxRes = await fetch(proxy, { headers: { Accept: 'text/plain' } });
      if (proxRes.ok) {
        return await proxRes.text();
      }
    } catch {
      // ignore
    }
  }
  return text;
}

export async function fetchPdfText(pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  const parsed = await pdfParse(buf);
  return parsed.text || '';
}

export async function fetchPdfTextCached(cacheKey: string, pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const fs = (await getAgentFS()).fs;
  const safeKey = cacheKey.replace(/[^a-z0-9-_]/gi, '_').toLowerCase().slice(0, 200);
  const pdfPath = `/papers/${safeKey}.pdf`;
  const txtPath = `/papers/${safeKey}.txt`;
  // Try text cache
  try {
    const text = (await fs.readFile(txtPath, 'utf8')) as string;
    if (text && text.length > 0) return text;
  } catch {
    // not cached
  }
  // Download PDF
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
      Referer: pdfUrl,
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  // Cache PDF
  try {
    await fs.writeFile(pdfPath, buf);
  } catch {
    // ignore cache write error
  }
  const parsed = await pdfParse(buf);
  const text = parsed.text || '';
  // Cache text
  try {
    await fs.writeFile(txtPath, text);
  } catch {
    // ignore
  }
  return text;
}

export function clipText(text: string, maxChars = 8000): string {
  if (text.length <= maxChars) return text;
  return text.slice(0, maxChars);
}

export async function resolvePdfUrl(landingUrl: string): Promise<string | undefined> {
  try {
    const u = new URL(landingUrl);
    // Direct PDF link
    if (/\.(pdf)(\?|#|$)/i.test(u.pathname)) {
      return landingUrl;
    }
    // arXiv: handle abs -> pdf and pdf without extension
    if (u.host.includes('arxiv.org')) {
      const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
      if (absMatch && absMatch[1]) {
        return `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
      }
      const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
      if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
        return `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
      }
    }
    // ACM DL DOI landing page
    if (u.host.includes('dl.acm.org') && /\/doi\//.test(u.pathname)) {
      const html = await fetchHtmlWithFallback(landingUrl);
      // 1) meta citation_pdf_url
      const meta = html.match(/<meta\s+name=["']citation_pdf_url["']\s+content=["']([^"']+)["']/i);
      if (meta?.[1]) {
        const abs = new URL(meta[1], landingUrl).toString();
        return abs;
      }
      // 2) explicit /doi/pdf or /doi/pdfdirect links
      const m = html.match(/href=["'](\/doi\/pdf(?:direct)?\/[^"']+)["']/i);
      if (m?.[1]) {
        const abs = new URL(m[1], landingUrl).toString();
        return abs;
      }
      // 3) try OpenGraph pdf link
      const og = html.match(/<meta\s+property=["']og:url["']\s+content=["']([^"']+)["']/i);
      if (og?.[1] && /\/doi\/pdf/.test(og[1])) {
        return new URL(og[1], landingUrl).toString();
      }
    }
    // DOI resolver: if arXiv DOI, map to arXiv pdf
    if (u.host.includes('doi.org')) {
      const arxivId = landingUrl.match(/10\.48550\/arXiv\.(\d+\.\d+)(v\d+)?/i);
      if (arxivId?.[1]) {
        return `https://arxiv.org/pdf/${arxivId[1]}.pdf`;
      }
    }
  } catch {
    // ignore
  }
  return undefined;
}






------------------------------------------
File: examples/mastra/research-assistant/src/mastra/workflows/research-workflow.ts
------------------------------------------

import { createStep, createWorkflow } from '@mastra/core/workflows';
import { z } from 'zod';
import { searchSemanticScholar, paperSchema, Paper } from '../tools/semantic-tool';
import { rankByLexicalScore } from '../scorers/lexical-scorer';

const querySchema = z.object({
  question: z.string().min(3).describe('Research question about databases'),
});

const paperWithWhySchema = paperSchema.extend({
  relevance: z.number(),
  why: z.string(),
});

const searchStep = createStep({
  id: 'search-semanticscholar',
  description: 'Search Semantic Scholar for VLDB/SIGMOD papers relevant to the question',
  inputSchema: querySchema,
  outputSchema: z.object({
    candidates: z.array(paperSchema),
  }),
  execute: async ({ inputData }) => {
    if (!inputData) throw new Error('Input data not found');
    const candidates: Paper[] = await searchSemanticScholar(inputData.question);
    return { candidates };
  },
});

const rankStep = createStep({
  id: 'rank-and-select',
  description: 'Rank candidates lexically and select top-5',
  inputSchema: z.object({
    question: z.string(),
    candidates: z.array(paperSchema),
  }),
  outputSchema: z.object({
    topPapers: z.array(paperWithWhySchema),
  }),
  execute: async ({ inputData }) => {
    if (!inputData) throw new Error('Input data not found');
    const ranked = rankByLexicalScore(inputData.question, inputData.candidates, 5);
    const topPapers = ranked.map(({ paper, score }) => {
      const why = buildWhy(inputData.question, paper);
      return { ...paper, relevance: Number(score.toFixed(3)), why };
    });
    return { topPapers };
  },
});

function buildWhy(question: string, paper: Paper): string {
  const qTokens = question
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, ' ')
    .split(/\s+/)
    .filter(Boolean);
  const matches = qTokens.filter((t) => paper.title.toLowerCase().includes(t));
  if (matches.length > 0) {
    return `Title matches keywords: ${Array.from(new Set(matches)).slice(0, 5).join(', ')}`;
  }
  return 'Relevant to query within VLDB/SIGMOD; selected via lexical similarity.';
}

const researchWorkflow = createWorkflow({
  id: 'research-workflow',
  inputSchema: querySchema,
  outputSchema: z.object({
    topPapers: z.array(paperWithWhySchema),
  }),
})
  .then(searchStep)
  .then(rankStep);

researchWorkflow.commit();

export { researchWorkflow };






------------------------------------------
File: examples/mastra/research-assistant/tsconfig.json
------------------------------------------

{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ES2022",
    "moduleResolution": "bundler",
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "strict": true,
    "skipLibCheck": true,
    "noEmit": true,
    "outDir": "dist"
  },
  "include": [
    "src/**/*",
    "test/**/*"
  ]
}




------------------------------------------
File: examples/openai-agents/research-assistant/.env.example
------------------------------------------

# OpenAI API Key (required)
OPENAI_API_KEY=your-api-key-here

# Optional: Custom AgentFS agent ID
# AGENTFS_ID=research-assistant




------------------------------------------
File: examples/openai-agents/research-assistant/agentfs.db-wal
------------------------------------------

[skipped: file appears binary or non-text]



------------------------------------------
File: examples/openai-agents/research-assistant/package-lock.json
------------------------------------------

{
  "name": "research-assistant-openai-agents",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "research-assistant-openai-agents",
      "version": "1.0.0",
      "license": "ISC",
      "dependencies": {
        "@openai/agents": "^0.3.1",
        "agentfs-sdk": "file:../../../sdk/typescript",
        "dotenv": "^16.4.7",
        "pdf-parse": "^1.1.1",
        "zod": "^3.24.1"
      },
      "bin": {
        "research-assistant": "dist/cli.js"
      },
      "devDependencies": {
        "@types/node": "^24.10.0",
        "@types/pdf-parse": "^1.1.4",
        "tsx": "^4.19.1",
        "typescript": "^5.9.3"
      },
      "engines": {
        "node": ">=20.9.0"
      }
    },
    "../../../sdk/typescript": {
      "name": "agentfs-sdk",
      "version": "0.1.0",
      "license": "MIT",
      "dependencies": {
        "@tursodatabase/database": "^0.3.2"
      },
      "devDependencies": {
        "@types/node": "^20.0.0",
        "@vitest/ui": "^4.0.1",
        "typescript": "^5.3.0",
        "vitest": "^4.0.1"
      }
    },
    "../../sdk/typescript": {
      "name": "agentfs-sdk",
      "version": "0.1.0",
      "extraneous": true,
      "license": "MIT",
      "dependencies": {
        "@tursodatabase/database": "^0.3.2"
      },
      "devDependencies": {
        "@types/node": "^20.0.0",
        "@vitest/ui": "^4.0.1",
        "typescript": "^5.3.0",
        "vitest": "^4.0.1"
      }
    },
    "node_modules/@esbuild/aix-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/aix-ppc64/-/aix-ppc64-0.25.12.tgz",
      "integrity": "sha512-Hhmwd6CInZ3dwpuGTF8fJG6yoWmsToE+vYgD4nytZVxcu1ulHpUQRAB1UJ8+N1Am3Mz4+xOByoQoSZf4D+CpkA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "aix"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm/-/android-arm-0.25.12.tgz",
      "integrity": "sha512-VJ+sKvNA/GE7Ccacc9Cha7bpS8nyzVv0jdVgwNDaR4gDMC/2TTRc33Ip8qrNYUcpkOHUT5OZ0bUcNNVZQ9RLlg==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-arm64/-/android-arm64-0.25.12.tgz",
      "integrity": "sha512-6AAmLG7zwD1Z159jCKPvAxZd4y/VTO0VkprYy+3N2FtJ8+BQWFXU+OxARIwA46c5tdD9SsKGZ/1ocqBS/gAKHg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/android-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/android-x64/-/android-x64-0.25.12.tgz",
      "integrity": "sha512-5jbb+2hhDHx5phYR2By8GTWEzn6I9UqR11Kwf22iKbNpYrsmRB18aX/9ivc5cabcUiAT/wM+YIZ6SG9QO6a8kg==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "android"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-arm64/-/darwin-arm64-0.25.12.tgz",
      "integrity": "sha512-N3zl+lxHCifgIlcMUP5016ESkeQjLj/959RxxNYIthIg+CQHInujFuXeWbWMgnTo4cp5XVHqFPmpyu9J65C1Yg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/darwin-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/darwin-x64/-/darwin-x64-0.25.12.tgz",
      "integrity": "sha512-HQ9ka4Kx21qHXwtlTUVbKJOAnmG1ipXhdWTmNXiPzPfWKpXqASVcWdnf2bnL73wgjNrFXAa3yYvBSd9pzfEIpA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-arm64/-/freebsd-arm64-0.25.12.tgz",
      "integrity": "sha512-gA0Bx759+7Jve03K1S0vkOu5Lg/85dou3EseOGUes8flVOGxbhDDh/iZaoek11Y8mtyKPGF3vP8XhnkDEAmzeg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/freebsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/freebsd-x64/-/freebsd-x64-0.25.12.tgz",
      "integrity": "sha512-TGbO26Yw2xsHzxtbVFGEXBFH0FRAP7gtcPE7P5yP7wGy7cXK2oO7RyOhL5NLiqTlBh47XhmIUXuGciXEqYFfBQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "freebsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm/-/linux-arm-0.25.12.tgz",
      "integrity": "sha512-lPDGyC1JPDou8kGcywY0YILzWlhhnRjdof3UlcoqYmS9El818LLfJJc3PXXgZHrHCAKs/Z2SeZtDJr5MrkxtOw==",
      "cpu": [
        "arm"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-arm64/-/linux-arm64-0.25.12.tgz",
      "integrity": "sha512-8bwX7a8FghIgrupcxb4aUmYDLp8pX06rGh5HqDT7bB+8Rdells6mHvrFHHW2JAOPZUbnjUpKTLg6ECyzvas2AQ==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ia32/-/linux-ia32-0.25.12.tgz",
      "integrity": "sha512-0y9KrdVnbMM2/vG8KfU0byhUN+EFCny9+8g202gYqSSVMonbsCfLjUO+rCci7pM0WBEtz+oK/PIwHkzxkyharA==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-loong64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-loong64/-/linux-loong64-0.25.12.tgz",
      "integrity": "sha512-h///Lr5a9rib/v1GGqXVGzjL4TMvVTv+s1DPoxQdz7l/AYv6LDSxdIwzxkrPW438oUXiDtwM10o9PmwS/6Z0Ng==",
      "cpu": [
        "loong64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-mips64el": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-mips64el/-/linux-mips64el-0.25.12.tgz",
      "integrity": "sha512-iyRrM1Pzy9GFMDLsXn1iHUm18nhKnNMWscjmp4+hpafcZjrr2WbT//d20xaGljXDBYHqRcl8HnxbX6uaA/eGVw==",
      "cpu": [
        "mips64el"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-ppc64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-ppc64/-/linux-ppc64-0.25.12.tgz",
      "integrity": "sha512-9meM/lRXxMi5PSUqEXRCtVjEZBGwB7P/D4yT8UG/mwIdze2aV4Vo6U5gD3+RsoHXKkHCfSxZKzmDssVlRj1QQA==",
      "cpu": [
        "ppc64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-riscv64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-riscv64/-/linux-riscv64-0.25.12.tgz",
      "integrity": "sha512-Zr7KR4hgKUpWAwb1f3o5ygT04MzqVrGEGXGLnj15YQDJErYu/BGg+wmFlIDOdJp0PmB0lLvxFIOXZgFRrdjR0w==",
      "cpu": [
        "riscv64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-s390x": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-s390x/-/linux-s390x-0.25.12.tgz",
      "integrity": "sha512-MsKncOcgTNvdtiISc/jZs/Zf8d0cl/t3gYWX8J9ubBnVOwlk65UIEEvgBORTiljloIWnBzLs4qhzPkJcitIzIg==",
      "cpu": [
        "s390x"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/linux-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/linux-x64/-/linux-x64-0.25.12.tgz",
      "integrity": "sha512-uqZMTLr/zR/ed4jIGnwSLkaHmPjOjJvnm6TVVitAa08SLS9Z0VM8wIRx7gWbJB5/J54YuIMInDquWyYvQLZkgw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "linux"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-arm64/-/netbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-xXwcTq4GhRM7J9A8Gv5boanHhRa/Q9KLVmcyXHCTaM4wKfIpWkdXiMog/KsnxzJ0A1+nD+zoecuzqPmCRyBGjg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/netbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/netbsd-x64/-/netbsd-x64-0.25.12.tgz",
      "integrity": "sha512-Ld5pTlzPy3YwGec4OuHh1aCVCRvOXdH8DgRjfDy/oumVovmuSzWfnSJg+VtakB9Cm0gxNO9BzWkj6mtO1FMXkQ==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "netbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-arm64/-/openbsd-arm64-0.25.12.tgz",
      "integrity": "sha512-fF96T6KsBo/pkQI950FARU9apGNTSlZGsv1jZBAlcLL1MLjLNIWPBkj5NlSz8aAzYKg+eNqknrUJ24QBybeR5A==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openbsd-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openbsd-x64/-/openbsd-x64-0.25.12.tgz",
      "integrity": "sha512-MZyXUkZHjQxUvzK7rN8DJ3SRmrVrke8ZyRusHlP+kuwqTcfWLyqMOE3sScPPyeIXN/mDJIfGXvcMqCgYKekoQw==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openbsd"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/openharmony-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/openharmony-arm64/-/openharmony-arm64-0.25.12.tgz",
      "integrity": "sha512-rm0YWsqUSRrjncSXGA7Zv78Nbnw4XL6/dzr20cyrQf7ZmRcsovpcRBdhD43Nuk3y7XIoW2OxMVvwuRvk9XdASg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "openharmony"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/sunos-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/sunos-x64/-/sunos-x64-0.25.12.tgz",
      "integrity": "sha512-3wGSCDyuTHQUzt0nV7bocDy72r2lI33QL3gkDNGkod22EsYl04sMf0qLb8luNKTOmgF/eDEDP5BFNwoBKH441w==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "sunos"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-arm64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-arm64/-/win32-arm64-0.25.12.tgz",
      "integrity": "sha512-rMmLrur64A7+DKlnSuwqUdRKyd3UE7oPJZmnljqEptesKM8wx9J8gx5u0+9Pq0fQQW8vqeKebwNXdfOyP+8Bsg==",
      "cpu": [
        "arm64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-ia32": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-ia32/-/win32-ia32-0.25.12.tgz",
      "integrity": "sha512-HkqnmmBoCbCwxUKKNPBixiWDGCpQGVsrQfJoVGYLPT41XWF8lHuE5N6WhVia2n4o5QK5M4tYr21827fNhi4byQ==",
      "cpu": [
        "ia32"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@esbuild/win32-x64": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/@esbuild/win32-x64/-/win32-x64-0.25.12.tgz",
      "integrity": "sha512-alJC0uCZpTFrSL0CCDjcgleBXPnCrEAhTBILpeAp7M/OFgoqtAetfBzX0xM00MUsVVPpVjlPuMbREqnZCXaTnA==",
      "cpu": [
        "x64"
      ],
      "dev": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "win32"
      ],
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/@modelcontextprotocol/sdk": {
      "version": "1.22.0",
      "resolved": "https://registry.npmjs.org/@modelcontextprotocol/sdk/-/sdk-1.22.0.tgz",
      "integrity": "sha512-VUpl106XVTCpDmTBil2ehgJZjhyLY2QZikzF8NvTXtLRF1CvO5iEE2UNZdVIUer35vFOwMKYeUGbjJtvPWan3g==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "ajv": "^8.17.1",
        "ajv-formats": "^3.0.1",
        "content-type": "^1.0.5",
        "cors": "^2.8.5",
        "cross-spawn": "^7.0.5",
        "eventsource": "^3.0.2",
        "eventsource-parser": "^3.0.0",
        "express": "^5.0.1",
        "express-rate-limit": "^7.5.0",
        "pkce-challenge": "^5.0.0",
        "raw-body": "^3.0.0",
        "zod": "^3.23.8",
        "zod-to-json-schema": "^3.24.1"
      },
      "engines": {
        "node": ">=18"
      },
      "peerDependencies": {
        "@cfworker/json-schema": "^4.1.1"
      },
      "peerDependenciesMeta": {
        "@cfworker/json-schema": {
          "optional": true
        }
      }
    },
    "node_modules/@openai/agents": {
      "version": "0.3.1",
      "resolved": "https://registry.npmjs.org/@openai/agents/-/agents-0.3.1.tgz",
      "integrity": "sha512-m8jw8CRhijRqZEaMhGVIOiXrOpW/n/EZwTV6WTU8eWwpBc2AtvJ7RQADZempNDyg42lCrrMFeofWXus8dlFzLQ==",
      "license": "MIT",
      "dependencies": {
        "@openai/agents-core": "0.3.1",
        "@openai/agents-openai": "0.3.1",
        "@openai/agents-realtime": "0.3.1",
        "debug": "^4.4.0",
        "openai": "^6"
      },
      "peerDependencies": {
        "zod": "^3.25.40 || ^4.0"
      }
    },
    "node_modules/@openai/agents-core": {
      "version": "0.3.1",
      "resolved": "https://registry.npmjs.org/@openai/agents-core/-/agents-core-0.3.1.tgz",
      "integrity": "sha512-HorYT4IFXYqjdw0DSYM0NnjhKMiEpt1tolMkwVkMH8rl3oH/i/ijEILmwuc/ewrcapNF4+RmOfUblbx2Gn9l0g==",
      "license": "MIT",
      "dependencies": {
        "debug": "^4.4.0",
        "openai": "^6"
      },
      "optionalDependencies": {
        "@modelcontextprotocol/sdk": "^1.17.2"
      },
      "peerDependencies": {
        "zod": "^3.25.40 || ^4.0"
      },
      "peerDependenciesMeta": {
        "zod": {
          "optional": true
        }
      }
    },
    "node_modules/@openai/agents-openai": {
      "version": "0.3.1",
      "resolved": "https://registry.npmjs.org/@openai/agents-openai/-/agents-openai-0.3.1.tgz",
      "integrity": "sha512-RrMywXdt8MBcTAuT8OP9Xvpl68ddC3IFZ0dqTz6F4U6MPfahejfhMGqn1W4urcZTls+3Xt8KuWldLM+EmKd4Qg==",
      "license": "MIT",
      "dependencies": {
        "@openai/agents-core": "0.3.1",
        "debug": "^4.4.0",
        "openai": "^6"
      },
      "peerDependencies": {
        "zod": "^3.25.40 || ^4.0"
      }
    },
    "node_modules/@openai/agents-realtime": {
      "version": "0.3.1",
      "resolved": "https://registry.npmjs.org/@openai/agents-realtime/-/agents-realtime-0.3.1.tgz",
      "integrity": "sha512-vNxOr6tnRTNRYcC76HAH9UKECaY9+QLMxwjtobk6jZmJnrDVD609+MEvKB32i4Om7B+wgOhtCnZ4pN5blHjRow==",
      "license": "MIT",
      "dependencies": {
        "@openai/agents-core": "0.3.1",
        "@types/ws": "^8.18.1",
        "debug": "^4.4.0",
        "ws": "^8.18.1"
      },
      "peerDependencies": {
        "zod": "^3.25.40 || ^4.0"
      }
    },
    "node_modules/@types/node": {
      "version": "24.10.1",
      "resolved": "https://registry.npmjs.org/@types/node/-/node-24.10.1.tgz",
      "integrity": "sha512-GNWcUTRBgIRJD5zj+Tq0fKOJ5XZajIiBroOF0yvj2bSU1WvNdYS/dn9UxwsujGW4JX06dnHyjV2y9rRaybH0iQ==",
      "license": "MIT",
      "dependencies": {
        "undici-types": "~7.16.0"
      }
    },
    "node_modules/@types/pdf-parse": {
      "version": "1.1.5",
      "resolved": "https://registry.npmjs.org/@types/pdf-parse/-/pdf-parse-1.1.5.tgz",
      "integrity": "sha512-kBfrSXsloMnUJOKi25s3+hRmkycHfLK6A09eRGqF/N8BkQoPUmaCr+q8Cli5FnfohEz/rsv82zAiPz/LXtOGhA==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/@types/ws": {
      "version": "8.18.1",
      "resolved": "https://registry.npmjs.org/@types/ws/-/ws-8.18.1.tgz",
      "integrity": "sha512-ThVF6DCVhA8kUGy+aazFQ4kXQ7E1Ty7A3ypFOe0IcJV8O/M511G99AW24irKrW56Wt44yG9+ij8FaqoBGkuBXg==",
      "license": "MIT",
      "dependencies": {
        "@types/node": "*"
      }
    },
    "node_modules/accepts": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/accepts/-/accepts-2.0.0.tgz",
      "integrity": "sha512-5cvg6CtKwfgdmVqY1WIiXKc3Q1bkRqGLi+2W/6ao+6Y7gu/RCwRuAhGEzh5B4KlszSuTLgZYuqFqo5bImjNKng==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "mime-types": "^3.0.0",
        "negotiator": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/agentfs-sdk": {
      "resolved": "../../../sdk/typescript",
      "link": true
    },
    "node_modules/ajv": {
      "version": "8.17.1",
      "resolved": "https://registry.npmjs.org/ajv/-/ajv-8.17.1.tgz",
      "integrity": "sha512-B/gBuNg5SiMTrPkC+A2+cW0RszwxYmn6VYxB/inlBStS5nx6xHIt/ehKRhIMhqusl7a8LjQoZnjCs5vhwxOQ1g==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "fast-deep-equal": "^3.1.3",
        "fast-uri": "^3.0.1",
        "json-schema-traverse": "^1.0.0",
        "require-from-string": "^2.0.2"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/epoberezkin"
      }
    },
    "node_modules/ajv-formats": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/ajv-formats/-/ajv-formats-3.0.1.tgz",
      "integrity": "sha512-8iUql50EUR+uUcdRQ3HDqa6EVyo3docL8g5WJ3FNcWmu62IbkGUue/pEyLBW8VGKKucTPgqeks4fIU1DA4yowQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "ajv": "^8.0.0"
      },
      "peerDependencies": {
        "ajv": "^8.0.0"
      },
      "peerDependenciesMeta": {
        "ajv": {
          "optional": true
        }
      }
    },
    "node_modules/body-parser": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/body-parser/-/body-parser-2.2.0.tgz",
      "integrity": "sha512-02qvAaxv8tp7fBa/mw1ga98OGm+eCbqzJOKoRt70sLmfEEi+jyBYVTDGfCL/k06/4EMk/z01gCe7HoCH/f2LTg==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "bytes": "^3.1.2",
        "content-type": "^1.0.5",
        "debug": "^4.4.0",
        "http-errors": "^2.0.0",
        "iconv-lite": "^0.6.3",
        "on-finished": "^2.4.1",
        "qs": "^6.14.0",
        "raw-body": "^3.0.0",
        "type-is": "^2.0.0"
      },
      "engines": {
        "node": ">=18"
      }
    },
    "node_modules/bytes": {
      "version": "3.1.2",
      "resolved": "https://registry.npmjs.org/bytes/-/bytes-3.1.2.tgz",
      "integrity": "sha512-/Nf7TyzTx6S3yRJObOAV7956r8cr2+Oj8AC5dt8wSP3BQAoeX58NoHyCU8P8zGkNXStjTSi6fzO6F0pBdcYbEg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/call-bind-apply-helpers": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/call-bind-apply-helpers/-/call-bind-apply-helpers-1.0.2.tgz",
      "integrity": "sha512-Sp1ablJ0ivDkSzjcaJdxEunN5/XvksFJ2sMBFfq6x0ryhQV/2b/KwFe21cMpmHtPOSij8K99/wSfoEuTObmuMQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "es-errors": "^1.3.0",
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/call-bound": {
      "version": "1.0.4",
      "resolved": "https://registry.npmjs.org/call-bound/-/call-bound-1.0.4.tgz",
      "integrity": "sha512-+ys997U96po4Kx/ABpBCqhA9EuxJaQWDQg7295H4hBphv3IZg0boBKuwYpt4YXp6MZ5AmZQnU/tyMTlRpaSejg==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "get-intrinsic": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/content-disposition": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/content-disposition/-/content-disposition-1.0.0.tgz",
      "integrity": "sha512-Au9nRL8VNUut/XSzbQA38+M78dzP4D+eqg3gfJHMIHHYa3bg067xj1KxMUWj+VULbiZMowKngFFbKczUrNJ1mg==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "safe-buffer": "5.2.1"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/content-type": {
      "version": "1.0.5",
      "resolved": "https://registry.npmjs.org/content-type/-/content-type-1.0.5.tgz",
      "integrity": "sha512-nTjqfcBFEipKdXCv4YDQWCfmcLZKm81ldF0pAopTvyrFGVbcR6P/VAAd5G7N+0tTr8QqiU0tFadD6FK4NtJwOA==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/cookie": {
      "version": "0.7.2",
      "resolved": "https://registry.npmjs.org/cookie/-/cookie-0.7.2.tgz",
      "integrity": "sha512-yki5XnKuf750l50uGTllt6kKILY4nQ1eNIQatoXEByZ5dWgnKqbnqmTrBE5B4N7lrMJKQ2ytWMiTO2o0v6Ew/w==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/cookie-signature": {
      "version": "1.2.2",
      "resolved": "https://registry.npmjs.org/cookie-signature/-/cookie-signature-1.2.2.tgz",
      "integrity": "sha512-D76uU73ulSXrD1UXF4KE2TMxVVwhsnCgfAyTg9k8P6KGZjlXKrOLe4dJQKI3Bxi5wjesZoFXJWElNWBjPZMbhg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=6.6.0"
      }
    },
    "node_modules/cors": {
      "version": "2.8.5",
      "resolved": "https://registry.npmjs.org/cors/-/cors-2.8.5.tgz",
      "integrity": "sha512-KIHbLJqu73RGr/hnbrO9uBeixNGuvSQjul/jdFvS/KFSIH1hWVd1ng7zOHx+YrEfInLG7q4n6GHQ9cDtxv/P6g==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "object-assign": "^4",
        "vary": "^1"
      },
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/cross-spawn": {
      "version": "7.0.6",
      "resolved": "https://registry.npmjs.org/cross-spawn/-/cross-spawn-7.0.6.tgz",
      "integrity": "sha512-uV2QOWP2nWzsy2aMp8aRibhi9dlzF5Hgh5SHaB9OiTGEyDTiJJyx0uy51QXdyWbtAHNua4XJzUKca3OzKUd3vA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "path-key": "^3.1.0",
        "shebang-command": "^2.0.0",
        "which": "^2.0.1"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/debug": {
      "version": "4.4.3",
      "resolved": "https://registry.npmjs.org/debug/-/debug-4.4.3.tgz",
      "integrity": "sha512-RGwwWnwQvkVfavKVt22FGLw+xYSdzARwm0ru6DhTVA3umU5hZc28V3kO4stgYryrTlLpuvgI9GiijltAjNbcqA==",
      "license": "MIT",
      "dependencies": {
        "ms": "^2.1.3"
      },
      "engines": {
        "node": ">=6.0"
      },
      "peerDependenciesMeta": {
        "supports-color": {
          "optional": true
        }
      }
    },
    "node_modules/depd": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/depd/-/depd-2.0.0.tgz",
      "integrity": "sha512-g7nH6P6dyDioJogAAGprGpCtVImJhpPk/roCzdb3fIh61/s/nPsfR6onyMwkCAR/OlC3yBC0lESvUoQEAssIrw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/dotenv": {
      "version": "16.6.1",
      "resolved": "https://registry.npmjs.org/dotenv/-/dotenv-16.6.1.tgz",
      "integrity": "sha512-uBq4egWHTcTt33a72vpSG0z3HnPuIl6NqYcTrKEg2azoEyl2hpW0zqlxysq2pK9HlDIHyHyakeYaYnSAwd8bow==",
      "license": "BSD-2-Clause",
      "engines": {
        "node": ">=12"
      },
      "funding": {
        "url": "https://dotenvx.com"
      }
    },
    "node_modules/dunder-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/dunder-proto/-/dunder-proto-1.0.1.tgz",
      "integrity": "sha512-KIN/nDJBQRcXw0MLVhZE9iQHmG68qAVIBg9CqmUYjmQIhgij9U5MFvrqkUL5FbtyyzZuOeOt0zdeRe4UY7ct+A==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.1",
        "es-errors": "^1.3.0",
        "gopd": "^1.2.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/ee-first": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/ee-first/-/ee-first-1.1.1.tgz",
      "integrity": "sha512-WMwm9LhRUo+WUaRN+vRuETqG89IgZphVSNkdFgeb6sS/E4OrDIN7t48CAewSHXc6C8lefD8KKfr5vY61brQlow==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/encodeurl": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/encodeurl/-/encodeurl-2.0.0.tgz",
      "integrity": "sha512-Q0n9HRi4m6JuGIV1eFlmvJB7ZEVxu93IrMyiMsGC0lrMJMWzRgx6WGquyfQgZVb31vhGgXnfmPNNXmxnOkRBrg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/es-define-property": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/es-define-property/-/es-define-property-1.0.1.tgz",
      "integrity": "sha512-e3nRfgfUZ4rNGL232gUgX06QNyyez04KdjFrF+LTRoOXmrOgFKDg4BCdsjW8EnT69eqdYGmRpJwiPVYNrCaW3g==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-errors": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/es-errors/-/es-errors-1.3.0.tgz",
      "integrity": "sha512-Zf5H2Kxt2xjTvbJvP2ZWLEICxA6j+hAmMzIlypy4xcBg1vKVnx89Wy0GbS+kf5cwCVFFzdCFh2XSCFNULS6csw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/es-object-atoms": {
      "version": "1.1.1",
      "resolved": "https://registry.npmjs.org/es-object-atoms/-/es-object-atoms-1.1.1.tgz",
      "integrity": "sha512-FGgH2h8zKNim9ljj7dankFPcICIK9Cp5bm+c2gQSYePhpaG5+esrLODihIorn+Pe6FGJzWhXQotPv73jTaldXA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "es-errors": "^1.3.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/esbuild": {
      "version": "0.25.12",
      "resolved": "https://registry.npmjs.org/esbuild/-/esbuild-0.25.12.tgz",
      "integrity": "sha512-bbPBYYrtZbkt6Os6FiTLCTFxvq4tt3JKall1vRwshA3fdVztsLAatFaZobhkBC8/BrPetoa0oksYoKXoG4ryJg==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "bin": {
        "esbuild": "bin/esbuild"
      },
      "engines": {
        "node": ">=18"
      },
      "optionalDependencies": {
        "@esbuild/aix-ppc64": "0.25.12",
        "@esbuild/android-arm": "0.25.12",
        "@esbuild/android-arm64": "0.25.12",
        "@esbuild/android-x64": "0.25.12",
        "@esbuild/darwin-arm64": "0.25.12",
        "@esbuild/darwin-x64": "0.25.12",
        "@esbuild/freebsd-arm64": "0.25.12",
        "@esbuild/freebsd-x64": "0.25.12",
        "@esbuild/linux-arm": "0.25.12",
        "@esbuild/linux-arm64": "0.25.12",
        "@esbuild/linux-ia32": "0.25.12",
        "@esbuild/linux-loong64": "0.25.12",
        "@esbuild/linux-mips64el": "0.25.12",
        "@esbuild/linux-ppc64": "0.25.12",
        "@esbuild/linux-riscv64": "0.25.12",
        "@esbuild/linux-s390x": "0.25.12",
        "@esbuild/linux-x64": "0.25.12",
        "@esbuild/netbsd-arm64": "0.25.12",
        "@esbuild/netbsd-x64": "0.25.12",
        "@esbuild/openbsd-arm64": "0.25.12",
        "@esbuild/openbsd-x64": "0.25.12",
        "@esbuild/openharmony-arm64": "0.25.12",
        "@esbuild/sunos-x64": "0.25.12",
        "@esbuild/win32-arm64": "0.25.12",
        "@esbuild/win32-ia32": "0.25.12",
        "@esbuild/win32-x64": "0.25.12"
      }
    },
    "node_modules/escape-html": {
      "version": "1.0.3",
      "resolved": "https://registry.npmjs.org/escape-html/-/escape-html-1.0.3.tgz",
      "integrity": "sha512-NiSupZ4OeuGwr68lGIeym/ksIZMJodUGOSCZ/FSnTxcrekbvqrgdUxlJOMpijaKZVjAJrWrGs/6Jy8OMuyj9ow==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/etag": {
      "version": "1.8.1",
      "resolved": "https://registry.npmjs.org/etag/-/etag-1.8.1.tgz",
      "integrity": "sha512-aIL5Fx7mawVa300al2BnEE4iNvo1qETxLrPI/o05L7z6go7fCw1J6EQmbK4FmJ2AS7kgVF/KEZWufBfdClMcPg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/eventsource": {
      "version": "3.0.7",
      "resolved": "https://registry.npmjs.org/eventsource/-/eventsource-3.0.7.tgz",
      "integrity": "sha512-CRT1WTyuQoD771GW56XEZFQ/ZoSfWid1alKGDYMmkt2yl8UXrVR4pspqWNEcqKvVIzg6PAltWjxcSSPrboA4iA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "eventsource-parser": "^3.0.1"
      },
      "engines": {
        "node": ">=18.0.0"
      }
    },
    "node_modules/eventsource-parser": {
      "version": "3.0.6",
      "resolved": "https://registry.npmjs.org/eventsource-parser/-/eventsource-parser-3.0.6.tgz",
      "integrity": "sha512-Vo1ab+QXPzZ4tCa8SwIHJFaSzy4R6SHf7BY79rFBDf0idraZWAkYrDjDj8uWaSm3S2TK+hJ7/t1CEmZ7jXw+pg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=18.0.0"
      }
    },
    "node_modules/express": {
      "version": "5.1.0",
      "resolved": "https://registry.npmjs.org/express/-/express-5.1.0.tgz",
      "integrity": "sha512-DT9ck5YIRU+8GYzzU5kT3eHGA5iL+1Zd0EutOmTE9Dtk+Tvuzd23VBU+ec7HPNSTxXYO55gPV/hq4pSBJDjFpA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "accepts": "^2.0.0",
        "body-parser": "^2.2.0",
        "content-disposition": "^1.0.0",
        "content-type": "^1.0.5",
        "cookie": "^0.7.1",
        "cookie-signature": "^1.2.1",
        "debug": "^4.4.0",
        "encodeurl": "^2.0.0",
        "escape-html": "^1.0.3",
        "etag": "^1.8.1",
        "finalhandler": "^2.1.0",
        "fresh": "^2.0.0",
        "http-errors": "^2.0.0",
        "merge-descriptors": "^2.0.0",
        "mime-types": "^3.0.0",
        "on-finished": "^2.4.1",
        "once": "^1.4.0",
        "parseurl": "^1.3.3",
        "proxy-addr": "^2.0.7",
        "qs": "^6.14.0",
        "range-parser": "^1.2.1",
        "router": "^2.2.0",
        "send": "^1.1.0",
        "serve-static": "^2.2.0",
        "statuses": "^2.0.1",
        "type-is": "^2.0.1",
        "vary": "^1.1.2"
      },
      "engines": {
        "node": ">= 18"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/express"
      }
    },
    "node_modules/express-rate-limit": {
      "version": "7.5.1",
      "resolved": "https://registry.npmjs.org/express-rate-limit/-/express-rate-limit-7.5.1.tgz",
      "integrity": "sha512-7iN8iPMDzOMHPUYllBEsQdWVB6fPDMPqwjBaFrgr4Jgr/+okjvzAy+UHlYYL/Vs0OsOrMkwS6PJDkFlJwoxUnw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 16"
      },
      "funding": {
        "url": "https://github.com/sponsors/express-rate-limit"
      },
      "peerDependencies": {
        "express": ">= 4.11"
      }
    },
    "node_modules/fast-deep-equal": {
      "version": "3.1.3",
      "resolved": "https://registry.npmjs.org/fast-deep-equal/-/fast-deep-equal-3.1.3.tgz",
      "integrity": "sha512-f3qQ9oQy9j2AhBe/H9VC91wLmKBCCU/gDOnKNAYG5hswO7BLKj09Hc5HYNz9cGI++xlpDCIgDaitVs03ATR84Q==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/fast-uri": {
      "version": "3.1.0",
      "resolved": "https://registry.npmjs.org/fast-uri/-/fast-uri-3.1.0.tgz",
      "integrity": "sha512-iPeeDKJSWf4IEOasVVrknXpaBV0IApz/gp7S2bb7Z4Lljbl2MGJRqInZiUrQwV16cpzw/D3S5j5Julj/gT52AA==",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/fastify"
        },
        {
          "type": "opencollective",
          "url": "https://opencollective.com/fastify"
        }
      ],
      "license": "BSD-3-Clause",
      "optional": true
    },
    "node_modules/finalhandler": {
      "version": "2.1.0",
      "resolved": "https://registry.npmjs.org/finalhandler/-/finalhandler-2.1.0.tgz",
      "integrity": "sha512-/t88Ty3d5JWQbWYgaOGCCYfXRwV1+be02WqYYlL6h0lEiUAMPM8o8qKGO01YIkOHzka2up08wvgYD0mDiI+q3Q==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "debug": "^4.4.0",
        "encodeurl": "^2.0.0",
        "escape-html": "^1.0.3",
        "on-finished": "^2.4.1",
        "parseurl": "^1.3.3",
        "statuses": "^2.0.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/forwarded": {
      "version": "0.2.0",
      "resolved": "https://registry.npmjs.org/forwarded/-/forwarded-0.2.0.tgz",
      "integrity": "sha512-buRG0fpBtRHSTCOASe6hD258tEubFoRLb4ZNA6NxMVHNw2gOcwHo9wyablzMzOA5z9xA9L1KNjk/Nt6MT9aYow==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/fresh": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/fresh/-/fresh-2.0.0.tgz",
      "integrity": "sha512-Rx/WycZ60HOaqLKAi6cHRKKI7zxWbJ31MhntmtwMoaTeF7XFH9hhBp8vITaMidfljRQ6eYWCKkaTK+ykVJHP2A==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/fsevents": {
      "version": "2.3.3",
      "resolved": "https://registry.npmjs.org/fsevents/-/fsevents-2.3.3.tgz",
      "integrity": "sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==",
      "dev": true,
      "hasInstallScript": true,
      "license": "MIT",
      "optional": true,
      "os": [
        "darwin"
      ],
      "engines": {
        "node": "^8.16.0 || ^10.6.0 || >=11.0.0"
      }
    },
    "node_modules/function-bind": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz",
      "integrity": "sha512-7XHNxH7qX9xG5mIwxkhumTox/MIRNcOgDrxWsMt2pAr23WHp6MrRlN7FBSFpCpr+oVO0F744iUgR82nJMfG2SA==",
      "license": "MIT",
      "optional": true,
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-intrinsic": {
      "version": "1.3.0",
      "resolved": "https://registry.npmjs.org/get-intrinsic/-/get-intrinsic-1.3.0.tgz",
      "integrity": "sha512-9fSjSaos/fRIVIp+xSJlE6lfwhES7LNtKaCBIamHsjr2na1BiABJPo0mOjjz8GJDURarmCPGqaiVg5mfjb98CQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "call-bind-apply-helpers": "^1.0.2",
        "es-define-property": "^1.0.1",
        "es-errors": "^1.3.0",
        "es-object-atoms": "^1.1.1",
        "function-bind": "^1.1.2",
        "get-proto": "^1.0.1",
        "gopd": "^1.2.0",
        "has-symbols": "^1.1.0",
        "hasown": "^2.0.2",
        "math-intrinsics": "^1.1.0"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/get-proto": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/get-proto/-/get-proto-1.0.1.tgz",
      "integrity": "sha512-sTSfBjoXBp89JvIKIefqw7U2CCebsc74kiY6awiGogKtoSGbgjYE/G/+l9sF3MWFPNc9IcoOC4ODfKHfxFmp0g==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "dunder-proto": "^1.0.1",
        "es-object-atoms": "^1.0.0"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/get-tsconfig": {
      "version": "4.13.0",
      "resolved": "https://registry.npmjs.org/get-tsconfig/-/get-tsconfig-4.13.0.tgz",
      "integrity": "sha512-1VKTZJCwBrvbd+Wn3AOgQP/2Av+TfTCOlE4AcRJE72W1ksZXbAx8PPBR9RzgTeSPzlPMHrbANMH3LbltH73wxQ==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "resolve-pkg-maps": "^1.0.0"
      },
      "funding": {
        "url": "https://github.com/privatenumber/get-tsconfig?sponsor=1"
      }
    },
    "node_modules/gopd": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/gopd/-/gopd-1.2.0.tgz",
      "integrity": "sha512-ZUKRh6/kUFoAiTAtTYPZJ3hw9wNxx+BIBOijnlG9PnrJsCcSjs1wyyD6vJpaYtgnzDrKYRSqf3OO6Rfa93xsRg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/has-symbols": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/has-symbols/-/has-symbols-1.1.0.tgz",
      "integrity": "sha512-1cDNdwJ2Jaohmb3sg4OmKaMBwuC48sYni5HUw2DvsC8LjGTLK9h+eb1X6RyuOHe4hT0ULCW68iomhjUoKUqlPQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/hasown": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/hasown/-/hasown-2.0.2.tgz",
      "integrity": "sha512-0hJU9SCPvmMzIBdZFqNPXWa6dqh7WdH0cII9y+CyS8rG3nL48Bclra9HmKhVVUHyPWNH5Y7xDwAB7bfgSjkUMQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "function-bind": "^1.1.2"
      },
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/http-errors": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/http-errors/-/http-errors-2.0.0.tgz",
      "integrity": "sha512-FtwrG/euBzaEjYeRqOgly7G0qviiXoJWnvEH2Z1plBdXgbyjv34pHTSb9zoeHMyDy33+DWy5Wt9Wo+TURtOYSQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "depd": "2.0.0",
        "inherits": "2.0.4",
        "setprototypeof": "1.2.0",
        "statuses": "2.0.1",
        "toidentifier": "1.0.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/http-errors/node_modules/statuses": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/statuses/-/statuses-2.0.1.tgz",
      "integrity": "sha512-RwNA9Z/7PrK06rYLIzFMlaF+l73iwpzsqRIFgbMLbTcLD6cOao82TaWefPXQvB2fOC4AjuYSEndS7N/mTCbkdQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/iconv-lite": {
      "version": "0.6.3",
      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.6.3.tgz",
      "integrity": "sha512-4fCk79wshMdzMp2rH06qWrJE4iolqLhCUH+OiuIgU++RB0+94NlDL81atO7GX55uUKueo0txHNtvEyI6D7WdMw==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "safer-buffer": ">= 2.1.2 < 3.0.0"
      },
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/inherits": {
      "version": "2.0.4",
      "resolved": "https://registry.npmjs.org/inherits/-/inherits-2.0.4.tgz",
      "integrity": "sha512-k/vGaX4/Yla3WzyMCvTQOXYeIHvqOKtnqBduzTHpzpQZzAskKMhZ2K+EnBiSM9zGSoIFeMpXKxa4dYeZIQqewQ==",
      "license": "ISC",
      "optional": true
    },
    "node_modules/ipaddr.js": {
      "version": "1.9.1",
      "resolved": "https://registry.npmjs.org/ipaddr.js/-/ipaddr.js-1.9.1.tgz",
      "integrity": "sha512-0KI/607xoxSToH7GjN1FfSbLoU0+btTicjsQSWQlh/hZykN8KpmMf7uYwPW3R+akZ6R/w18ZlXSHBYXiYUPO3g==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/is-promise": {
      "version": "4.0.0",
      "resolved": "https://registry.npmjs.org/is-promise/-/is-promise-4.0.0.tgz",
      "integrity": "sha512-hvpoI6korhJMnej285dSg6nu1+e6uxs7zG3BYAm5byqDsgJNWwxzM6z6iZiAgQR4TJ30JmBTOwqZUw3WlyH3AQ==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/isexe": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/isexe/-/isexe-2.0.0.tgz",
      "integrity": "sha512-RHxMLp9lnKHGHRng9QFhRCMbYAcVpn69smSGcq3f36xjgVVWThj4qqLbTLlq7Ssj8B+fIQ1EuCEGI2lKsyQeIw==",
      "license": "ISC",
      "optional": true
    },
    "node_modules/json-schema-traverse": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/json-schema-traverse/-/json-schema-traverse-1.0.0.tgz",
      "integrity": "sha512-NM8/P9n3XjXhIZn1lLhkFaACTOURQXjWhV4BA/RnOv8xvgqtqpAX9IO4mRQxSx1Rlo4tqzeqb0sOlruaOy3dug==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/math-intrinsics": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz",
      "integrity": "sha512-/IXtbwEk5HTPyEwyKX6hGkYXxM9nbj64B+ilVJnC/R6B0pH5G4V3b0pVbL7DBj4tkhBAppbQUlf6F6Xl9LHu1g==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      }
    },
    "node_modules/media-typer": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/media-typer/-/media-typer-1.1.0.tgz",
      "integrity": "sha512-aisnrDP4GNe06UcKFnV5bfMNPBUw4jsLGaWwWfnH3v02GnBuXX2MCVn5RbrWo0j3pczUilYblq7fQ7Nw2t5XKw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/merge-descriptors": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/merge-descriptors/-/merge-descriptors-2.0.0.tgz",
      "integrity": "sha512-Snk314V5ayFLhp3fkUREub6WtjBfPdCPY1Ln8/8munuLuiYhsABgBVWsozAG+MWMbVEvcdcpbi9R7ww22l9Q3g==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=18"
      },
      "funding": {
        "url": "https://github.com/sponsors/sindresorhus"
      }
    },
    "node_modules/mime-db": {
      "version": "1.54.0",
      "resolved": "https://registry.npmjs.org/mime-db/-/mime-db-1.54.0.tgz",
      "integrity": "sha512-aU5EJuIN2WDemCcAp2vFBfp/m4EAhWJnUNSSw0ixs7/kXbd6Pg64EmwJkNdFhB8aWt1sH2CTXrLxo/iAGV3oPQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/mime-types": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/mime-types/-/mime-types-3.0.1.tgz",
      "integrity": "sha512-xRc4oEhT6eaBpU1XF7AjpOFD+xQmXNB5OVKwp4tqCuBpHLS/ZbBDrc07mYTDqVMg6PfxUjjNp85O6Cd2Z/5HWA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "mime-db": "^1.54.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/ms": {
      "version": "2.1.3",
      "resolved": "https://registry.npmjs.org/ms/-/ms-2.1.3.tgz",
      "integrity": "sha512-6FlzubTLZG3J2a/NVCAleEhjzq5oxgHyaCU9yYXvcLsvoVaHJq/s5xXI6/XXP6tz7R9xAOtHnSO/tXtF3WRTlA==",
      "license": "MIT"
    },
    "node_modules/negotiator": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/negotiator/-/negotiator-1.0.0.tgz",
      "integrity": "sha512-8Ofs/AUQh8MaEcrlq5xOX0CQ9ypTF5dl78mjlMNfOK08fzpgTHQRQPBxcPlEtIw0yRpws+Zo/3r+5WRby7u3Gg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/node-ensure": {
      "version": "0.0.0",
      "resolved": "https://registry.npmjs.org/node-ensure/-/node-ensure-0.0.0.tgz",
      "integrity": "sha512-DRI60hzo2oKN1ma0ckc6nQWlHU69RH6xN0sjQTjMpChPfTYvKZdcQFfdYK2RWbJcKyUizSIy/l8OTGxMAM1QDw==",
      "license": "MIT"
    },
    "node_modules/object-assign": {
      "version": "4.1.1",
      "resolved": "https://registry.npmjs.org/object-assign/-/object-assign-4.1.1.tgz",
      "integrity": "sha512-rJgTQnkUnH1sFw8yT6VSU3zD3sWmu6sZhIseY8VX+GRu3P6F7Fu+JNDoXfklElbLJSnc3FUQHVe4cU5hj+BcUg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/object-inspect": {
      "version": "1.13.4",
      "resolved": "https://registry.npmjs.org/object-inspect/-/object-inspect-1.13.4.tgz",
      "integrity": "sha512-W67iLl4J2EXEGTbfeHCffrjDfitvLANg0UlX3wFUUSTx92KXRFegMHUVgSqE+wvhAbi4WqjGg9czysTV2Epbew==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/on-finished": {
      "version": "2.4.1",
      "resolved": "https://registry.npmjs.org/on-finished/-/on-finished-2.4.1.tgz",
      "integrity": "sha512-oVlzkg3ENAhCk2zdv7IJwd/QUD4z2RxRwpkcGY8psCVcCYZNq4wYnVWALHM+brtuJjePWiYF/ClmuDr8Ch5+kg==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "ee-first": "1.1.1"
      },
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/once": {
      "version": "1.4.0",
      "resolved": "https://registry.npmjs.org/once/-/once-1.4.0.tgz",
      "integrity": "sha512-lNaJgI+2Q5URQBkccEKHTQOPaXdUxnZZElQTZY0MFUAuaEqe1E+Nyvgdz/aIyNi6Z9MzO5dv1H8n58/GELp3+w==",
      "license": "ISC",
      "optional": true,
      "dependencies": {
        "wrappy": "1"
      }
    },
    "node_modules/openai": {
      "version": "6.9.0",
      "resolved": "https://registry.npmjs.org/openai/-/openai-6.9.0.tgz",
      "integrity": "sha512-n2sJRYmM+xfJ0l3OfH8eNnIyv3nQY7L08gZQu3dw6wSdfPtKAk92L83M2NIP5SS8Cl/bsBBG3yKzEOjkx0O+7A==",
      "license": "Apache-2.0",
      "bin": {
        "openai": "bin/cli"
      },
      "peerDependencies": {
        "ws": "^8.18.0",
        "zod": "^3.25 || ^4.0"
      },
      "peerDependenciesMeta": {
        "ws": {
          "optional": true
        },
        "zod": {
          "optional": true
        }
      }
    },
    "node_modules/parseurl": {
      "version": "1.3.3",
      "resolved": "https://registry.npmjs.org/parseurl/-/parseurl-1.3.3.tgz",
      "integrity": "sha512-CiyeOxFT/JZyN5m0z9PfXw4SCBJ6Sygz1Dpl0wqjlhDEGGBP1GnsUVEL0p63hoG1fcj3fHynXi9NYO4nWOL+qQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/path-key": {
      "version": "3.1.1",
      "resolved": "https://registry.npmjs.org/path-key/-/path-key-3.1.1.tgz",
      "integrity": "sha512-ojmeN0qd+y0jszEtoY48r0Peq5dwMEkIlCOu6Q5f41lfkswXuKtYrhgoTpLnyIcHm24Uhqx+5Tqm2InSwLhE6Q==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/path-to-regexp": {
      "version": "8.3.0",
      "resolved": "https://registry.npmjs.org/path-to-regexp/-/path-to-regexp-8.3.0.tgz",
      "integrity": "sha512-7jdwVIRtsP8MYpdXSwOS0YdD0Du+qOoF/AEPIt88PcCFrZCzx41oxku1jD88hZBwbNUIEfpqvuhjFaMAqMTWnA==",
      "license": "MIT",
      "optional": true,
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/express"
      }
    },
    "node_modules/pdf-parse": {
      "version": "1.1.4",
      "resolved": "https://registry.npmjs.org/pdf-parse/-/pdf-parse-1.1.4.tgz",
      "integrity": "sha512-XRIRcLgk6ZnUbsHsYXExMw+krrPE81hJ6FQPLdBNhhBefqIQKXu/WeTgNBGSwPrfU0v+UCEwn7AoAUOsVKHFvQ==",
      "license": "MIT",
      "dependencies": {
        "node-ensure": "^0.0.0"
      },
      "engines": {
        "node": ">=6.8.1"
      },
      "funding": {
        "type": "github",
        "url": "https://github.com/sponsors/mehmet-kozan"
      }
    },
    "node_modules/pkce-challenge": {
      "version": "5.0.0",
      "resolved": "https://registry.npmjs.org/pkce-challenge/-/pkce-challenge-5.0.0.tgz",
      "integrity": "sha512-ueGLflrrnvwB3xuo/uGob5pd5FN7l0MsLf0Z87o/UQmRtwjvfylfc9MurIxRAWywCYTgrvpXBcqjV4OfCYGCIQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=16.20.0"
      }
    },
    "node_modules/proxy-addr": {
      "version": "2.0.7",
      "resolved": "https://registry.npmjs.org/proxy-addr/-/proxy-addr-2.0.7.tgz",
      "integrity": "sha512-llQsMLSUDUPT44jdrU/O37qlnifitDP+ZwrmmZcoSKyLKvtZxpyV0n2/bD/N4tBAAZ/gJEdZU7KMraoK1+XYAg==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "forwarded": "0.2.0",
        "ipaddr.js": "1.9.1"
      },
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/qs": {
      "version": "6.14.0",
      "resolved": "https://registry.npmjs.org/qs/-/qs-6.14.0.tgz",
      "integrity": "sha512-YWWTjgABSKcvs/nWBi9PycY/JiPJqOD4JA6o9Sej2AtvSGarXxKC3OQSk4pAarbdQlKAh5D4FCQkJNkW+GAn3w==",
      "license": "BSD-3-Clause",
      "optional": true,
      "dependencies": {
        "side-channel": "^1.1.0"
      },
      "engines": {
        "node": ">=0.6"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/range-parser": {
      "version": "1.2.1",
      "resolved": "https://registry.npmjs.org/range-parser/-/range-parser-1.2.1.tgz",
      "integrity": "sha512-Hrgsx+orqoygnmhFbKaHE6c296J+HTAQXoxEF6gNupROmmGJRoyzfG3ccAveqCBrwr/2yxQ5BVd/GTl5agOwSg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/raw-body": {
      "version": "3.0.1",
      "resolved": "https://registry.npmjs.org/raw-body/-/raw-body-3.0.1.tgz",
      "integrity": "sha512-9G8cA+tuMS75+6G/TzW8OtLzmBDMo8p1JRxN5AZ+LAp8uxGA8V8GZm4GQ4/N5QNQEnLmg6SS7wyuSmbKepiKqA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "bytes": "3.1.2",
        "http-errors": "2.0.0",
        "iconv-lite": "0.7.0",
        "unpipe": "1.0.0"
      },
      "engines": {
        "node": ">= 0.10"
      }
    },
    "node_modules/raw-body/node_modules/iconv-lite": {
      "version": "0.7.0",
      "resolved": "https://registry.npmjs.org/iconv-lite/-/iconv-lite-0.7.0.tgz",
      "integrity": "sha512-cf6L2Ds3h57VVmkZe+Pn+5APsT7FpqJtEhhieDCvrE2MK5Qk9MyffgQyuxQTm6BChfeZNtcOLHp9IcWRVcIcBQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "safer-buffer": ">= 2.1.2 < 3.0.0"
      },
      "engines": {
        "node": ">=0.10.0"
      },
      "funding": {
        "type": "opencollective",
        "url": "https://opencollective.com/express"
      }
    },
    "node_modules/require-from-string": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/require-from-string/-/require-from-string-2.0.2.tgz",
      "integrity": "sha512-Xf0nWe6RseziFMu+Ap9biiUbmplq6S9/p+7w7YXP/JBHhrUDDUhwa+vANyubuqfZWTveU//DYVGsDG7RKL/vEw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=0.10.0"
      }
    },
    "node_modules/resolve-pkg-maps": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/resolve-pkg-maps/-/resolve-pkg-maps-1.0.0.tgz",
      "integrity": "sha512-seS2Tj26TBVOC2NIc2rOe2y2ZO7efxITtLZcGSOnHHNOQ7CkiUBfw0Iw2ck6xkIhPwLhKNLS8BO+hEpngQlqzw==",
      "dev": true,
      "license": "MIT",
      "funding": {
        "url": "https://github.com/privatenumber/resolve-pkg-maps?sponsor=1"
      }
    },
    "node_modules/router": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/router/-/router-2.2.0.tgz",
      "integrity": "sha512-nLTrUKm2UyiL7rlhapu/Zl45FwNgkZGaCpZbIHajDYgwlJCOzLSk+cIPAnsEqV955GjILJnKbdQC1nVPz+gAYQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "debug": "^4.4.0",
        "depd": "^2.0.0",
        "is-promise": "^4.0.0",
        "parseurl": "^1.3.3",
        "path-to-regexp": "^8.0.0"
      },
      "engines": {
        "node": ">= 18"
      }
    },
    "node_modules/safe-buffer": {
      "version": "5.2.1",
      "resolved": "https://registry.npmjs.org/safe-buffer/-/safe-buffer-5.2.1.tgz",
      "integrity": "sha512-rp3So07KcdmmKbGvgaNxQSJr7bGVSVk5S9Eq1F+ppbRo70+YeaDxkw5Dd8NPN+GD6bjnYm2VuPuCXmpuYvmCXQ==",
      "funding": [
        {
          "type": "github",
          "url": "https://github.com/sponsors/feross"
        },
        {
          "type": "patreon",
          "url": "https://www.patreon.com/feross"
        },
        {
          "type": "consulting",
          "url": "https://feross.org/support"
        }
      ],
      "license": "MIT",
      "optional": true
    },
    "node_modules/safer-buffer": {
      "version": "2.1.2",
      "resolved": "https://registry.npmjs.org/safer-buffer/-/safer-buffer-2.1.2.tgz",
      "integrity": "sha512-YZo3K82SD7Riyi0E1EQPojLz7kpepnSQI9IyPbHHg1XXXevb5dJI7tpyN2ADxGcQbHG7vcyRHk0cbwqcQriUtg==",
      "license": "MIT",
      "optional": true
    },
    "node_modules/send": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/send/-/send-1.2.0.tgz",
      "integrity": "sha512-uaW0WwXKpL9blXE2o0bRhoL2EGXIrZxQ2ZQ4mgcfoBxdFmQold+qWsD2jLrfZ0trjKL6vOw0j//eAwcALFjKSw==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "debug": "^4.3.5",
        "encodeurl": "^2.0.0",
        "escape-html": "^1.0.3",
        "etag": "^1.8.1",
        "fresh": "^2.0.0",
        "http-errors": "^2.0.0",
        "mime-types": "^3.0.1",
        "ms": "^2.1.3",
        "on-finished": "^2.4.1",
        "range-parser": "^1.2.1",
        "statuses": "^2.0.1"
      },
      "engines": {
        "node": ">= 18"
      }
    },
    "node_modules/serve-static": {
      "version": "2.2.0",
      "resolved": "https://registry.npmjs.org/serve-static/-/serve-static-2.2.0.tgz",
      "integrity": "sha512-61g9pCh0Vnh7IutZjtLGGpTA355+OPn2TyDv/6ivP2h/AdAVX9azsoxmg2/M6nZeQZNYBEwIcsne1mJd9oQItQ==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "encodeurl": "^2.0.0",
        "escape-html": "^1.0.3",
        "parseurl": "^1.3.3",
        "send": "^1.2.0"
      },
      "engines": {
        "node": ">= 18"
      }
    },
    "node_modules/setprototypeof": {
      "version": "1.2.0",
      "resolved": "https://registry.npmjs.org/setprototypeof/-/setprototypeof-1.2.0.tgz",
      "integrity": "sha512-E5LDX7Wrp85Kil5bhZv46j8jOeboKq5JMmYM3gVGdGH8xFpPWXUMsNrlODCrkoxMEeNi/XZIwuRvY4XNwYMJpw==",
      "license": "ISC",
      "optional": true
    },
    "node_modules/shebang-command": {
      "version": "2.0.0",
      "resolved": "https://registry.npmjs.org/shebang-command/-/shebang-command-2.0.0.tgz",
      "integrity": "sha512-kHxr2zZpYtdmrN1qDjrrX/Z1rR1kG8Dx+gkpK1G4eXmvXswmcE1hTWBWYUzlraYw1/yZp6YuDY77YtvbN0dmDA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "shebang-regex": "^3.0.0"
      },
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/shebang-regex": {
      "version": "3.0.0",
      "resolved": "https://registry.npmjs.org/shebang-regex/-/shebang-regex-3.0.0.tgz",
      "integrity": "sha512-7++dFhtcx3353uBaq8DDR4NuxBetBzC7ZQOhmTQInHEd6bSrXdiEyzCvG07Z44UYdLShWUyXt5M/yhz8ekcb1A==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=8"
      }
    },
    "node_modules/side-channel": {
      "version": "1.1.0",
      "resolved": "https://registry.npmjs.org/side-channel/-/side-channel-1.1.0.tgz",
      "integrity": "sha512-ZX99e6tRweoUXqR+VBrslhda51Nh5MTQwou5tnUDgbtyM0dBgmhEDtWGP/xbKn6hqfPRHujUNwz5fy/wbbhnpw==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "es-errors": "^1.3.0",
        "object-inspect": "^1.13.3",
        "side-channel-list": "^1.0.0",
        "side-channel-map": "^1.0.1",
        "side-channel-weakmap": "^1.0.2"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-list": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/side-channel-list/-/side-channel-list-1.0.0.tgz",
      "integrity": "sha512-FCLHtRD/gnpCiCHEiJLOwdmFP+wzCmDEkc9y7NsYxeF4u7Btsn1ZuwgwJGxImImHicJArLP4R0yX4c2KCrMrTA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "es-errors": "^1.3.0",
        "object-inspect": "^1.13.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-map": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/side-channel-map/-/side-channel-map-1.0.1.tgz",
      "integrity": "sha512-VCjCNfgMsby3tTdo02nbjtM/ewra6jPHmpThenkTYh8pG9ucZ/1P8So4u4FGBek/BjpOVsDCMoLA/iuBKIFXRA==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "call-bound": "^1.0.2",
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.5",
        "object-inspect": "^1.13.3"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/side-channel-weakmap": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/side-channel-weakmap/-/side-channel-weakmap-1.0.2.tgz",
      "integrity": "sha512-WPS/HvHQTYnHisLo9McqBHOJk2FkHO/tlpvldyrnem4aeQp4hai3gythswg6p01oSoTl58rcpiFAjF2br2Ak2A==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "call-bound": "^1.0.2",
        "es-errors": "^1.3.0",
        "get-intrinsic": "^1.2.5",
        "object-inspect": "^1.13.3",
        "side-channel-map": "^1.0.1"
      },
      "engines": {
        "node": ">= 0.4"
      },
      "funding": {
        "url": "https://github.com/sponsors/ljharb"
      }
    },
    "node_modules/statuses": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/statuses/-/statuses-2.0.2.tgz",
      "integrity": "sha512-DvEy55V3DB7uknRo+4iOGT5fP1slR8wQohVdknigZPMpMstaKJQWhwiYBACJE3Ul2pTnATihhBYnRhZQHGBiRw==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/toidentifier": {
      "version": "1.0.1",
      "resolved": "https://registry.npmjs.org/toidentifier/-/toidentifier-1.0.1.tgz",
      "integrity": "sha512-o5sSPKEkg/DIQNmH43V0/uerLrpzVedkUh8tGNvaeXpfpuwjKenlSox/2O/BTlZUtEe+JG7s5YhEz608PlAHRA==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">=0.6"
      }
    },
    "node_modules/tsx": {
      "version": "4.20.6",
      "resolved": "https://registry.npmjs.org/tsx/-/tsx-4.20.6.tgz",
      "integrity": "sha512-ytQKuwgmrrkDTFP4LjR0ToE2nqgy886GpvRSpU0JAnrdBYppuY5rLkRUYPU1yCryb24SsKBTL/hlDQAEFVwtZg==",
      "dev": true,
      "license": "MIT",
      "dependencies": {
        "esbuild": "~0.25.0",
        "get-tsconfig": "^4.7.5"
      },
      "bin": {
        "tsx": "dist/cli.mjs"
      },
      "engines": {
        "node": ">=18.0.0"
      },
      "optionalDependencies": {
        "fsevents": "~2.3.3"
      }
    },
    "node_modules/type-is": {
      "version": "2.0.1",
      "resolved": "https://registry.npmjs.org/type-is/-/type-is-2.0.1.tgz",
      "integrity": "sha512-OZs6gsjF4vMp32qrCbiVSkrFmXtG/AZhY3t0iAMrMBiAZyV9oALtXO8hsrHbMXF9x6L3grlFuwW2oAz7cav+Gw==",
      "license": "MIT",
      "optional": true,
      "dependencies": {
        "content-type": "^1.0.5",
        "media-typer": "^1.1.0",
        "mime-types": "^3.0.0"
      },
      "engines": {
        "node": ">= 0.6"
      }
    },
    "node_modules/typescript": {
      "version": "5.9.3",
      "resolved": "https://registry.npmjs.org/typescript/-/typescript-5.9.3.tgz",
      "integrity": "sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==",
      "dev": true,
      "license": "Apache-2.0",
      "bin": {
        "tsc": "bin/tsc",
        "tsserver": "bin/tsserver"
      },
      "engines": {
        "node": ">=14.17"
      }
    },
    "node_modules/undici-types": {
      "version": "7.16.0",
      "resolved": "https://registry.npmjs.org/undici-types/-/undici-types-7.16.0.tgz",
      "integrity": "sha512-Zz+aZWSj8LE6zoxD+xrjh4VfkIG8Ya6LvYkZqtUQGJPZjYl53ypCaUwWqo7eI0x66KBGeRo+mlBEkMSeSZ38Nw==",
      "license": "MIT"
    },
    "node_modules/unpipe": {
      "version": "1.0.0",
      "resolved": "https://registry.npmjs.org/unpipe/-/unpipe-1.0.0.tgz",
      "integrity": "sha512-pjy2bYhSsufwWlKwPc+l3cN7+wuJlK6uz0YdJEOlQDbl6jo/YlPi4mb8agUkVC8BF7V8NuzeyPNqRksA3hztKQ==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/vary": {
      "version": "1.1.2",
      "resolved": "https://registry.npmjs.org/vary/-/vary-1.1.2.tgz",
      "integrity": "sha512-BNGbWLfd0eUPabhkXUVm0j8uuvREyTh5ovRa/dyow/BqAbZJyC+5fU+IzQOzmAKzYqYRAISoRhdQr3eIZ/PXqg==",
      "license": "MIT",
      "optional": true,
      "engines": {
        "node": ">= 0.8"
      }
    },
    "node_modules/which": {
      "version": "2.0.2",
      "resolved": "https://registry.npmjs.org/which/-/which-2.0.2.tgz",
      "integrity": "sha512-BLI3Tl1TW3Pvl70l3yq3Y64i+awpwXqsGBYWkkqMtnbXgrMD+yj7rhW0kuEDxzJaYXGjEW5ogapKNMEKNMjibA==",
      "license": "ISC",
      "optional": true,
      "dependencies": {
        "isexe": "^2.0.0"
      },
      "bin": {
        "node-which": "bin/node-which"
      },
      "engines": {
        "node": ">= 8"
      }
    },
    "node_modules/wrappy": {
      "version": "1.0.2",
      "resolved": "https://registry.npmjs.org/wrappy/-/wrappy-1.0.2.tgz",
      "integrity": "sha512-l4Sp/DRseor9wL6EvV2+TuQn63dMkPjZ/sp9XkghTEbV9KlPS1xUsZ3u7/IQO4wxtcFB4bgpQPRcR3QCvezPcQ==",
      "license": "ISC",
      "optional": true
    },
    "node_modules/ws": {
      "version": "8.18.3",
      "resolved": "https://registry.npmjs.org/ws/-/ws-8.18.3.tgz",
      "integrity": "sha512-PEIGCY5tSlUt50cqyMXfCzX+oOPqN0vuGqWzbcJ2xvnkzkq46oOpz7dQaTDBdfICb4N14+GARUDw2XV2N4tvzg==",
      "license": "MIT",
      "engines": {
        "node": ">=10.0.0"
      },
      "peerDependencies": {
        "bufferutil": "^4.0.1",
        "utf-8-validate": ">=5.0.2"
      },
      "peerDependenciesMeta": {
        "bufferutil": {
          "optional": true
        },
        "utf-8-validate": {
          "optional": true
        }
      }
    },
    "node_modules/zod": {
      "version": "3.25.76",
      "resolved": "https://registry.npmjs.org/zod/-/zod-3.25.76.tgz",
      "integrity": "sha512-gzUt/qt81nXsFGKIFcC3YnfEAx5NkunCfnDlvuBSSFS02bcXu4Lmea0AFIUwbLWxWPx3d9p8S5QoaujKcNQxcQ==",
      "license": "MIT",
      "funding": {
        "url": "https://github.com/sponsors/colinhacks"
      }
    },
    "node_modules/zod-to-json-schema": {
      "version": "3.24.6",
      "resolved": "https://registry.npmjs.org/zod-to-json-schema/-/zod-to-json-schema-3.24.6.tgz",
      "integrity": "sha512-h/z3PKvcTcTetyjl1fkj79MHNEjm+HpD6NXheWjzOekY7kV+lwDYnHw+ivHkijnCSMz1yJaWBD9vu/Fcmk+vEg==",
      "license": "ISC",
      "optional": true,
      "peerDependencies": {
        "zod": "^3.24.1"
      }
    }
  }
}




------------------------------------------
File: examples/openai-agents/research-assistant/package.json
------------------------------------------

{
  "name": "research-assistant-openai-agents",
  "version": "1.0.0",
  "description": "A research assistant built with OpenAI Agents SDK that answers database research questions",
  "type": "module",
  "main": "dist/cli.js",
  "bin": {
    "research-assistant": "dist/cli.js"
  },
  "scripts": {
    "build": "tsc",
    "ask": "tsx src/cli.ts",
    "dev": "tsx src/cli.ts"
  },
  "keywords": [
    "openai",
    "agent",
    "research",
    "database",
    "ai"
  ],
  "author": "",
  "license": "ISC",
  "engines": {
    "node": ">=20.9.0"
  },
  "dependencies": {
    "@openai/agents": "^0.3.1",
    "agentfs-sdk": "file:../../../sdk/typescript",
    "dotenv": "^16.4.7",
    "pdf-parse": "^1.1.1",
    "zod": "^3.24.1"
  },
  "devDependencies": {
    "@types/node": "^24.10.0",
    "@types/pdf-parse": "^1.1.4",
    "tsx": "^4.19.1",
    "typescript": "^5.9.3"
  }
}




------------------------------------------
File: examples/openai-agents/research-assistant/README.md
------------------------------------------

# Research Assistant (VLDB + SIGMOD)

A minimal research assistant that answers database research questions by finding most relevant papers from the VLDB and SIGMOD conference proceedings.

## Getting Started

Install dependencies:

```console
npm install
```

Ask a research question:

```
npm run ask -- "How do learned indexes compare to B+ trees?"
```




------------------------------------------
File: examples/openai-agents/research-assistant/src/agent.ts
------------------------------------------

import { Agent } from '@openai/agents';
import { semanticScholarTool } from './tools/semantic-tool.js';
import { pdfFetchTool, resolvePdfUrlTool } from './tools/pdf-tool.js';

export const researchAgent = new Agent({
  name: 'Research Assistant',
  instructions: `
You are a database research expert who provides comprehensive, evidence-based analysis.

When a user asks a research question:

1. **Search for papers**: Use the 'search_semanticscholar_vldb_sigmod' tool with the user's query to find relevant papers from VLDB and SIGMOD venues.

2. **Get full paper content**: For the most relevant papers (typically 3-5 papers):
   - If a paper has a pdfUrl, use the 'fetch_pdf_text' tool to get the complete paper text
   - If a paper only has a regular URL, first try 'resolve_pdf_url' to find the PDF link, then fetch it
   - Read the FULL TEXT carefully to extract specific findings, numbers, and comparisons
   - If PDF fetching fails, work with the abstract, but note the limitation

3. **Analyze the content**: Review the full paper texts, focusing on:
   - Key findings and performance metrics (extract specific numbers!)
   - Comparisons and benchmarks between systems
   - Trade-offs and limitations discussed by authors
   - Theoretical and practical insights
   - Methodologies and experimental setups

4. **Synthesize a comprehensive answer** that:
   - Directly answers the research question with specific evidence from the papers
   - Includes concrete performance numbers, comparisons, and trade-offs
   - Cites sources using the format (FirstAuthor, Year) - e.g., (Marcus, 2020), (Liu, 2024)
   - Uses semicolons for multiple citations: (Author1, Year1; Author2, Year2)
   - Highlights areas of agreement or disagreement between papers
   - Organizes findings into clear sections with headers like "Key Findings", "Performance Metrics", "Trade-offs"
   - Mentions when papers show different results or conflicting evidence

5. **Include a references section** at the end with full citations in this format:
   - AuthorLastName et al. (Year). Paper Title — Venue Year
   - URL

CRITICAL RULES:
- DO NOT just list papers. ANSWER the question with evidence!
- DO NOT give superficial summaries. Include SPECIFIC DETAILS from the papers!
- ALWAYS fetch and read full PDFs for thorough analysis - abstracts alone are insufficient!
- Include ACTUAL NUMBERS and metrics from the papers, not vague descriptions!
- Be thorough, accurate, and evidence-based in your analysis!
`,
  model: 'gpt-4o-mini',
  tools: [semanticScholarTool, pdfFetchTool, resolvePdfUrlTool],
});




------------------------------------------
File: examples/openai-agents/research-assistant/src/cli.ts
------------------------------------------

#!/usr/bin/env node
import 'dotenv/config';
import { run } from '@openai/agents';
import { researchAgent } from './agent.js';
import { getAgentFS } from './utils/agentfs.js';

async function main() {
  const args = process.argv.slice(2);
  const question = args.join(' ').trim();

  if (!question) {
    console.error('Usage: npm run ask -- "<your research question>"');
    process.exit(1);
  }

  try {
    // Initialize agentfs
    await getAgentFS();

    // Run the agent with the question
    const result = await run(researchAgent, question);

    // Output the agent's response
    console.log(result.finalOutput);

  } catch (err: any) {
    console.error('Error:', err?.message || err);
    process.exit(1);
  }
}

main();




------------------------------------------
File: examples/openai-agents/research-assistant/src/tools/pdf-tool.ts
------------------------------------------

import { tool } from '@openai/agents';
import { z } from 'zod';
import { fetchPdfTextCached, resolvePdfUrl } from '../utils/pdf.js';
import { getAgentFS } from '../utils/agentfs.js';

export const pdfFetchTool = tool({
  name: 'fetch_pdf_text',
  description:
    'Fetches and parses full text content from a research paper PDF URL. Use this when you need the complete paper content for detailed analysis beyond the abstract.',
  parameters: z.object({
    pdfUrl: z.string().describe('Direct URL to the PDF file'),
    paperTitle: z.string().describe('Title of the paper (used for caching)'),
    year: z.number().describe('Publication year (used for caching)'),
  }),
  execute: async ({ pdfUrl, paperTitle, year }) => {
    // Generate cache key from title and year
    const cacheKey = `${paperTitle}-${year}`
      .toLowerCase()
      .replace(/[^a-z0-9-_]+/g, '-')
      .replace(/-+/g, '-')
      .replace(/^-|-$/g, '')
      .slice(0, 150);

    const agentfs = await getAgentFS();

    // Check if already cached
    const txtPath = `/papers/${cacheKey}.txt`;
    try {
      const cached = await agentfs.fs.readFile(txtPath);
      if (cached && cached.length > 300) {
        return { text: cached, cached: true };
      }
    } catch {
      // Not cached, proceed to fetch
    }

    // Fetch and parse PDF
    const text = await fetchPdfTextCached(cacheKey, pdfUrl);

    if (!text || text.length < 300) {
      throw new Error('PDF text extraction failed or returned insufficient content');
    }

    return { text, cached: false };
  },
});

export const resolvePdfUrlTool = tool({
  name: 'resolve_pdf_url',
  description:
    'Attempts to resolve a landing page URL (like a DOI or ACM DL page) to a direct PDF URL. Use this when you have a paper URL but need the PDF link.',
  parameters: z.object({
    url: z.string().describe('Landing page URL for the paper'),
  }),
  execute: async ({ url }) => {
    const pdfUrl = await resolvePdfUrl(url);

    return {
      pdfUrl: pdfUrl || undefined,
      success: !!pdfUrl,
    };
  },
});




------------------------------------------
File: examples/openai-agents/research-assistant/src/tools/semantic-tool.ts
------------------------------------------

import { tool } from '@openai/agents';
import { z } from 'zod';

export const paperSchema = z.object({
  paperId: z.string(),
  title: z.string(),
  authors: z.array(z.string()),
  venue: z.string(),
  year: z.number(),
  url: z.string().url(),
  abstractText: z.string().optional(),
  pdfUrl: z.string().url().optional(),
});
export type Paper = z.infer<typeof paperSchema>;

type S2Author = { name?: string };
type S2Venue = { displayName?: string } | null;
type S2ExternalIds = { DOI?: string | null };
type S2Paper = {
  paperId?: string;
  title?: string;
  year?: number;
  authors?: S2Author[];
  url?: string | null;
  abstract?: string | null;
  venue?: string | null;
  publicationVenue?: S2Venue;
  externalIds?: S2ExternalIds;
  isOpenAccess?: boolean;
  openAccessPdf?: { url?: string | null } | null;
};

type S2SearchResponse = {
  data?: S2Paper[];
  total?: number;
};

function isTargetVenue(venue?: string | null): boolean {
  if (!venue) return false;
  const v = venue.toLowerCase();
  return (
    v.includes('sigmod') ||
    v.includes('acm sigmod') ||
    v.includes('sigmod conference') ||
    v.includes('proceedings of the vldb endowment') ||
    v.includes('pvl db') ||
    v.includes('pvlb') ||
    v.includes('pvlbd') ||
    v.includes('vldb') // keep broad to catch PVLDB; risk: VLDB Journal may appear
  );
}

async function sleep(ms: number) {
  return new Promise((r) => setTimeout(r, ms));
}

async function fetchWithBackoff(url: string, init: RequestInit, maxRetries = 5, initialDelayMs = 500): Promise<Response> {
  let attempt = 0;
  let delay = initialDelayMs;
  while (true) {
    const res = await fetch(url, init);
    if (res.status !== 429 && res.status < 500) {
      return res;
    }
    if (attempt >= maxRetries) {
      return res;
    }
    // Prefer Retry-After header if present
    const retryAfter = res.headers.get('Retry-After');
    let waitMs = delay;
    if (retryAfter) {
      const secs = Number(retryAfter);
      if (!Number.isNaN(secs) && secs > 0) waitMs = secs * 1000;
    }
    await sleep(waitMs);
    attempt += 1;
    delay = Math.min(delay * 2, 8000);
  }
}

export async function searchSemanticScholar(query: string): Promise<Paper[]> {
  const base = 'https://api.semanticscholar.org/graph/v1/paper/search';
  const params = new URLSearchParams({
    query,
    limit: '100',
    fields: 'title,venue,publicationVenue,year,authors,url,abstract,externalIds,isOpenAccess,openAccessPdf',
  });
  const url = `${base}?${params.toString()}`;
  const res = await fetchWithBackoff(
    url,
    {
      headers: {
        Accept: 'application/json',
        'User-Agent': 'research-assistant',
      },
    },
    5,
    500,
  );
  if (!res.ok) {
    throw new Error(`Semantic Scholar request failed with status ${res.status}`);
  }
  const data = (await res.json()) as S2SearchResponse;
  const arr = data.data || [];
  const mapped: Paper[] = arr
    .map((p) => {
      const title = (p.title || '').trim();
      const authors = (p.authors || []).map((a) => (a.name || '').trim()).filter(Boolean);
      const venue =
        (p.publicationVenue?.displayName || p.venue || '').toString().trim();
      const year = Number(p.year || 0);
      const doi = p.externalIds?.DOI || null;
      const rawUrl = (doi ? `https://doi.org/${doi}` : p.url || '').toString().trim();
      const url = rawUrl;
      const paperId = (doi || p.paperId || url || title).toString();
      const abstractText = (p.abstract || undefined)?.toString();
      let pdfUrl =
        (p.openAccessPdf?.url || undefined) ||
        (doi && doi.toLowerCase().startsWith('10.48550/arxiv.')
          ? `https://arxiv.org/pdf/${doi.toLowerCase().split('arxiv.')[1]}.pdf`
          : undefined);
      // Fallback: if the URL is an arXiv abstract, derive the PDF URL
      try {
        const u = new URL(url);
        if (u.host.includes('arxiv.org')) {
          const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
          if (absMatch && absMatch[1]) {
            pdfUrl = `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
          } else {
            const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
            if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
              pdfUrl = `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
            }
          }
        }
      } catch {
        // ignore URL parse errors
      }
      return {
        paperId,
        title,
        authors,
        venue,
        year,
        url,
        abstractText,
        pdfUrl,
      } as Paper;
    })
    .filter((p) => p.title && p.url);
  // Filter to target venues and dedupe by URL
  const filtered = mapped.filter((p) => isTargetVenue(p.venue));
  const seen = new Set<string>();
  const deduped = filtered.filter((p) => {
    const key = p.url;
    if (seen.has(key)) return false;
    seen.add(key);
    return true;
  });
  return deduped;
}

export const semanticScholarTool = tool({
  name: 'search_semanticscholar_vldb_sigmod',
  description:
    'Search Semantic Scholar for papers relevant to a query, constrained to VLDB (PVLDB) and SIGMOD venues',
  parameters: z.object({
    query: z.string().describe('User research question or keywords'),
  }),
  execute: async ({ query }) => {
    const papers = await searchSemanticScholar(query);
    return { papers };
  },
});




------------------------------------------
File: examples/openai-agents/research-assistant/src/utils/agentfs.ts
------------------------------------------

import { AgentFS } from 'agentfs-sdk';

let instance: AgentFS | null = null;

export async function getAgentFS(): Promise<AgentFS> {
  if (!instance) {
    const id = process.env.AGENTFS_ID || 'research-assistant';
    instance = await AgentFS.open({ id });
  }
  return instance;
}




------------------------------------------
File: examples/openai-agents/research-assistant/src/utils/pdf.ts
------------------------------------------

import pdfParse from 'pdf-parse';
import { getAgentFS } from './agentfs.js';

async function fetchHtmlWithFallback(url: string): Promise<string> {
  const res = await fetch(url, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      'Accept-Language': 'en-US,en;q=0.9',
      Referer: url,
    },
  });
  const text = await res.text();
  if (!res.ok || /Access Denied|Robot Check|unusual traffic/i.test(text)) {
    try {
      const u = new URL(url);
      const proxy = `https://r.jina.ai/http://${u.host}${u.pathname}${u.search || ''}`;
      const proxRes = await fetch(proxy, { headers: { Accept: 'text/plain' } });
      if (proxRes.ok) {
        return await proxRes.text();
      }
    } catch {
      // ignore
    }
  }
  return text;
}

export async function fetchPdfText(pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  const parsed = await pdfParse(buf);
  return parsed.text || '';
}

export async function fetchPdfTextCached(cacheKey: string, pdfUrl: string, maxBytes = 20_000_000): Promise<string> {
  const fs = (await getAgentFS()).fs;
  const safeKey = cacheKey.replace(/[^a-z0-9-_]/gi, '_').toLowerCase().slice(0, 200);
  const pdfPath = `/papers/${safeKey}.pdf`;
  const txtPath = `/papers/${safeKey}.txt`;
  // Try text cache
  try {
    const text = (await fs.readFile(txtPath, 'utf8')) as string;
    if (text && text.length > 0) return text;
  } catch {
    // not cached
  }
  // Download PDF
  const res = await fetch(pdfUrl, {
    headers: {
      'User-Agent':
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
      Accept: 'application/pdf',
      Referer: pdfUrl,
    },
  });
  if (!res.ok) {
    throw new Error(`PDF fetch failed: ${res.status}`);
  }
  const buf = Buffer.from(await res.arrayBuffer());
  if (buf.byteLength > maxBytes) {
    throw new Error(`PDF too large: ${buf.byteLength} bytes`);
  }
  // Cache PDF
  try {
    await fs.writeFile(pdfPath, buf);
  } catch {
    // ignore cache write error
  }
  const parsed = await pdfParse(buf);
  const text = parsed.text || '';
  // Cache text
  try {
    await fs.writeFile(txtPath, text);
  } catch {
    // ignore
  }
  return text;
}

export function clipText(text: string, maxChars = 8000): string {
  if (text.length <= maxChars) return text;
  return text.slice(0, maxChars);
}

export async function resolvePdfUrl(landingUrl: string): Promise<string | undefined> {
  try {
    const u = new URL(landingUrl);
    // Direct PDF link
    if (/\.(pdf)(\?|#|$)/i.test(u.pathname)) {
      return landingUrl;
    }
    // arXiv: handle abs -> pdf and pdf without extension
    if (u.host.includes('arxiv.org')) {
      const absMatch = u.pathname.match(/^\/abs\/(.+)$/i);
      if (absMatch && absMatch[1]) {
        return `https://arxiv.org/pdf/${absMatch[1]}.pdf`;
      }
      const pdfMatch = u.pathname.match(/^\/pdf\/(.+)$/i);
      if (pdfMatch && pdfMatch[1] && !pdfMatch[1].endsWith('.pdf')) {
        return `https://arxiv.org/pdf/${pdfMatch[1]}.pdf`;
      }
    }
    // ACM DL DOI landing page
    if (u.host.includes('dl.acm.org') && /\/doi\//.test(u.pathname)) {
      const html = await fetchHtmlWithFallback(landingUrl);
      // 1) meta citation_pdf_url
      const meta = html.match(/<meta\s+name=["']citation_pdf_url["']\s+content=["']([^"']+)["']/i);
      if (meta?.[1]) {
        const abs = new URL(meta[1], landingUrl).toString();
        return abs;
      }
      // 2) explicit /doi/pdf or /doi/pdfdirect links
      const m = html.match(/href=["'](\/doi\/pdf(?:direct)?\/[^"']+)["']/i);
      if (m?.[1]) {
        const abs = new URL(m[1], landingUrl).toString();
        return abs;
      }
      // 3) try OpenGraph pdf link
      const og = html.match(/<meta\s+property=["']og:url["']\s+content=["']([^"']+)["']/i);
      if (og?.[1] && /\/doi\/pdf/.test(og[1])) {
        return new URL(og[1], landingUrl).toString();
      }
    }
    // DOI resolver: if arXiv DOI, map to arXiv pdf
    if (u.host.includes('doi.org')) {
      const arxivId = landingUrl.match(/10\.48550\/arXiv\.(\d+\.\d+)(v\d+)?/i);
      if (arxivId?.[1]) {
        return `https://arxiv.org/pdf/${arxivId[1]}.pdf`;
      }
    }
  } catch {
    // ignore
  }
  return undefined;
}




------------------------------------------
File: examples/openai-agents/research-assistant/tsconfig.json
------------------------------------------

{
  "compilerOptions": {
    "target": "ES2022",
    "module": "ESNext",
    "lib": ["ES2022"],
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "allowJs": true,
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "outDir": "./dist",
    "rootDir": "./src",
    "declaration": true,
    "declarationMap": true,
    "sourceMap": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}




------------------------------------------
File: MANUAL.md
------------------------------------------

# AgentFS Reference Guide

Command-line reference for the AgentFS CLI.

For guides, tutorials, and SDK documentation, see [docs.turso.tech/agentfs](https://docs.turso.tech/agentfs).

## Installation

```bash
curl -fsSL https://github.com/tursodatabase/agentfs/releases/latest/download/agentfs-installer.sh | sh
```

## Commands

### agentfs init

Initialize a new agent filesystem.

```
agentfs init [OPTIONS] [ID]
```

**Arguments:**
- `ID` - Agent identifier (default: `agent-{timestamp}`)

**Options:**
- `--force` - Overwrite existing agent filesystem
- `--base <PATH>` - Base directory for overlay filesystem (copy-on-write)
- `--key <KEY>` - Hex-encoded encryption key for local encryption
- `--cipher <CIPHER>` - Cipher algorithm (required with `--key`)
- `--sync-remote-url <URL>` - Remote Turso database URL for sync
- `--sync-partial-prefetch` - Enable prefetching for partial sync
- `--sync-partial-segment-size <SIZE>` - Segment size for partial sync
- `--sync-partial-bootstrap-query <QUERY>` - Custom bootstrap query
- `--sync-partial-bootstrap-length <LENGTH>` - Bootstrap prefix length

**Note:** Local encryption and cloud sync cannot be used together.

**Options (continued):**
- `-c, --command <CMD>` - Command to execute after initialization (see below)
- `--backend <BACKEND>` - Mount backend for `-c` option (`fuse` or `nfs`)

**Running a command after init:**

The `-c` option initializes the filesystem, mounts it to a temporary directory, runs the specified command with that directory as the working directory, then automatically unmounts.

```bash
# Initialize and run a command in the new filesystem
agentfs init my-agent -c "touch hello.txt && ls -la"

# With overlay filesystem
agentfs init my-overlay --base /path/to/project -c "make build"
```

### agentfs exec

Execute a command with an AgentFS filesystem mounted (Unix only).

```
agentfs exec [OPTIONS] <ID_OR_PATH> <COMMAND> [ARGS]...
```

Mounts the specified AgentFS to a temporary directory, runs the command with that directory as the working directory, then automatically unmounts. This is useful for running tools that need filesystem access without a persistent mount.

If the AgentFS was initialized with `--base` (overlay mode), the overlay filesystem is used automatically.

**Arguments:**
- `ID_OR_PATH` - Agent identifier or database path
- `COMMAND` - Command to execute
- `ARGS` - Arguments for the command

**Options:**
- `--backend <BACKEND>` - Mount backend (`fuse` on Linux, `nfs` on macOS by default)
- `--key <KEY>` - Hex-encoded encryption key for encrypted databases
- `--cipher <CIPHER>` - Cipher algorithm (required with `--key`)

**Examples:**

```bash
# Run ls in the AgentFS root
agentfs exec my-agent ls -la

# Run a build command
agentfs exec my-overlay make build

# With encryption
agentfs exec my-agent --key $KEY --cipher aes256gcm cat /config.json
```

### agentfs run

Execute a program in a sandboxed environment with copy-on-write filesystem.

```
agentfs run [OPTIONS] <COMMAND> [ARGS]...
```

**Options:**
- `--session <ID>` - Named session for persistence across runs
- `--allow <PATH>` - Allow write access to additional directories (repeatable)
- `--no-default-allows` - Disable default allowed directories
- `--key <KEY>` - Hex-encoded encryption key for delta layer
- `--cipher <CIPHER>` - Cipher algorithm (required with `--key`)
- `--experimental-sandbox` - Use ptrace-based syscall interception (Linux only)
- `--strace` - Show intercepted syscalls (requires `--experimental-sandbox`)

**Platform behavior:**

Linux uses FUSE + overlay filesystem with user namespaces. macOS uses NFS + overlay filesystem with Apple's Sandbox.

Default allowed directories (macOS): `~/.claude`, `~/.codex`, `~/.config`, `~/.cache`, `~/.local`, `~/.npm`, `/tmp`

### agentfs mount

Mount an agent filesystem or list mounted filesystems.

```
agentfs mount [OPTIONS] [ID_OR_PATH] [MOUNT_POINT]
```

Without arguments, lists all mounted agentfs filesystems.

**Options:**
- `-a, --auto-unmount` - Automatically unmount on exit
- `--allow-root` - Allow root user to access filesystem
- `-f, --foreground` - Run in foreground
- `--uid <UID>` - User ID for all files
- `--gid <GID>` - Group ID for all files

**Unmounting:**
- Linux: `fusermount -u <MOUNT_POINT>`
- macOS: `umount <MOUNT_POINT>`

### agentfs serve mcp

Start an MCP (Model Context Protocol) server.

```
agentfs serve mcp <ID_OR_PATH> [OPTIONS]
```

**Options:**
- `--tools <TOOLS>` - Comma-separated list of tools to expose (default: all)

**Available tools:**

Filesystem: `read_file`, `write_file`, `readdir`, `mkdir`, `remove`, `rename`, `stat`, `access`

Key-Value: `kv_get`, `kv_set`, `kv_delete`, `kv_list`

### agentfs serve nfs

Start an NFS server to export AgentFS over the network.

```
agentfs serve nfs <ID_OR_PATH> [OPTIONS]
```

**Options:**
- `--bind <IP>` - IP address to bind (default: `127.0.0.1`)
- `--port <PORT>` - Port to listen on (default: `11111`)

**Mounting from client:**
```bash
mount -t nfs -o vers=3,tcp,port=11111,mountport=11111,nolock <HOST>:/ <MOUNT_POINT>
```

### agentfs sync

Synchronize agent filesystem with a remote Turso database.

```
agentfs sync <ID_OR_PATH> <SUBCOMMAND>
```

**Subcommands:**
- `pull` - Pull remote changes
- `push` - Push local changes
- `stats` - View sync statistics
- `checkpoint` - Create checkpoint

### agentfs migrate

Migrate database schema to the current version.

```
agentfs migrate [OPTIONS] <ID_OR_PATH>
```

Upgrades an AgentFS database schema to the latest version. This is necessary when using databases created with older versions of AgentFS.

**Arguments:**
- `ID_OR_PATH` - Agent identifier or database path

**Options:**
- `--dry-run` - Preview migration without applying changes

**Examples:**

```bash
# Preview pending migrations
agentfs migrate my-agent --dry-run

# Apply migrations
agentfs migrate my-agent

# Migrate using database path
agentfs migrate .agentfs/my-agent.db
```

**Output:**

The command displays the current and target schema versions, then applies any necessary migrations:

```
Database: .agentfs/my-agent.db
Current schema version: v0.2
Target schema version: v0.4

Applying migrations...
  Migrating v0.2 -> v0.4...
    Added atime_nsec column to fs_inode
    Added mtime_nsec column to fs_inode
    Added ctime_nsec column to fs_inode
    Added rdev column to fs_inode
  v0.2 -> v0.4 migration complete.

Migration completed successfully.
```

**Notes:**
- Migrations are idempotent and safe to run multiple times
- Always backup your database before running migrations on production data

### agentfs fs

Filesystem operations on agent databases.

**Common Options:**
- `--key <KEY>` - Hex-encoded encryption key for encrypted databases
- `--cipher <CIPHER>` - Cipher algorithm (required with `--key`)

#### agentfs fs ls

```
agentfs fs <ID_OR_PATH> [OPTIONS] ls [FS_PATH]
```

List files and directories. Output: `f <name>` for files, `d <name>` for directories.

#### agentfs fs cat

```
agentfs fs <ID_OR_PATH> [OPTIONS] cat <FILE_PATH>
```

Display file contents.

#### agentfs fs write

```
agentfs fs <ID_OR_PATH> [OPTIONS] write <FILE_PATH> <CONTENT>
```

Write content to a file.

### agentfs diff

Show filesystem changes in overlay mode.

```
agentfs diff <ID_OR_PATH>
```

### agentfs timeline

Display agent action timeline from the tool call audit log.

```
agentfs timeline [OPTIONS] <ID_OR_PATH>
```

**Options:**
- `--limit <N>` - Limit entries (default: 100)
- `--filter <TOOL>` - Filter by tool name
- `--status <STATUS>` - Filter by status: `pending`, `success`, `error`
- `--format <FORMAT>` - Output format: `table`, `json` (default: table)

### agentfs completions

Manage shell completions.

```
agentfs completions install [SHELL]
agentfs completions uninstall [SHELL]
agentfs completions show
```

Supported shells: `bash`, `zsh`, `fish`, `powershell`

## Environment Variables

**Configuration variables:**

| Variable | Description |
|----------|-------------|
| `AGENTFS_KEY` | Default encryption key (hex-encoded) |
| `AGENTFS_CIPHER` | Default cipher algorithm |
| `TURSO_DB_AUTH_TOKEN` | Authentication token for cloud sync |

**Variables set inside the sandbox:**

| Variable | Description |
|----------|-------------|
| `AGENTFS` | Set to `1` inside AgentFS sandbox |
| `AGENTFS_SANDBOX` | Sandbox type: `macos-sandbox` or `linux-namespace` |
| `AGENTFS_SESSION` | Current session ID |

## Local Encryption

AgentFS supports encrypting the local SQLite database at rest using libSQL's encryption feature.

**Supported ciphers:**
- `aes256gcm` - AES-256-GCM (requires 64-character hex key)
- `aes128gcm` - AES-128-GCM (requires 32-character hex key)
- `aegis256` - AEGIS-256 (requires 64-character hex key)
- `aegis128l` - AEGIS-128L (requires 32-character hex key)
- `aegis128x2`, `aegis128x4`, `aegis256x2`, `aegis256x4` - AEGIS variants

**Example: Create an encrypted filesystem**

```bash
# Generate a 256-bit key (64 hex characters)
KEY=$(openssl rand -hex 32)

# Initialize with encryption
agentfs init --key $KEY --cipher aes256gcm my-secure-agent

# Access the filesystem
agentfs fs my-secure-agent --key $KEY --cipher aes256gcm ls /
```

**Example: Encrypted sandbox session**

```bash
agentfs run --key $KEY --cipher aes256gcm -- bash
```

**Using environment variables:**

```bash
export AGENTFS_KEY=$(openssl rand -hex 32)
export AGENTFS_CIPHER=aes256gcm

agentfs init my-secure-agent
agentfs fs my-secure-agent ls /
```

**Limitations:**
- Local encryption cannot be used with cloud sync (`--sync-remote-url`)

## Files

- `.agentfs/<ID>.db` - Agent filesystem database
- `~/.config/agentfs/` - Configuration directory

## See Also

- [AgentFS Documentation](https://docs.turso.tech/agentfs) - Guides, tutorials, SDK docs
- [AgentFS Specification](SPEC.md) - SQLite schema specification
- [GitHub Repository](https://github.com/tursodatabase/agentfs) - Source code and examples




------------------------------------------
File: README.md
------------------------------------------

<p align="center">
  <h1 align="center">AgentFS</h1>
</p>

<p align="center">
  The filesystem for agents.
</p>

<p align="center">
  <a title="Build Status" target="_blank" href="https://github.com/tursodatabase/agentfs/actions/workflows/rust.yml"><img src="https://img.shields.io/github/actions/workflow/status/tursodatabase/agentfs/rust.yml?style=flat-square"></a>
  <a title="Rust" target="_blank" href="https://crates.io/crates/agentfs-sdk"><img alt="Crate" src="https://img.shields.io/crates/v/agentfs-sdk"></a>
  <a title="JavaScript" target="_blank" href="https://www.npmjs.com/package/agentfs-sdk"><img alt="NPM" src="https://img.shields.io/npm/v/agentfs-sdk"></a>
  <a title="Python" target="_blank" href="https://pypi.org/project/agentfs-sdk/"><img alt="PyPI" src="https://img.shields.io/pypi/v/agentfs-sdk"></a>
  <a title="MIT" target="_blank" href="https://github.com/tursodatabase/agentfs/blob/main/LICENSE.md"><img src="http://img.shields.io/badge/license-MIT-orange.svg?style=flat-square"></a>
</p>
<p align="center">
  <a title="Users's Discord" target="_blank" href="https://tur.so/discord"><img alt="Chat with other users of Turso (and Turso Cloud) on Discord" src="https://img.shields.io/discord/933071162680958986?label=Discord&logo=Discord&style=social&label=Users"></a>
</p>

---

> **⚠️ Warning:** This software is in BETA. It may still contain bugs and unexpected behavior. Use caution with production data and ensure you have backups.

## 🎯 What is AgentFS?

AgentFS is a filesystem explicitly designed for AI agents. Just as traditional filesystems provide file and directory abstractions for applications, AgentFS provides the storage abstractions that AI agents need.

The AgentFS repository consists of the following:

* **SDK** - [TypeScript](sdk/typescript), [Python](sdk/python), and [Rust](sdk/rust) libraries for programmatic filesystem access.
* **[CLI](MANUAL.md)** - Command-line interface for managing agent filesystems:
  - Mount AgentFS on host filesystem with FUSE on Linux and NFS on macOS.
  - Access AgentFS files with a command line tool.
* **[AgentFS Specification](SPEC.md)** - SQLite-based agent filesystem specification.

## 💡 Why AgentFS?

AgentFS provides the following benefits for agent state management:

* **Auditability**: Every file operation, tool call, and state change is recorded in a SQLite database file. Query your agent's complete history with SQL to debug issues, analyze behavior, or meet compliance requirements.
* **Reproducibility**: Snapshot an agent's state at any point with cp agent.db snapshot.db. Restore it later to reproduce exact execution states, test what-if scenarios, or roll back mistakes.
* **Portability**: The entire agent runtime—files, state, history —is stored in a single SQLite file. Move it between machines, check it into version control, or deploy it to any system where Turso runs.

Read more about the motivation for AgentFS in the announcement [blog post](https://turso.tech/blog/agentfs).

## 🧑‍💻 Getting Started

### Using the CLI

Install the AgentFS CLI:

```bash
curl -fsSL https://agentfs.ai/install | bash
```

Initialize an agent filesystem:

```bash
$ agentfs init my-agent
Created agent filesystem: .agentfs/my-agent.db
Agent ID: my-agent
```

Inspect the agent filesystem:

```bash
$ agentfs fs ls my-agent
Using agent: my-agent
f hello.txt

$ agentfs fs cat my-agent hello.txt
hello from agent
```

You can also use a database path directly:

```bash
$ agentfs fs cat .agentfs/my-agent.db hello.txt
hello from agent
```

View the agent's action timeline:

```bash
$ agentfs timeline my-agent
ID   TOOL                 STATUS       DURATION STARTED
4    execute_code         pending            -- 2024-01-05 09:44:20
3    api_call             error           300ms 2024-01-05 09:44:15
2    read_file            success          50ms 2024-01-05 09:44:10
1    web_search           success        1200ms 2024-01-05 09:43:45
```

You can mount an agent filesystem using FUSE (Linux) or NFS (macOS):

```bash
$ agentfs mount my-agent ./mnt
$ echo "hello" > ./mnt/hello.txt
$ cat ./mnt/hello.txt
hello
```

You can also run a program in an experimental sandbox with the agent filesystem mounted at `/agent`:

```bash
$ agentfs run /bin/bash
Welcome to AgentFS!

$ echo "hello from agent" > /agent/hello.txt
$ cat /agent/hello.txt
hello from agent
$ exit
```

Read the **[User Manual](MANUAL.md)** for complete documentation.

### Using the SDK

Install the SDK in your project:

```bash
npm install agentfs-sdk
```

Use it in your agent code:

```typescript
import { AgentFS } from 'agentfs-sdk';

// Persistent storage with identifier
const agent = await AgentFS.open({ id: 'my-agent' });
// Creates: .agentfs/my-agent.db

// Or use ephemeral in-memory database
const ephemeralAgent = await AgentFS.open();

// Key-value operations
await agent.kv.set('user:preferences', { theme: 'dark' });
const prefs = await agent.kv.get('user:preferences');

// Filesystem operations
await agent.fs.writeFile('/output/report.pdf', pdfBuffer);
const files = await agent.fs.readdir('/output');

// Tool call tracking
await agent.tools.record(
  'web_search',
  Date.now() / 1000,
  Date.now() / 1000 + 1.5,
  { query: 'AI' },
  { results: [...] }
);
```

### Examples

This source repository also contains examples that demonstrate how to integrate AgentFS with some popular AI frameworks:

- **[Mastra](examples/mastra/research-assistant)** - Research assistant using the Mastra AI framework
- **[Claude Agent SDK](examples/claude-agent/research-assistant)** - Research assistant using Anthropic's Claude Agent SDK
- **[OpenAI Agents](examples/openai-agents/research-assistant)** - Research assistant using OpenAI Agents SDK
- **[Firecracker](examples/firecracker)** - Minimal Firecracker VM with AgentFS mounted via NFSv3
- **[AI SDK + just-bash](examples/ai-sdk-just-bash)** - Interactive AI agent using Vercel AI SDK with just-bash for command execution
- **[Cloudflare Workers](examples/cloudflare)** - AI agent using AI SDK + just-bash on Cloudflare Workers with Durable Objects storage

See the **[examples](examples)** directory for more details.

## 🔧 How AgentFS Works?

<img align="right" width="40%" src=".github/assets/agentfs-arch.svg">

AgentFS is an agent filesystem accessible through an SDK that provides three essential interfaces for agent state management:

* **Filesystem:** A POSIX-like filesystem for files and directories
* **Key-Value:** A key-value store for agent state and context
* **Toolcall:** A toolcall audit trail for debugging and analysis

At the heart of AgentFS is the [agent filesystem](SPEC.md), a complete SQLite-based storage system for agents implemented using [Turso](https://github.com/tursodatabase/turso). Everything an agent does—every file it creates, every piece of state it stores, every tool it invokes—lives in a single SQLite database file.

## 🤔 FAQ

### How is AgentFS different from _X_?

[Bubblewrap](https://github.com/containers/bubblewrap) provides filesystem isolation using Linux namespaces and overlays. While you could achieve similar isolation with a `bwrap` call that mounts `/` read-only and uses `--tmp-overlay` on the working directory, the key difference is persistence and queryability: with AgentFS, the upper filesystem is stored in a single SQLite database file, which you can query, snapshot, and move to another machine. Read more about the motivation in the announcement [blog post](https://turso.tech/blog/agentfs).

[Docker Sandbox](https://www.docker.com/blog/docker-sandboxes-a-new-approach-for-coding-agent-safety/) and AgentFS are complementary rather than competing. AgentFS answers "what happened and what's the state?" while Docker Sandboxes answer "how do I run this safely?" You could use both together: run an agent inside a Docker Sandbox for security, while using AgentFS inside that sandbox for structured state management and audit trails.

[Git worktrees](https://git-scm.com/docs/git-worktree) let you check out multiple branches of a repository into separate directories, allowing agents to work on independent copies of the source code—similar to AgentFS. But AgentFS solves the problem at a lower level. With git worktrees, nothing prevents an agent from modifying files outside its worktree: another agent's worktree, system files, or anything else on the filesystem. The isolation is purely conventional, not enforced. AgentFS provides filesystem-level copy-on-write isolation that's system-wide and cannot be bypassed—letting you safely run untrusted agents. And because it operates below git, it also handles untracked files, making it useful beyond just version-controlled source code.

### Why implement AgentFS at the filesystem layer instead of using containers or VMs?

The filesystem layer gives us capabilities that block devices can't. First, because everything is stored in structured SQLite tables, you can query the filesystem, which is essential for auditability and debugging agent behavior. Second, SQLite's write-ahead log enables snapshotting and time-travel forking by capturing every filesystem change. Third, we can provide an SDK that works in environments such as serverless or the browser, where there's no way to mount a block device at all. Note that this approach works fine with containers and VMs too—you can use AgentFS via remote filesystem protocols like NFS or through mechanisms like virtio-fuse.

## 📚 Learn More

- **[User Manual](MANUAL.md)** - Complete guide to using the AgentFS CLI and SDK
- **[Agent Filesystem Specification](SPEC.md)** - Technical specification of the agent filesystem SQLite schema
- **[SDK Examples](examples/)** - Working code examples using AgentFS
- **[Turso database](https://github.com/tursodatabase/turso)** - an in-process SQL database, compatible with SQLite.

### Blog Posts

- **[Introducing AgentFS](https://turso.tech/blog/agentfs)** - The motivation behind AgentFS
- **[AgentFS with FUSE](https://turso.tech/blog/agentfs-fuse)** - Mounting agent filesystems using FUSE
- **[AgentFS with Overlay Filesystem](https://turso.tech/blog/agentfs-overlay)** - Sandboxing agents with copy-on-write overlays
- **[AI Agents with Just Bash](https://turso.tech/blog/agentfs-just-bash)** - Safe bash command execution for agents
- **[AgentFS in the Browser](https://turso.tech/blog/agentfs_browser)** - Running AgentFS in browsers with WebAssembly
- **[Making Coding Agents Safe Using LlamaIndex](https://www.llamaindex.ai/blog/making-coding-agents-safe-using-llamaindex)** - Using AgentFS with LlamaIndex

## 📝 License

MIT




------------------------------------------
File: sdk/python/agentfs_sdk/__init__.py
------------------------------------------

"""AgentFS Python SDK

A filesystem and key-value store for AI agents, powered by SQLite.
"""

from .agentfs import AgentFS, AgentFSOptions
from .errors import ErrnoException, FsErrorCode, FsSyscall
from .filesystem import S_IFDIR, S_IFLNK, S_IFMT, S_IFREG, Filesystem, Stats
from .kvstore import KvStore
from .toolcalls import ToolCall, ToolCalls, ToolCallStats

__version__ = "0.6.0"

__all__ = [
    "AgentFS",
    "AgentFSOptions",
    "KvStore",
    "Filesystem",
    "Stats",
    "S_IFDIR",
    "S_IFLNK",
    "S_IFMT",
    "S_IFREG",
    "ToolCalls",
    "ToolCall",
    "ToolCallStats",
    "ErrnoException",
    "FsErrorCode",
    "FsSyscall",
]




------------------------------------------
File: sdk/python/agentfs_sdk/agentfs.py
------------------------------------------

"""Main AgentFS class"""

import os
import re
from dataclasses import dataclass
from typing import Optional

from turso.aio import Connection, connect

from .filesystem import Filesystem
from .kvstore import KvStore
from .toolcalls import ToolCalls


@dataclass
class AgentFSOptions:
    """Configuration options for opening an AgentFS instance

    Attributes:
        id: Unique identifier for the agent.
            - If provided without `path`: Creates storage at `.agentfs/{id}.db`
            - If provided with `path`: Uses the specified path
        path: Explicit path to the database file.
            - If provided: Uses the specified path directly
            - Can be combined with `id`
    """

    id: Optional[str] = None
    path: Optional[str] = None


class AgentFS:
    """AgentFS - A filesystem and key-value store for AI agents

    Provides a unified interface for persistent storage using SQLite,
    with support for key-value storage, filesystem operations, and
    tool call tracking.
    """

    def __init__(self, db: Connection, kv: KvStore, fs: Filesystem, tools: ToolCalls):
        """Private constructor - use AgentFS.open() instead"""
        self._db = db
        self.kv = kv
        self.fs = fs
        self.tools = tools

    @staticmethod
    async def open(options: AgentFSOptions) -> "AgentFS":
        """Open an agent filesystem

        Args:
            options: Configuration options (id and/or path required)

        Returns:
            Fully initialized AgentFS instance

        Raises:
            ValueError: If neither id nor path is provided, or if id contains invalid characters

        Example:
            >>> # Using id (creates .agentfs/my-agent.db)
            >>> agent = await AgentFS.open(AgentFSOptions(id='my-agent'))
            >>>
            >>> # Using id with custom path
            >>> agent = await AgentFS.open(AgentFSOptions(id='my-agent', path='./data/mydb.db'))
            >>>
            >>> # Using path only
            >>> agent = await AgentFS.open(AgentFSOptions(path='./data/mydb.db'))
        """
        # Require at least id or path
        if not options.id and not options.path:
            raise ValueError("AgentFS.open() requires at least 'id' or 'path'.")

        # Validate agent ID if provided
        if options.id and not re.match(r"^[a-zA-Z0-9_-]+$", options.id):
            raise ValueError(
                "Agent ID must contain only alphanumeric characters, hyphens, and underscores"
            )

        # Determine database path: explicit path takes precedence, otherwise use id-based path
        if options.path:
            db_path = options.path
        else:
            # id is guaranteed to be defined here (we checked not id and not path above)
            directory = ".agentfs"
            if not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
            db_path = f"{directory}/{options.id}.db"

        # Connect to the database to ensure it's created
        db = await connect(db_path)

        return await AgentFS.open_with(db)

    @staticmethod
    async def open_with(db: Connection) -> "AgentFS":
        """Open an AgentFS instance with an existing database connection

        Args:
            db: An existing pyturso.aio Connection

        Returns:
            Fully initialized AgentFS instance
        """
        # Initialize all components in parallel
        kv = await KvStore.from_database(db)
        fs = await Filesystem.from_database(db)
        tools = await ToolCalls.from_database(db)

        return AgentFS(db, kv, fs, tools)

    def get_database(self) -> Connection:
        """Get the underlying Database connection"""
        return self._db

    async def close(self) -> None:
        """Close the database connection"""
        await self._db.close()

    async def __aenter__(self) -> "AgentFS":
        """Context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc, tb) -> None:
        """Context manager exit"""
        await self.close()




------------------------------------------
File: sdk/python/agentfs_sdk/constants.py
------------------------------------------

"""Filesystem constants"""

# File types for mode field
S_IFMT = 0o170000  # File type mask
S_IFREG = 0o100000  # Regular file
S_IFDIR = 0o040000  # Directory
S_IFLNK = 0o120000  # Symbolic link

# Default permissions
DEFAULT_FILE_MODE = S_IFREG | 0o644  # Regular file, rw-r--r--
DEFAULT_DIR_MODE = S_IFDIR | 0o755  # Directory, rwxr-xr-x

DEFAULT_CHUNK_SIZE = 4096




------------------------------------------
File: sdk/python/agentfs_sdk/errors.py
------------------------------------------

"""Error types for filesystem operations"""

from typing import Literal, Optional

# POSIX-style error codes for filesystem operations
FsErrorCode = Literal[
    "ENOENT",  # No such file or directory
    "EEXIST",  # File already exists
    "EISDIR",  # Is a directory (when file expected)
    "ENOTDIR",  # Not a directory (when directory expected)
    "ENOTEMPTY",  # Directory not empty
    "EPERM",  # Operation not permitted
    "EINVAL",  # Invalid argument
    "ENOSYS",  # Function not implemented (use for symlinks)
]

# Filesystem syscall names for error reporting
# rm, scandir and copyfile are not actual syscalls but used for convenience
FsSyscall = Literal[
    "open",
    "stat",
    "mkdir",
    "rmdir",
    "rm",
    "unlink",
    "rename",
    "scandir",
    "copyfile",
    "access",
]


class ErrnoException(Exception):
    """Exception with errno-style attributes

    Args:
        code: POSIX error code (e.g., 'ENOENT')
        syscall: System call name (e.g., 'open')
        path: Optional path involved in the error
        message: Optional custom message (defaults to code)

    Example:
        >>> raise ErrnoException('ENOENT', 'open', '/missing.txt')
        ErrnoException: ENOENT: no such file or directory, open '/missing.txt'
    """

    def __init__(
        self,
        code: FsErrorCode,
        syscall: FsSyscall,
        path: Optional[str] = None,
        message: Optional[str] = None,
    ):
        base = message if message else code
        suffix = f" '{path}'" if path is not None else ""
        error_message = f"{code}: {base}, {syscall}{suffix}"
        super().__init__(error_message)
        self.code = code
        self.syscall = syscall
        self.path = path




------------------------------------------
File: sdk/python/agentfs_sdk/filesystem.py
------------------------------------------

"""Filesystem implementation"""

import time
from dataclasses import dataclass
from typing import List, Optional, Union

from turso.aio import Connection

from .constants import (
    DEFAULT_CHUNK_SIZE,
    DEFAULT_DIR_MODE,
    DEFAULT_FILE_MODE,
    S_IFDIR,
    S_IFLNK,
    S_IFMT,
    S_IFREG,
)
from .errors import ErrnoException, FsSyscall
from .guards import (
    assert_inode_is_directory,
    assert_not_root,
    assert_not_symlink_mode,
    assert_readable_existing_inode,
    assert_readdir_target_inode,
    assert_unlink_target_inode,
    assert_writable_existing_inode,
    get_inode_mode_or_throw,
    normalize_rm_options,
    throw_enoent_unless_force,
)

# Re-export constants for backwards compatibility
__all__ = ["Filesystem", "Stats", "S_IFMT", "S_IFREG", "S_IFDIR", "S_IFLNK"]


@dataclass
class Stats:
    """File/directory statistics

    Attributes:
        ino: Inode number
        mode: File mode and permissions
        nlink: Number of hard links
        uid: User ID
        gid: Group ID
        size: File size in bytes
        atime: Access time (Unix timestamp)
        mtime: Modification time (Unix timestamp)
        ctime: Change time (Unix timestamp)
    """

    ino: int
    mode: int
    nlink: int
    uid: int
    gid: int
    size: int
    atime: int
    mtime: int
    ctime: int

    def is_file(self) -> bool:
        """Check if this is a regular file"""
        return (self.mode & S_IFMT) == S_IFREG

    def is_directory(self) -> bool:
        """Check if this is a directory"""
        return (self.mode & S_IFMT) == S_IFDIR

    def is_symbolic_link(self) -> bool:
        """Check if this is a symbolic link"""
        return (self.mode & S_IFMT) == S_IFLNK


class Filesystem:
    """Virtual filesystem backed by SQLite

    Provides a POSIX-like filesystem interface with support for
    files, directories, and symbolic links.
    """

    def __init__(self, db: Connection):
        """Private constructor - use Filesystem.from_database() instead"""
        self._db = db
        self._root_ino = 1
        self._chunk_size = DEFAULT_CHUNK_SIZE

    @staticmethod
    async def from_database(db: Connection) -> "Filesystem":
        """Create a Filesystem from an existing database connection

        Args:
            db: An existing pyturso.aio Connection

        Returns:
            Fully initialized Filesystem instance
        """
        fs = Filesystem(db)
        await fs._initialize()
        return fs

    def get_chunk_size(self) -> int:
        """Get the configured chunk size"""
        return self._chunk_size

    async def _initialize(self) -> None:
        """Initialize the database schema"""
        # Create all tables
        await self._db.executescript("""
            CREATE TABLE IF NOT EXISTS fs_config (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            );

            CREATE TABLE IF NOT EXISTS fs_inode (
                ino INTEGER PRIMARY KEY AUTOINCREMENT,
                mode INTEGER NOT NULL,
                nlink INTEGER NOT NULL DEFAULT 0,
                uid INTEGER NOT NULL DEFAULT 0,
                gid INTEGER NOT NULL DEFAULT 0,
                size INTEGER NOT NULL DEFAULT 0,
                atime INTEGER NOT NULL,
                mtime INTEGER NOT NULL,
                ctime INTEGER NOT NULL,
                rdev INTEGER NOT NULL DEFAULT 0,
                atime_nsec INTEGER NOT NULL DEFAULT 0,
                mtime_nsec INTEGER NOT NULL DEFAULT 0,
                ctime_nsec INTEGER NOT NULL DEFAULT 0
            );

            CREATE TABLE IF NOT EXISTS fs_dentry (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                parent_ino INTEGER NOT NULL,
                ino INTEGER NOT NULL,
                UNIQUE(parent_ino, name)
            );

            CREATE INDEX IF NOT EXISTS idx_fs_dentry_parent
            ON fs_dentry(parent_ino, name);

            CREATE TABLE IF NOT EXISTS fs_data (
                ino INTEGER NOT NULL,
                chunk_index INTEGER NOT NULL,
                data BLOB NOT NULL,
                PRIMARY KEY (ino, chunk_index)
            );

            CREATE TABLE IF NOT EXISTS fs_symlink (
                ino INTEGER PRIMARY KEY,
                target TEXT NOT NULL
            );
        """)
        await self._db.commit()

        # Initialize config and root directory
        self._chunk_size = await self._ensure_root()

    async def _ensure_root(self) -> int:
        """Ensure config and root directory exist, returns the chunk_size"""
        # Ensure chunk_size config exists and get its value
        cursor = await self._db.execute("SELECT value FROM fs_config WHERE key = 'chunk_size'")
        config = await cursor.fetchone()

        if not config:
            await self._db.execute(
                "INSERT INTO fs_config (key, value) VALUES ('chunk_size', ?)",
                (str(DEFAULT_CHUNK_SIZE),),
            )
            await self._db.commit()
            chunk_size = DEFAULT_CHUNK_SIZE
        else:
            chunk_size = int(config[0]) if config[0] else DEFAULT_CHUNK_SIZE

        # Set schema version (keep in sync with AGENTFS_SCHEMA_VERSION in sdk/rust/src/schema.rs)
        await self._db.execute(
            "INSERT OR REPLACE INTO fs_config (key, value) VALUES ('schema_version', '0.4')"
        )
        await self._db.commit()

        # Ensure root directory exists
        cursor = await self._db.execute("SELECT ino FROM fs_inode WHERE ino = ?", (self._root_ino,))
        root = await cursor.fetchone()

        if not root:
            now = int(time.time())
            await self._db.execute(
                """
                INSERT INTO fs_inode (ino, mode, nlink, uid, gid, size, atime, mtime, ctime)
                VALUES (?, ?, 1, 0, 0, 0, ?, ?, ?)
                """,
                (self._root_ino, DEFAULT_DIR_MODE, now, now, now),
            )
            await self._db.commit()

        return chunk_size

    def _normalize_path(self, path: str) -> str:
        """Normalize a path"""
        # Remove trailing slashes except for root
        normalized = path.rstrip("/") or "/"
        # Ensure leading slash
        return normalized if normalized.startswith("/") else "/" + normalized

    def _split_path(self, path: str) -> List[str]:
        """Split path into components"""
        normalized = self._normalize_path(path)
        if normalized == "/":
            return []
        return [p for p in normalized.split("/") if p]

    async def _resolve_path(self, path: str) -> Optional[int]:
        """Resolve a path to an inode number"""
        normalized = self._normalize_path(path)

        # Root directory
        if normalized == "/":
            return self._root_ino

        parts = self._split_path(normalized)
        current_ino = self._root_ino

        # Traverse the path
        for name in parts:
            cursor = await self._db.execute(
                """
                SELECT ino FROM fs_dentry
                WHERE parent_ino = ? AND name = ?
                """,
                (current_ino, name),
            )
            result = await cursor.fetchone()

            if not result:
                return None

            current_ino = result[0]

        return current_ino

    async def _resolve_parent(self, path: str) -> Optional[tuple]:
        """Get parent directory inode and basename from path"""
        normalized = self._normalize_path(path)

        if normalized == "/":
            return None  # Root has no parent

        parts = self._split_path(normalized)
        name = parts[-1]
        parent_path = "/" if len(parts) == 1 else "/" + "/".join(parts[:-1])

        parent_ino = await self._resolve_path(parent_path)

        if parent_ino is None:
            return None

        return (parent_ino, name)

    async def _create_inode(self, mode: int, uid: int = 0, gid: int = 0) -> int:
        """Create an inode

        Note: We use RETURNING clause which requires explicit cursor close
        when working with CDC-enabled TursoDB connections.
        """
        now = int(time.time())
        cursor = self._db.cursor()
        try:
            await cursor.execute(
                """
                INSERT INTO fs_inode (mode, uid, gid, size, atime, mtime, ctime)
                VALUES (?, ?, ?, 0, ?, ?, ?)
                RETURNING ino
                """,
                (mode, uid, gid, now, now, now),
            )
            row = await cursor.fetchone()
            assert row is not None
            ino = row[0]
        finally:
            await cursor.close()
        # Commit after cursor is closed
        await self._db.commit()
        return ino

    async def _create_dentry(self, parent_ino: int, name: str, ino: int) -> None:
        """Create a directory entry"""
        await self._db.execute(
            """
            INSERT INTO fs_dentry (name, parent_ino, ino)
            VALUES (?, ?, ?)
            """,
            (name, parent_ino, ino),
        )
        # Increment link count
        await self._db.execute(
            "UPDATE fs_inode SET nlink = nlink + 1 WHERE ino = ?",
            (ino,),
        )
        await self._db.commit()

    async def _ensure_parent_dirs(self, path: str) -> None:
        """Ensure parent directories exist"""
        parts = self._split_path(path)

        # Remove the filename, keep only directory parts
        parts = parts[:-1]

        current_ino = self._root_ino
        current_path = ""

        for name in parts:
            current_path += "/" + name

            # Check if this directory exists
            cursor = await self._db.execute(
                """
                SELECT ino FROM fs_dentry
                WHERE parent_ino = ? AND name = ?
                """,
                (current_ino, name),
            )
            result = await cursor.fetchone()

            if not result:
                # Create directory
                dir_ino = await self._create_inode(DEFAULT_DIR_MODE)
                await self._create_dentry(current_ino, name, dir_ino)
                current_ino = dir_ino
            else:
                current_ino = result[0]

    async def _get_link_count(self, ino: int) -> int:
        """Get link count for an inode"""
        cursor = await self._db.execute("SELECT nlink FROM fs_inode WHERE ino = ?", (ino,))
        result = await cursor.fetchone()
        return result[0] if result else 0

    async def _get_inode_mode(self, ino: int) -> Optional[int]:
        """Get mode for an inode"""
        cursor = await self._db.execute("SELECT mode FROM fs_inode WHERE ino = ?", (ino,))
        row = await cursor.fetchone()
        return row[0] if row else None

    async def _resolve_path_or_throw(self, path: str, syscall: FsSyscall) -> tuple[str, int]:
        """Resolve path to inode or throw ENOENT"""
        normalized_path = self._normalize_path(path)
        ino = await self._resolve_path(normalized_path)
        if ino is None:
            raise ErrnoException(
                code="ENOENT",
                syscall=syscall,
                path=normalized_path,
                message="no such file or directory",
            )
        return (normalized_path, ino)

    async def write_file(
        self,
        path: str,
        content: Union[str, bytes],
        encoding: str = "utf-8",
    ) -> None:
        """Write content to a file

        Args:
            path: Path to the file
            content: Content to write (string or bytes)
            encoding: Text encoding (default: 'utf-8')

        Example:
            >>> await fs.write_file('/data/config.json', '{"key": "value"}')
        """
        # Ensure parent directories exist
        await self._ensure_parent_dirs(path)

        normalized_path = self._normalize_path(path)
        # Check if file already exists
        ino = await self._resolve_path(normalized_path)

        if ino is not None:
            # Validate existing inode
            await assert_writable_existing_inode(self._db, ino, "open", normalized_path)
            # Update existing file
            await self._update_file_content(ino, content, encoding)
        else:
            # Create new file
            parent = await self._resolve_parent(normalized_path)
            if not parent:
                raise ErrnoException(
                    code="ENOENT",
                    syscall="open",
                    path=normalized_path,
                    message="no such file or directory",
                )

            parent_ino, name = parent

            # Ensure parent is a directory
            await assert_inode_is_directory(self._db, parent_ino, "open", normalized_path)

            # Create inode
            file_ino = await self._create_inode(DEFAULT_FILE_MODE)

            # Create directory entry
            await self._create_dentry(parent_ino, name, file_ino)

            # Write content
            await self._update_file_content(file_ino, content, encoding)

    async def _update_file_content(
        self, ino: int, content: Union[str, bytes], encoding: str = "utf-8"
    ) -> None:
        """Update file content"""
        buffer = content.encode(encoding) if isinstance(content, str) else content
        now = int(time.time())

        # Delete existing data chunks
        await self._db.execute("DELETE FROM fs_data WHERE ino = ?", (ino,))

        # Write data in chunks
        if len(buffer) > 0:
            chunk_index = 0
            for offset in range(0, len(buffer), self._chunk_size):
                chunk = buffer[offset : min(offset + self._chunk_size, len(buffer))]
                await self._db.execute(
                    """
                    INSERT INTO fs_data (ino, chunk_index, data)
                    VALUES (?, ?, ?)
                    """,
                    (ino, chunk_index, chunk),
                )
                chunk_index += 1

        # Update inode size and mtime
        await self._db.execute(
            """
            UPDATE fs_inode
            SET size = ?, mtime = ?
            WHERE ino = ?
            """,
            (len(buffer), now, ino),
        )
        await self._db.commit()

    async def read_file(self, path: str, encoding: Optional[str] = "utf-8") -> Union[bytes, str]:
        """Read content from a file

        Args:
            path: Path to the file
            encoding: Text encoding (default: 'utf-8'). Set to None to return bytes.

        Returns:
            File content as string (if encoding specified) or bytes

        Example:
            >>> content = await fs.read_file('/data/config.json')
            >>> data = await fs.read_file('/data/image.png', encoding=None)
        """
        normalized_path, ino = await self._resolve_path_or_throw(path, "open")

        await assert_readable_existing_inode(self._db, ino, "open", normalized_path)

        # Get all data chunks
        cursor = await self._db.execute(
            """
            SELECT data FROM fs_data
            WHERE ino = ?
            ORDER BY chunk_index ASC
            """,
            (ino,),
        )
        rows = await cursor.fetchall()

        if not rows:
            combined = b""
        else:
            # Concatenate all chunks
            combined = b"".join(row[0] for row in rows)

        # Update atime
        now = int(time.time())
        await self._db.execute("UPDATE fs_inode SET atime = ? WHERE ino = ?", (now, ino))
        await self._db.commit()

        if encoding:
            return combined.decode(encoding)
        return combined

    async def readdir(self, path: str) -> List[str]:
        """List directory contents

        Args:
            path: Path to the directory

        Returns:
            List of entry names

        Example:
            >>> entries = await fs.readdir('/data')
            >>> for entry in entries:
            >>>     print(entry)
        """
        normalized_path, ino = await self._resolve_path_or_throw(path, "scandir")

        await assert_readdir_target_inode(self._db, ino, normalized_path)

        # Get all directory entries
        cursor = await self._db.execute(
            """
            SELECT name FROM fs_dentry
            WHERE parent_ino = ?
            ORDER BY name ASC
            """,
            (ino,),
        )
        rows = await cursor.fetchall()

        return [row[0] for row in rows]

    async def unlink(self, path: str) -> None:
        """Delete a file (unlink)

        Args:
            path: Path to the file

        Example:
            >>> await fs.unlink('/data/temp.txt')
        """
        normalized_path = self._normalize_path(path)
        assert_not_root(normalized_path, "unlink")
        normalized_path, ino = await self._resolve_path_or_throw(normalized_path, "unlink")

        await assert_unlink_target_inode(self._db, ino, normalized_path)

        parent = await self._resolve_parent(normalized_path)
        # parent is guaranteed to exist here since normalized_path != '/'
        assert parent is not None
        parent_ino, name = parent

        # Delete the directory entry
        await self._db.execute(
            """
            DELETE FROM fs_dentry
            WHERE parent_ino = ? AND name = ?
            """,
            (parent_ino, name),
        )

        # Decrement link count
        await self._db.execute(
            "UPDATE fs_inode SET nlink = nlink - 1 WHERE ino = ?",
            (ino,),
        )

        # Check if this was the last link to the inode
        link_count = await self._get_link_count(ino)
        if link_count == 0:
            # Delete the inode
            await self._db.execute("DELETE FROM fs_inode WHERE ino = ?", (ino,))

            # Delete all data chunks
            await self._db.execute("DELETE FROM fs_data WHERE ino = ?", (ino,))

        await self._db.commit()

    # Backwards-compatible alias
    async def delete_file(self, path: str) -> None:
        """Delete a file (deprecated, use unlink instead)

        Args:
            path: Path to the file

        Example:
            >>> await fs.delete_file('/data/temp.txt')
        """
        return await self.unlink(path)

    async def stat(self, path: str) -> Stats:
        """Get file/directory statistics

        Args:
            path: Path to the file or directory

        Returns:
            Stats object with file information

        Example:
            >>> stats = await fs.stat('/data/config.json')
            >>> print(f"Size: {stats.size} bytes")
            >>> print(f"Is file: {stats.is_file()}")
        """
        normalized_path, ino = await self._resolve_path_or_throw(path, "stat")

        cursor = await self._db.execute(
            """
            SELECT ino, mode, nlink, uid, gid, size, atime, mtime, ctime
            FROM fs_inode
            WHERE ino = ?
            """,
            (ino,),
        )
        row = await cursor.fetchone()

        if not row:
            raise ErrnoException(
                code="ENOENT",
                syscall="stat",
                path=normalized_path,
                message="no such file or directory",
            )

        return Stats(
            ino=row[0],
            mode=row[1],
            nlink=row[2],
            uid=row[3],
            gid=row[4],
            size=row[5],
            atime=row[6],
            mtime=row[7],
            ctime=row[8],
        )

    async def mkdir(self, path: str) -> None:
        """Create a directory (non-recursive)

        Args:
            path: Path to the directory to create

        Example:
            >>> await fs.mkdir('/data/new_dir')
        """
        normalized_path = self._normalize_path(path)

        existing = await self._resolve_path(normalized_path)
        if existing is not None:
            raise ErrnoException(
                code="EEXIST",
                syscall="mkdir",
                path=normalized_path,
                message="file already exists",
            )

        parent = await self._resolve_parent(normalized_path)
        if not parent:
            raise ErrnoException(
                code="ENOENT",
                syscall="mkdir",
                path=normalized_path,
                message="no such file or directory",
            )

        parent_ino, name = parent
        await assert_inode_is_directory(self._db, parent_ino, "mkdir", normalized_path)

        dir_ino = await self._create_inode(DEFAULT_DIR_MODE)
        try:
            await self._create_dentry(parent_ino, name, dir_ino)
        except Exception:
            raise ErrnoException(
                code="EEXIST",
                syscall="mkdir",
                path=normalized_path,
                message="file already exists",
            )

    async def rmdir(self, path: str) -> None:
        """Remove an empty directory

        Args:
            path: Path to the directory to remove

        Example:
            >>> await fs.rmdir('/data/empty_dir')
        """
        normalized_path = self._normalize_path(path)
        assert_not_root(normalized_path, "rmdir")

        normalized_path, ino = await self._resolve_path_or_throw(normalized_path, "rmdir")

        mode = await get_inode_mode_or_throw(self._db, ino, "rmdir", normalized_path)
        assert_not_symlink_mode(mode, "rmdir", normalized_path)
        if (mode & S_IFMT) != S_IFDIR:
            raise ErrnoException(
                code="ENOTDIR",
                syscall="rmdir",
                path=normalized_path,
                message="not a directory",
            )

        cursor = await self._db.execute(
            """
            SELECT 1 as one FROM fs_dentry
            WHERE parent_ino = ?
            LIMIT 1
            """,
            (ino,),
        )
        child = await cursor.fetchone()
        if child:
            raise ErrnoException(
                code="ENOTEMPTY",
                syscall="rmdir",
                path=normalized_path,
                message="directory not empty",
            )

        parent = await self._resolve_parent(normalized_path)
        if not parent:
            raise ErrnoException(
                code="EPERM",
                syscall="rmdir",
                path=normalized_path,
                message="operation not permitted",
            )

        parent_ino, name = parent
        await self._remove_dentry_and_maybe_inode(parent_ino, name, ino)

    async def rm(
        self,
        path: str,
        force: bool = False,
        recursive: bool = False,
    ) -> None:
        """Remove a file or directory

        Args:
            path: Path to remove
            force: If True, ignore nonexistent files
            recursive: If True, remove directories and their contents recursively

        Example:
            >>> await fs.rm('/data/file.txt')
            >>> await fs.rm('/data/dir', recursive=True)
        """
        normalized_path = self._normalize_path(path)
        options = normalize_rm_options({"force": force, "recursive": recursive})
        force = options["force"]
        recursive = options["recursive"]
        assert_not_root(normalized_path, "rm")

        ino = await self._resolve_path(normalized_path)
        if ino is None:
            throw_enoent_unless_force(normalized_path, "rm", force)
            return

        mode = await get_inode_mode_or_throw(self._db, ino, "rm", normalized_path)
        assert_not_symlink_mode(mode, "rm", normalized_path)

        parent = await self._resolve_parent(normalized_path)
        if not parent:
            raise ErrnoException(
                code="EPERM",
                syscall="rm",
                path=normalized_path,
                message="operation not permitted",
            )

        parent_ino, name = parent

        if (mode & S_IFMT) == S_IFDIR:
            if not recursive:
                raise ErrnoException(
                    code="EISDIR",
                    syscall="rm",
                    path=normalized_path,
                    message="illegal operation on a directory",
                )

            await self._rm_dir_contents_recursive(ino)
            await self._remove_dentry_and_maybe_inode(parent_ino, name, ino)
            return

        # Regular file
        await self._remove_dentry_and_maybe_inode(parent_ino, name, ino)

    async def _rm_dir_contents_recursive(self, dir_ino: int) -> None:
        """Recursively remove directory contents"""
        cursor = await self._db.execute(
            """
            SELECT name, ino FROM fs_dentry
            WHERE parent_ino = ?
            ORDER BY name ASC
            """,
            (dir_ino,),
        )
        children = await cursor.fetchall()

        for name, child_ino in children:
            mode = await self._get_inode_mode(child_ino)
            if mode is None:
                # DB inconsistency; treat as already gone
                continue

            if (mode & S_IFMT) == S_IFDIR:
                await self._rm_dir_contents_recursive(child_ino)
                await self._remove_dentry_and_maybe_inode(dir_ino, name, child_ino)
            else:
                # Not supported yet (symlinks)
                assert_not_symlink_mode(mode, "rm", "<symlink>")
                await self._remove_dentry_and_maybe_inode(dir_ino, name, child_ino)

    async def _remove_dentry_and_maybe_inode(self, parent_ino: int, name: str, ino: int) -> None:
        """Remove directory entry and inode if last link"""
        await self._db.execute(
            """
            DELETE FROM fs_dentry
            WHERE parent_ino = ? AND name = ?
            """,
            (parent_ino, name),
        )

        # Decrement link count
        await self._db.execute(
            "UPDATE fs_inode SET nlink = nlink - 1 WHERE ino = ?",
            (ino,),
        )

        link_count = await self._get_link_count(ino)
        if link_count == 0:
            await self._db.execute("DELETE FROM fs_inode WHERE ino = ?", (ino,))
            await self._db.execute("DELETE FROM fs_data WHERE ino = ?", (ino,))

        await self._db.commit()

    async def rename(self, old_path: str, new_path: str) -> None:
        """Rename (move) a file or directory

        Args:
            old_path: Current path
            new_path: New path

        Example:
            >>> await fs.rename('/data/old.txt', '/data/new.txt')
        """
        old_normalized = self._normalize_path(old_path)
        new_normalized = self._normalize_path(new_path)

        # No-op
        if old_normalized == new_normalized:
            return

        assert_not_root(old_normalized, "rename")
        assert_not_root(new_normalized, "rename")

        old_parent = await self._resolve_parent(old_normalized)
        if not old_parent:
            raise ErrnoException(
                code="EPERM",
                syscall="rename",
                path=old_normalized,
                message="operation not permitted",
            )

        new_parent = await self._resolve_parent(new_normalized)
        if not new_parent:
            raise ErrnoException(
                code="ENOENT",
                syscall="rename",
                path=new_normalized,
                message="no such file or directory",
            )

        new_parent_ino, new_name = new_parent

        # Ensure destination parent exists and is a directory
        await assert_inode_is_directory(self._db, new_parent_ino, "rename", new_normalized)

        # Begin transaction
        # Note: turso.aio doesn't support explicit BEGIN, but execute should be atomic
        try:
            old_normalized, old_ino = await self._resolve_path_or_throw(old_normalized, "rename")
            old_mode = await get_inode_mode_or_throw(self._db, old_ino, "rename", old_normalized)
            assert_not_symlink_mode(old_mode, "rename", old_normalized)
            old_is_dir = (old_mode & S_IFMT) == S_IFDIR

            # Prevent renaming a directory into its own subtree (would create cycles)
            if old_is_dir and new_normalized.startswith(old_normalized + "/"):
                raise ErrnoException(
                    code="EINVAL",
                    syscall="rename",
                    path=new_normalized,
                    message="invalid argument",
                )

            new_ino = await self._resolve_path(new_normalized)
            if new_ino is not None:
                new_mode = await get_inode_mode_or_throw(
                    self._db, new_ino, "rename", new_normalized
                )
                assert_not_symlink_mode(new_mode, "rename", new_normalized)
                new_is_dir = (new_mode & S_IFMT) == S_IFDIR

                if new_is_dir and not old_is_dir:
                    raise ErrnoException(
                        code="EISDIR",
                        syscall="rename",
                        path=new_normalized,
                        message="illegal operation on a directory",
                    )
                if not new_is_dir and old_is_dir:
                    raise ErrnoException(
                        code="ENOTDIR",
                        syscall="rename",
                        path=new_normalized,
                        message="not a directory",
                    )

                # If replacing a directory, it must be empty
                if new_is_dir:
                    cursor = await self._db.execute(
                        """
                        SELECT 1 as one FROM fs_dentry
                        WHERE parent_ino = ?
                        LIMIT 1
                        """,
                        (new_ino,),
                    )
                    child = await cursor.fetchone()
                    if child:
                        raise ErrnoException(
                            code="ENOTEMPTY",
                            syscall="rename",
                            path=new_normalized,
                            message="directory not empty",
                        )

                # Remove the destination entry (and inode if this was the last link)
                await self._remove_dentry_and_maybe_inode(new_parent_ino, new_name, new_ino)

            # Move the directory entry
            old_parent_ino, old_name = old_parent
            await self._db.execute(
                """
                UPDATE fs_dentry
                SET parent_ino = ?, name = ?
                WHERE parent_ino = ? AND name = ?
                """,
                (new_parent_ino, new_name, old_parent_ino, old_name),
            )

            # Update timestamps
            now = int(time.time())
            await self._db.execute(
                """
                UPDATE fs_inode
                SET ctime = ?
                WHERE ino = ?
                """,
                (now, old_ino),
            )

            await self._db.execute(
                """
                UPDATE fs_inode
                SET mtime = ?, ctime = ?
                WHERE ino = ?
                """,
                (now, now, old_parent_ino),
            )
            if new_parent_ino != old_parent_ino:
                await self._db.execute(
                    """
                    UPDATE fs_inode
                    SET mtime = ?, ctime = ?
                    WHERE ino = ?
                    """,
                    (now, now, new_parent_ino),
                )

            await self._db.commit()
        except Exception:
            # turso.aio doesn't have explicit rollback, changes are rolled back automatically
            raise

    async def copy_file(self, src: str, dest: str) -> None:
        """Copy a file. Overwrites destination if it exists.

        Args:
            src: Source file path
            dest: Destination file path

        Example:
            >>> await fs.copy_file('/data/src.txt', '/data/dest.txt')
        """
        src_normalized = self._normalize_path(src)
        dest_normalized = self._normalize_path(dest)

        if src_normalized == dest_normalized:
            raise ErrnoException(
                code="EINVAL",
                syscall="copyfile",
                path=dest_normalized,
                message="invalid argument",
            )

        # Resolve and validate source
        src_normalized, src_ino = await self._resolve_path_or_throw(src_normalized, "copyfile")
        await assert_readable_existing_inode(self._db, src_ino, "copyfile", src_normalized)

        cursor = await self._db.execute(
            """
            SELECT mode, uid, gid, size FROM fs_inode WHERE ino = ?
            """,
            (src_ino,),
        )
        src_row = await cursor.fetchone()
        if not src_row:
            raise ErrnoException(
                code="ENOENT",
                syscall="copyfile",
                path=src_normalized,
                message="no such file or directory",
            )

        src_mode, src_uid, src_gid, src_size = src_row

        # Destination parent must exist and be a directory
        dest_parent = await self._resolve_parent(dest_normalized)
        if not dest_parent:
            raise ErrnoException(
                code="ENOENT",
                syscall="copyfile",
                path=dest_normalized,
                message="no such file or directory",
            )

        dest_parent_ino, dest_name = dest_parent
        await assert_inode_is_directory(self._db, dest_parent_ino, "copyfile", dest_normalized)

        try:
            now = int(time.time())

            # If destination exists, it must be a file (overwrite semantics)
            dest_ino = await self._resolve_path(dest_normalized)
            if dest_ino is not None:
                dest_mode = await get_inode_mode_or_throw(
                    self._db, dest_ino, "copyfile", dest_normalized
                )
                assert_not_symlink_mode(dest_mode, "copyfile", dest_normalized)
                if (dest_mode & S_IFMT) == S_IFDIR:
                    raise ErrnoException(
                        code="EISDIR",
                        syscall="copyfile",
                        path=dest_normalized,
                        message="illegal operation on a directory",
                    )

                # Replace destination contents
                await self._db.execute("DELETE FROM fs_data WHERE ino = ?", (dest_ino,))
                await self._db.commit()

                # Copy data chunks
                cursor = await self._db.execute(
                    """
                    SELECT chunk_index, data FROM fs_data
                    WHERE ino = ?
                    ORDER BY chunk_index ASC
                    """,
                    (src_ino,),
                )
                src_chunks = await cursor.fetchall()
                for chunk_index, data in src_chunks:
                    await self._db.execute(
                        """
                        INSERT INTO fs_data (ino, chunk_index, data)
                        VALUES (?, ?, ?)
                        """,
                        (dest_ino, chunk_index, data),
                    )

                await self._db.execute(
                    """
                    UPDATE fs_inode
                    SET mode = ?, uid = ?, gid = ?, size = ?, mtime = ?, ctime = ?
                    WHERE ino = ?
                    """,
                    (src_mode, src_uid, src_gid, src_size, now, now, dest_ino),
                )
            else:
                # Create new destination inode + dentry
                dest_ino_created = await self._create_inode(src_mode, src_uid, src_gid)
                await self._create_dentry(dest_parent_ino, dest_name, dest_ino_created)

                # Copy data chunks
                cursor = await self._db.execute(
                    """
                    SELECT chunk_index, data FROM fs_data
                    WHERE ino = ?
                    ORDER BY chunk_index ASC
                    """,
                    (src_ino,),
                )
                src_chunks = await cursor.fetchall()
                for chunk_index, data in src_chunks:
                    await self._db.execute(
                        """
                        INSERT INTO fs_data (ino, chunk_index, data)
                        VALUES (?, ?, ?)
                        """,
                        (dest_ino_created, chunk_index, data),
                    )

                await self._db.execute(
                    """
                    UPDATE fs_inode
                    SET size = ?, mtime = ?, ctime = ?
                    WHERE ino = ?
                    """,
                    (src_size, now, now, dest_ino_created),
                )

            await self._db.commit()
        except Exception:
            raise

    async def access(self, path: str) -> None:
        """Test a user's permissions for the file or directory.
        Currently supports existence checks only (F_OK semantics).

        Args:
            path: Path to check

        Example:
            >>> await fs.access('/data/config.json')
        """
        normalized_path = self._normalize_path(path)
        ino = await self._resolve_path(normalized_path)
        if ino is None:
            raise ErrnoException(
                code="ENOENT",
                syscall="access",
                path=normalized_path,
                message="no such file or directory",
            )




------------------------------------------
File: sdk/python/agentfs_sdk/guards.py
------------------------------------------

"""Guard functions for filesystem operations validation"""

from typing import Any, Dict, Optional

from turso.aio import Connection

from .constants import S_IFDIR, S_IFLNK, S_IFMT
from .errors import ErrnoException, FsSyscall


async def _get_inode_mode(db: Connection, ino: int) -> Optional[int]:
    """Get mode for an inode"""
    cursor = await db.execute("SELECT mode FROM fs_inode WHERE ino = ?", (ino,))
    row = await cursor.fetchone()
    return row[0] if row else None


def _is_dir_mode(mode: int) -> bool:
    """Check if mode represents a directory"""
    return (mode & S_IFMT) == S_IFDIR


async def get_inode_mode_or_throw(
    db: Connection,
    ino: int,
    syscall: FsSyscall,
    path: str,
) -> int:
    """Get inode mode or throw ENOENT if not found"""
    mode = await _get_inode_mode(db, ino)
    if mode is None:
        raise ErrnoException(
            code="ENOENT",
            syscall=syscall,
            path=path,
            message="no such file or directory",
        )
    return mode


def assert_not_root(path: str, syscall: FsSyscall) -> None:
    """Assert that path is not root directory"""
    if path == "/":
        raise ErrnoException(
            code="EPERM",
            syscall=syscall,
            path=path,
            message="operation not permitted on root directory",
        )


def normalize_rm_options(options: Optional[Dict[str, Any]]) -> Dict[str, bool]:
    """Normalize rm options to ensure force and recursive are booleans"""
    return {
        "force": options.get("force", False) if options else False,
        "recursive": options.get("recursive", False) if options else False,
    }


def throw_enoent_unless_force(path: str, syscall: FsSyscall, force: bool) -> None:
    """Throw ENOENT unless force flag is set"""
    if force:
        return
    raise ErrnoException(
        code="ENOENT",
        syscall=syscall,
        path=path,
        message="no such file or directory",
    )


def assert_not_symlink_mode(mode: int, syscall: FsSyscall, path: str) -> None:
    """Assert that mode does not represent a symlink"""
    if (mode & S_IFMT) == S_IFLNK:
        raise ErrnoException(
            code="ENOSYS",
            syscall=syscall,
            path=path,
            message="symbolic links not supported yet",
        )


async def _assert_existing_non_dir_non_symlink_inode(
    db: Connection,
    ino: int,
    syscall: FsSyscall,
    full_path_for_error: str,
) -> None:
    """Assert inode exists and is neither directory nor symlink"""
    mode = await _get_inode_mode(db, ino)
    if mode is None:
        raise ErrnoException(
            code="ENOENT",
            syscall=syscall,
            path=full_path_for_error,
            message="no such file or directory",
        )
    if _is_dir_mode(mode):
        raise ErrnoException(
            code="EISDIR",
            syscall=syscall,
            path=full_path_for_error,
            message="illegal operation on a directory",
        )
    assert_not_symlink_mode(mode, syscall, full_path_for_error)


async def assert_inode_is_directory(
    db: Connection,
    ino: int,
    syscall: FsSyscall,
    full_path_for_error: str,
) -> None:
    """Assert that inode is a directory"""
    mode = await _get_inode_mode(db, ino)
    if mode is None:
        raise ErrnoException(
            code="ENOENT",
            syscall=syscall,
            path=full_path_for_error,
            message="no such file or directory",
        )
    if not _is_dir_mode(mode):
        raise ErrnoException(
            code="ENOTDIR",
            syscall=syscall,
            path=full_path_for_error,
            message="not a directory",
        )


async def assert_writable_existing_inode(
    db: Connection,
    ino: int,
    syscall: FsSyscall,
    full_path_for_error: str,
) -> None:
    """Assert inode is writable (exists and is not directory/symlink)"""
    await _assert_existing_non_dir_non_symlink_inode(db, ino, syscall, full_path_for_error)


async def assert_readable_existing_inode(
    db: Connection,
    ino: int,
    syscall: FsSyscall,
    full_path_for_error: str,
) -> None:
    """Assert inode is readable (exists and is not directory/symlink)"""
    await _assert_existing_non_dir_non_symlink_inode(db, ino, syscall, full_path_for_error)


async def assert_readdir_target_inode(
    db: Connection,
    ino: int,
    full_path_for_error: str,
) -> None:
    """Assert inode is a valid readdir target (directory, not symlink)"""
    syscall: FsSyscall = "scandir"
    mode = await _get_inode_mode(db, ino)
    if mode is None:
        raise ErrnoException(
            code="ENOENT",
            syscall=syscall,
            path=full_path_for_error,
            message="no such file or directory",
        )
    assert_not_symlink_mode(mode, syscall, full_path_for_error)
    if not _is_dir_mode(mode):
        raise ErrnoException(
            code="ENOTDIR",
            syscall=syscall,
            path=full_path_for_error,
            message="not a directory",
        )


async def assert_unlink_target_inode(
    db: Connection,
    ino: int,
    full_path_for_error: str,
) -> None:
    """Assert inode is a valid unlink target (file, not directory/symlink)"""
    syscall: FsSyscall = "unlink"
    mode = await _get_inode_mode(db, ino)
    if mode is None:
        raise ErrnoException(
            code="ENOENT",
            syscall=syscall,
            path=full_path_for_error,
            message="no such file or directory",
        )
    if _is_dir_mode(mode):
        raise ErrnoException(
            code="EISDIR",
            syscall=syscall,
            path=full_path_for_error,
            message="illegal operation on a directory",
        )
    assert_not_symlink_mode(mode, syscall, full_path_for_error)




------------------------------------------
File: sdk/python/agentfs_sdk/kvstore.py
------------------------------------------

"""Key-Value Store implementation"""

import json
from typing import Any, Dict, List, Optional, TypeVar

from turso.aio import Connection

T = TypeVar("T")


class KvStore:
    """Key-Value store backed by SQLite

    Provides a simple key-value interface with JSON serialization
    for storing arbitrary Python objects.
    """

    def __init__(self, db: Connection):
        """Private constructor - use KvStore.from_database() instead"""
        self._db = db

    @staticmethod
    async def from_database(db: Connection) -> "KvStore":
        """Create a KvStore from an existing database connection

        Args:
            db: An existing pyturso.aio Connection

        Returns:
            Fully initialized KvStore instance
        """
        kv = KvStore(db)
        await kv._initialize()
        return kv

    async def _initialize(self) -> None:
        """Initialize the database schema"""
        # Create the key-value store table if it doesn't exist
        await self._db.executescript("""
            CREATE TABLE IF NOT EXISTS kv_store (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL,
                created_at INTEGER DEFAULT (unixepoch()),
                updated_at INTEGER DEFAULT (unixepoch())
            );

            CREATE INDEX IF NOT EXISTS idx_kv_store_created_at
            ON kv_store(created_at);
        """)
        await self._db.commit()

    async def set(self, key: str, value: Any) -> None:
        """Set a key-value pair

        Args:
            key: The key to store
            value: The value to store (will be JSON serialized)

        Example:
            >>> await kv.set('user:123', {'name': 'Alice', 'age': 30})
        """
        # Serialize the value to JSON
        serialized_value = json.dumps(value)

        # Use prepared statement to insert or update
        await self._db.execute(
            """
            INSERT INTO kv_store (key, value, updated_at)
            VALUES (?, ?, unixepoch())
            ON CONFLICT(key) DO UPDATE SET
                value = excluded.value,
                updated_at = unixepoch()
            """,
            (key, serialized_value),
        )
        await self._db.commit()

    async def get(self, key: str, default: Optional[T] = None) -> Optional[T]:
        """Get a value by key

        Args:
            key: The key to retrieve
            default: Default value if key is not found

        Returns:
            The deserialized value, or default if key doesn't exist

        Example:
            >>> user = await kv.get('user:123')
            >>> if user:
            >>>     print(user['name'])
        """
        cursor = await self._db.execute("SELECT value FROM kv_store WHERE key = ?", (key,))
        row = await cursor.fetchone()

        if not row:
            return default

        # Deserialize the JSON value
        return json.loads(row[0])

    async def list(self, prefix: str) -> List[Dict[str, Any]]:
        """List all keys matching a prefix

        Args:
            prefix: The prefix to match

        Returns:
            List of dictionaries with 'key' and 'value' fields

        Example:
            >>> users = await kv.list('user:')
            >>> for item in users:
            >>>     print(f"{item['key']}: {item['value']}")
        """
        # Escape special characters for LIKE query
        escaped = prefix.replace("\\", "\\\\").replace("%", "\\%").replace("_", "\\_")

        cursor = await self._db.execute(
            "SELECT key, value FROM kv_store WHERE key LIKE ? ESCAPE '\\'", (escaped + "%",)
        )
        rows = await cursor.fetchall()

        return [{"key": row[0], "value": json.loads(row[1])} for row in rows]

    async def delete(self, key: str) -> None:
        """Delete a key-value pair

        Args:
            key: The key to delete

        Example:
            >>> await kv.delete('user:123')
        """
        await self._db.execute("DELETE FROM kv_store WHERE key = ?", (key,))
        await self._db.commit()




------------------------------------------
File: sdk/python/agentfs_sdk/toolcalls.py
------------------------------------------

"""Tool calls tracking implementation"""

import json
import time
from dataclasses import dataclass
from typing import Any, List, Literal, Optional

from turso.aio import Connection


@dataclass
class ToolCall:
    """Tool call record

    Attributes:
        id: Unique identifier for the tool call
        name: Name of the tool
        parameters: Tool parameters (optional)
        result: Tool result (optional)
        error: Error message if the call failed (optional)
        status: Status of the tool call
        started_at: Unix timestamp when the call started
        completed_at: Unix timestamp when the call completed (optional)
        duration_ms: Duration in milliseconds (optional)
    """

    id: int
    name: str
    parameters: Optional[Any] = None
    result: Optional[Any] = None
    error: Optional[str] = None
    status: Literal["pending", "success", "error"] = "pending"
    started_at: int = 0
    completed_at: Optional[int] = None
    duration_ms: Optional[int] = None


@dataclass
class ToolCallStats:
    """Tool call statistics

    Attributes:
        name: Name of the tool
        total_calls: Total number of calls
        successful: Number of successful calls
        failed: Number of failed calls
        avg_duration_ms: Average duration in milliseconds
    """

    name: str
    total_calls: int
    successful: int
    failed: int
    avg_duration_ms: float


class ToolCalls:
    """Tool calls tracking backed by SQLite

    Provides tracking and analytics for tool/function calls,
    recording timing, parameters, results, and errors.
    """

    def __init__(self, db: Connection):
        """Private constructor - use ToolCalls.from_database() instead"""
        self._db = db

    @staticmethod
    async def from_database(db: Connection) -> "ToolCalls":
        """Create a ToolCalls from an existing database connection

        Args:
            db: An existing pyturso.aio Connection

        Returns:
            Fully initialized ToolCalls instance
        """
        tools = ToolCalls(db)
        await tools._initialize()
        return tools

    async def _initialize(self) -> None:
        """Initialize the database schema"""
        await self._db.executescript("""
            CREATE TABLE IF NOT EXISTS tool_calls (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                parameters TEXT,
                result TEXT,
                error TEXT,
                status TEXT NOT NULL DEFAULT 'pending',
                started_at INTEGER NOT NULL,
                completed_at INTEGER,
                duration_ms INTEGER
            );

            CREATE INDEX IF NOT EXISTS idx_tool_calls_name
            ON tool_calls(name);

            CREATE INDEX IF NOT EXISTS idx_tool_calls_started_at
            ON tool_calls(started_at);
        """)
        await self._db.commit()

    async def start(self, name: str, parameters: Optional[Any] = None) -> int:
        """Start a new tool call and mark it as pending

        Args:
            name: Name of the tool
            parameters: Tool parameters (will be JSON serialized)

        Returns:
            ID of the created tool call record

        Example:
            >>> call_id = await tools.start('search', {'query': 'Python'})

        Note: We use RETURNING clause which requires explicit cursor close
        when working with CDC-enabled TursoDB connections.
        """
        serialized_params = json.dumps(parameters) if parameters is not None else None
        started_at = int(time.time())

        cursor = self._db.cursor()
        try:
            await cursor.execute(
                """
                INSERT INTO tool_calls (name, parameters, status, started_at)
                VALUES (?, ?, 'pending', ?)
                RETURNING id
                """,
                (name, serialized_params, started_at),
            )
            row = await cursor.fetchone()
            assert row is not None
            call_id = row[0]
        finally:
            await cursor.close()
        await self._db.commit()
        return call_id

    async def success(self, call_id: int, result: Optional[Any] = None) -> None:
        """Mark a tool call as successful

        Args:
            call_id: ID of the tool call
            result: Tool result (will be JSON serialized)

        Example:
            >>> await tools.success(call_id, {'results': [...]})
        """
        serialized_result = json.dumps(result) if result is not None else None
        completed_at = int(time.time())

        # Get the started_at time to calculate duration
        cursor = await self._db.execute(
            "SELECT started_at FROM tool_calls WHERE id = ?", (call_id,)
        )
        row = await cursor.fetchone()

        if not row:
            raise ValueError(f"Tool call with ID {call_id} not found")

        duration_ms = (completed_at - row[0]) * 1000

        await self._db.execute(
            """
            UPDATE tool_calls
            SET status = 'success', result = ?, completed_at = ?, duration_ms = ?
            WHERE id = ?
            """,
            (serialized_result, completed_at, duration_ms, call_id),
        )
        await self._db.commit()

    async def error(self, call_id: int, error: str) -> None:
        """Mark a tool call as failed

        Args:
            call_id: ID of the tool call
            error: Error message

        Example:
            >>> await tools.error(call_id, 'Connection timeout')
        """
        completed_at = int(time.time())

        # Get the started_at time to calculate duration
        cursor = await self._db.execute(
            "SELECT started_at FROM tool_calls WHERE id = ?", (call_id,)
        )
        row = await cursor.fetchone()

        if not row:
            raise ValueError(f"Tool call with ID {call_id} not found")

        duration_ms = (completed_at - row[0]) * 1000

        await self._db.execute(
            """
            UPDATE tool_calls
            SET status = 'error', error = ?, completed_at = ?, duration_ms = ?
            WHERE id = ?
            """,
            (error, completed_at, duration_ms, call_id),
        )
        await self._db.commit()

    async def record(
        self,
        name: str,
        started_at: int,
        completed_at: int,
        parameters: Optional[Any] = None,
        result: Optional[Any] = None,
        error: Optional[str] = None,
    ) -> int:
        """Record a completed tool call

        Either result or error should be provided, not both.

        Args:
            name: Name of the tool
            started_at: Unix timestamp when the call started
            completed_at: Unix timestamp when the call completed
            parameters: Tool parameters (will be JSON serialized)
            result: Tool result (will be JSON serialized)
            error: Error message if the call failed

        Returns:
            ID of the created tool call record

        Example:
            >>> call_id = await tools.record(
            ...     'search',
            ...     started_at=1234567890,
            ...     completed_at=1234567892,
            ...     parameters={'query': 'Python'},
            ...     result={'results': [...]}
            ... )

        Note: We use RETURNING clause which requires explicit cursor close
        when working with CDC-enabled TursoDB connections.
        """
        serialized_params = json.dumps(parameters) if parameters is not None else None
        serialized_result = json.dumps(result) if result is not None else None
        duration_ms = (completed_at - started_at) * 1000
        status = "error" if error else "success"

        cursor = self._db.cursor()
        try:
            await cursor.execute(
                """
                INSERT INTO tool_calls (
                    name, parameters, result, error, status,
                    started_at, completed_at, duration_ms
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                RETURNING id
                """,
                (
                    name,
                    serialized_params,
                    serialized_result,
                    error,
                    status,
                    started_at,
                    completed_at,
                    duration_ms,
                ),
            )
            row = await cursor.fetchone()
            assert row is not None
            call_id = row[0]
        finally:
            await cursor.close()
        await self._db.commit()
        return call_id

    async def get(self, call_id: int) -> Optional[ToolCall]:
        """Get a specific tool call by ID

        Args:
            call_id: ID of the tool call

        Returns:
            ToolCall object or None if not found

        Example:
            >>> call = await tools.get(123)
            >>> if call:
            >>>     print(f"Tool: {call.name}, Status: {call.status}")
        """
        cursor = await self._db.execute("SELECT * FROM tool_calls WHERE id = ?", (call_id,))
        row = await cursor.fetchone()

        if not row:
            return None

        return self._row_to_tool_call(row)

    async def get_by_name(self, name: str, limit: Optional[int] = None) -> List[ToolCall]:
        """Query tool calls by name

        Args:
            name: Name of the tool
            limit: Maximum number of results (optional)

        Returns:
            List of ToolCall objects, ordered by most recent first

        Example:
            >>> calls = await tools.get_by_name('search', limit=10)
            >>> for call in calls:
            >>>     print(f"ID: {call.id}, Status: {call.status}")
        """
        limit_clause = f"LIMIT {limit}" if limit is not None else ""

        cursor = await self._db.execute(
            f"""
            SELECT * FROM tool_calls
            WHERE name = ?
            ORDER BY started_at DESC
            {limit_clause}
            """,
            (name,),
        )
        rows = await cursor.fetchall()

        return [self._row_to_tool_call(row) for row in rows]

    async def get_recent(self, since: int, limit: Optional[int] = None) -> List[ToolCall]:
        """Query recent tool calls

        Args:
            since: Unix timestamp to filter calls after
            limit: Maximum number of results (optional)

        Returns:
            List of ToolCall objects, ordered by most recent first

        Example:
            >>> # Get calls from the last hour
            >>> since = int(time.time()) - 3600
            >>> calls = await tools.get_recent(since)
        """
        limit_clause = f"LIMIT {limit}" if limit is not None else ""

        cursor = await self._db.execute(
            f"""
            SELECT * FROM tool_calls
            WHERE started_at > ?
            ORDER BY started_at DESC
            {limit_clause}
            """,
            (since,),
        )
        rows = await cursor.fetchall()

        return [self._row_to_tool_call(row) for row in rows]

    async def get_stats(self) -> List[ToolCallStats]:
        """Get performance statistics for all tools

        Only includes completed calls (success or error), not pending ones.

        Returns:
            List of ToolCallStats objects, ordered by total calls descending

        Example:
            >>> stats = await tools.get_stats()
            >>> for stat in stats:
            >>>     print(f"{stat.name}: {stat.total_calls} calls, "
            >>>           f"{stat.avg_duration_ms:.2f}ms avg")
        """
        cursor = await self._db.execute("""
            SELECT
                name,
                COUNT(*) as total_calls,
                SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful,
                SUM(CASE WHEN status = 'error' THEN 1 ELSE 0 END) as failed,
                AVG(duration_ms) as avg_duration_ms
            FROM tool_calls
            WHERE status != 'pending'
            GROUP BY name
            ORDER BY total_calls DESC
        """)
        rows = await cursor.fetchall()

        return [
            ToolCallStats(
                name=row[0],
                total_calls=row[1],
                successful=row[2],
                failed=row[3],
                avg_duration_ms=row[4] if row[4] is not None else 0.0,
            )
            for row in rows
        ]

    def _row_to_tool_call(self, row: tuple) -> ToolCall:
        """Helper to convert database row to ToolCall object"""
        return ToolCall(
            id=row[0],
            name=row[1],
            parameters=json.loads(row[2]) if row[2] is not None else None,
            result=json.loads(row[3]) if row[3] is not None else None,
            error=row[4] if row[4] is not None else None,
            status=row[5],
            started_at=row[6],
            completed_at=row[7] if row[7] is not None else None,
            duration_ms=row[8] if row[8] is not None else None,
        )




------------------------------------------
File: sdk/python/examples/filesystem_demo.py
------------------------------------------

"""Filesystem example for AgentFS Python SDK"""

import asyncio
from datetime import datetime

from agentfs_sdk import AgentFS, AgentFSOptions


async def main():
    # Initialize AgentFS with persistent storage
    agentfs = await AgentFS.open(AgentFSOptions(id="filesystem-demo"))

    # Write a file
    print("Writing file...")
    await agentfs.fs.write_file("/documents/readme.txt", "Hello, world!")

    # Read the file
    print("\nReading file...")
    content = await agentfs.fs.read_file("/documents/readme.txt")
    print(f"Content: {content}")

    # Get file stats
    print("\nFile stats:")
    stats = await agentfs.fs.stat("/documents/readme.txt")
    print(f"  Inode: {stats.ino}")
    print(f"  Size: {stats.size} bytes")
    print(f"  Mode: {oct(stats.mode)}")
    print(f"  Links: {stats.nlink}")
    print(f"  Is file: {stats.is_file()}")
    print(f"  Is directory: {stats.is_directory()}")
    print(f"  Created: {datetime.fromtimestamp(stats.ctime).isoformat()}")
    print(f"  Modified: {datetime.fromtimestamp(stats.mtime).isoformat()}")

    # List directory
    print("\nListing /documents:")
    files = await agentfs.fs.readdir("/documents")
    print(f"  Files: {files}")

    # Write more files
    await agentfs.fs.write_file("/documents/notes.txt", "Some notes")
    await agentfs.fs.write_file("/images/photo.jpg", b"binary data here")

    # List root
    print("\nListing /:")
    root_files = await agentfs.fs.readdir("/")
    print(f"  Directories: {root_files}")

    # Check directory stats
    print("\nDirectory stats for /documents:")
    dir_stats = await agentfs.fs.stat("/documents")
    print(f"  Is directory: {dir_stats.is_directory()}")
    print(f"  Mode: {oct(dir_stats.mode)}")

    # Close the database
    await agentfs.close()


if __name__ == "__main__":
    asyncio.run(main())




------------------------------------------
File: sdk/python/examples/kvstore_demo.py
------------------------------------------

"""Key-Value Store example for AgentFS Python SDK"""

import asyncio
import json
import time

from agentfs_sdk import AgentFS, AgentFSOptions


async def main():
    # Initialize AgentFS with persistent storage
    agentfs = await AgentFS.open(AgentFSOptions(id="kvstore-demo"))

    print("=== KvStore Example ===\n")

    # Example 1: Store and retrieve simple values
    print("1. Storing simple values:")
    await agentfs.kv.set("username", "alice")
    await agentfs.kv.set("age", 30)
    await agentfs.kv.set("active", True)

    username = await agentfs.kv.get("username")
    age = await agentfs.kv.get("age")
    active = await agentfs.kv.get("active")

    print(f"  Username: {username}")
    print(f"  Age: {age}")
    print(f"  Active: {active}\n")

    # Example 2: Store and retrieve objects
    print("2. Storing complex objects:")
    user = {
        "id": 1,
        "name": "Alice Johnson",
        "email": "alice@example.com",
        "preferences": {"theme": "dark", "notifications": True},
    }

    await agentfs.kv.set("user:1", user)
    retrieved_user = await agentfs.kv.get("user:1")
    print(f"  Stored user: {json.dumps(retrieved_user, indent=2)}\n")

    # Example 3: Store and retrieve arrays
    print("3. Storing arrays:")
    tags = ["python", "database", "ai", "agent"]
    await agentfs.kv.set("tags", tags)
    retrieved_tags = await agentfs.kv.get("tags")
    assert isinstance(retrieved_tags, list)
    print(f"  Tags: {', '.join(retrieved_tags)}\n")

    # Example 4: Update existing values
    print("4. Updating existing values:")
    print(f"  Age before update: {await agentfs.kv.get('age')}")
    await agentfs.kv.set("age", 31)
    print(f"  Age after update: {await agentfs.kv.get('age')}\n")

    # Example 5: Delete values
    print("5. Deleting values:")
    print(f"  Username before delete: {await agentfs.kv.get('username')}")
    await agentfs.kv.delete("username")
    print(f"  Username after delete: {await agentfs.kv.get('username')}\n")

    # Example 6: Handle non-existent keys
    print("6. Retrieving non-existent keys:")
    non_existent = await agentfs.kv.get("does-not-exist")
    print(f"  Result: {non_existent}\n")

    # Example 7: Use cases for AI agents
    print("7. AI Agent use cases:")

    # Session state
    await agentfs.kv.set(
        "session:current",
        {"conversationId": "conv-123", "userId": "user-456", "startTime": int(time.time() * 1000)},
    )

    # Agent memory
    await agentfs.kv.set(
        "memory:user-preferences",
        {"language": "en", "responseStyle": "concise", "expertise": "intermediate"},
    )

    # Task queue
    await agentfs.kv.set(
        "tasks:pending",
        [
            {"id": 1, "task": "Process document", "priority": "high"},
            {"id": 2, "task": "Send notification", "priority": "low"},
        ],
    )

    print(f"  Session: {json.dumps(await agentfs.kv.get('session:current'), indent=2)}")
    print(f"  Memory: {json.dumps(await agentfs.kv.get('memory:user-preferences'), indent=2)}")
    print(f"  Tasks: {json.dumps(await agentfs.kv.get('tasks:pending'), indent=2)}")

    print("\n=== Example Complete ===")

    # Close the database
    await agentfs.close()


if __name__ == "__main__":
    asyncio.run(main())




------------------------------------------
File: sdk/python/examples/toolcalls_demo.py
------------------------------------------

"""Tool Calls tracking example for AgentFS Python SDK"""

import asyncio
import json
import time

from agentfs_sdk import AgentFS, AgentFSOptions


async def main():
    # Create an agent with persistent storage
    agentfs = await AgentFS.open(AgentFSOptions(id="toolcalls-demo"))

    print("=== Tool Call Tracking Example ===\n")

    # Example 1: Successful tool call
    print("1. Tracking a successful web search:")
    start_time1 = int(time.time())

    # Simulate some work
    await asyncio.sleep(0.1)

    end_time1 = int(time.time())
    search_id = await agentfs.tools.record(
        "web_search",
        start_time1,
        end_time1,
        parameters={"query": "AI agents and LLMs", "maxResults": 10},
        result={
            "results": [
                {"title": "Understanding AI Agents", "url": "https://example.com/1"},
                {"title": "LLM Best Practices", "url": "https://example.com/2"},
            ],
            "count": 2,
        },
    )
    print(f"   Recorded tool call with ID: {search_id}\n")

    # Example 2: Failed tool call
    print("2. Tracking a failed API call:")
    start_time2 = int(time.time())

    await asyncio.sleep(0.05)

    end_time2 = int(time.time())
    api_id = await agentfs.tools.record(
        "api_call",
        start_time2,
        end_time2,
        parameters={"endpoint": "/users", "method": "GET"},
        error="Connection timeout after 30s",
    )
    print(f"   Recorded failed call with ID: {api_id}\n")

    # Example 3: Multiple tool calls
    print("3. Tracking multiple database queries:")
    for i in range(3):
        start = int(time.time())
        await asyncio.sleep(0.02)
        end = int(time.time())

        await agentfs.tools.record(
            "database_query",
            start,
            end,
            parameters={"sql": f"SELECT * FROM users WHERE id = {i + 1}"},
            result={"rows": 1},
        )
    print("   Created 3 database query records\n")

    # Example 4: Using start/success/error pattern
    print("4. Using start/success pattern:")
    call_id = await agentfs.tools.start("data_processing", {"file": "data.csv"})
    await asyncio.sleep(0.05)
    await agentfs.tools.success(call_id, {"rows_processed": 1000})
    print(f"   Completed tool call {call_id}\n")

    # Query tool calls by name
    print("5. Querying tool calls by name:")
    searches = await agentfs.tools.get_by_name("web_search")
    print(f"   Found {len(searches)} web search calls")
    if searches:
        search = searches[0]
        print(f"   - Duration: {search.duration_ms}ms")
        print(f"   - Parameters: {json.dumps(search.parameters)}")
        print(f"   - Result: {json.dumps(search.result)}")
    print()

    # Get recent tool calls
    print("6. Getting recent tool calls:")
    one_minute_ago = int(time.time()) - 60
    recent = await agentfs.tools.get_recent(one_minute_ago)
    print(f"   Found {len(recent)} calls in the last minute:")
    for tc in recent:
        status = "failed" if tc.error else "success"
        print(f"   - {tc.name} ({status})")
    print()

    # Get performance statistics
    print("7. Performance statistics:")
    stats = await agentfs.tools.get_stats()
    print("   Tool Performance:")
    for stat in stats:
        print(f"   - {stat.name}:")
        print(f"     Total: {stat.total_calls}, Success: {stat.successful}, Failed: {stat.failed}")
        print(f"     Avg Duration: {stat.avg_duration_ms:.2f}ms")

    # Clean up
    await agentfs.close()
    print("\n✓ Example completed successfully!")


if __name__ == "__main__":
    asyncio.run(main())




------------------------------------------
File: sdk/python/pyproject.toml
------------------------------------------

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "agentfs-sdk"
version = "0.6.0"
description = "AgentFS Python SDK - A filesystem and key-value store for AI agents"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
keywords = ["ai", "agent", "turso", "sqlite", "key-value", "filesystem"]
authors = [
    {name = "Turso"}
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    'Operating System :: POSIX :: Linux',
    'Operating System :: Microsoft :: Windows',
    'Operating System :: MacOS',
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

dependencies = [
    "pyturso==0.4.4",
]

[dependency-groups]
dev = [
    "pytest>=9.0.0",
    "pytest-asyncio>=1.3.0",
    "ruff>=0.14.0",
    "ty>=0.0.1a34",
]

[project.urls]
Homepage = "https://github.com/tursodatabase/agentfs"
Source = "https://github.com/tursodatabase/agentfs"
Repository = "https://github.com/tursodatabase/agentfs"
Issues = "https://github.com/tursodatabase/agentfs/issues"

[tool.setuptools.packages.find]
include = ["agentfs_sdk*"]

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I"]
ignore = []

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.pytest.ini_options]
asyncio_mode = "auto"
testpaths = ["tests"]




------------------------------------------
File: sdk/python/README.md
------------------------------------------

# AgentFS Python SDK

A filesystem and key-value store for AI agents, powered by SQLite and [pyturso](https://pypi.org/project/pyturso/).

## Installation

```bash
pip install agentfs-sdk
```

## Quick Start

```python
import asyncio
from agentfs_sdk import AgentFS, AgentFSOptions

async def main():
    # Open an agent filesystem
    agent = await AgentFS.open(AgentFSOptions(id='my-agent'))

    # Use key-value store
    await agent.kv.set('config', {'debug': True, 'version': '1.0'})
    config = await agent.kv.get('config')
    print(f"Config: {config}")

    # Use filesystem
    await agent.fs.write_file('/data/notes.txt', 'Hello, AgentFS!')
    content = await agent.fs.read_file('/data/notes.txt')
    print(f"Content: {content}")

    # Track tool calls
    call_id = await agent.tools.start('search', {'query': 'Python'})
    await agent.tools.success(call_id, {'results': ['result1', 'result2']})

    # Get statistics
    stats = await agent.tools.get_stats()
    for stat in stats:
        print(f"{stat.name}: {stat.total_calls} calls, {stat.avg_duration_ms:.2f}ms avg")

    # Close the database
    await agent.close()

if __name__ == '__main__':
    asyncio.run(main())
```

## Features

### Key-Value Store

Simple key-value storage with JSON serialization:

```python
# Set a value
await agent.kv.set('user:123', {'name': 'Alice', 'age': 30})

# Get a value
user = await agent.kv.get('user:123')

# List by prefix
users = await agent.kv.list('user:')

# Delete a value
await agent.kv.delete('user:123')
```

### Filesystem

POSIX-like filesystem operations:

```python
# Write a file (creates parent directories automatically)
await agent.fs.write_file('/data/config.json', '{"key": "value"}')

# Read a file
content = await agent.fs.read_file('/data/config.json')

# Read as bytes
data = await agent.fs.read_file('/data/image.png', encoding=None)

# List directory
entries = await agent.fs.readdir('/data')

# Get file stats
stats = await agent.fs.stat('/data/config.json')
print(f"Size: {stats.size} bytes")
print(f"Modified: {stats.mtime}")
print(f"Is file: {stats.is_file()}")

# Delete a file
await agent.fs.delete_file('/data/config.json')
```

### Tool Calls Tracking

Track and analyze tool/function calls:

```python
# Start a tool call
call_id = await agent.tools.start('search', {'query': 'Python'})

# Mark as successful
await agent.tools.success(call_id, {'results': [...]})

# Or mark as failed
await agent.tools.error(call_id, 'Connection timeout')

# Record a completed call
await agent.tools.record(
    'search',
    started_at=1234567890,
    completed_at=1234567892,
    parameters={'query': 'Python'},
    result={'results': [...]}
)

# Query tool calls
calls = await agent.tools.get_by_name('search', limit=10)
recent = await agent.tools.get_recent(since=1234567890)

# Get statistics
stats = await agent.tools.get_stats()
for stat in stats:
    print(f"{stat.name}: {stat.successful}/{stat.total_calls} successful")
```

## Configuration

### Using Agent ID

Creates a database at `.agentfs/{id}.db`:

```python
agent = await AgentFS.open(AgentFSOptions(id='my-agent'))
```

### Using Custom Path

Specify a custom database path:

```python
agent = await AgentFS.open(AgentFSOptions(path='./data/mydb.db'))
```

### Using Both

You can specify both for clarity:

```python
agent = await AgentFS.open(AgentFSOptions(id='my-agent', path='./data/mydb.db'))
```

## Context Manager Support

Use AgentFS with async context managers:

```python
async with await AgentFS.open(AgentFSOptions(id='my-agent')) as agent:
    await agent.kv.set('key', 'value')
    # Database is automatically closed when exiting the context
```

## Development

### Setup

```bash
# Install dependencies
uv sync --group dev

# Run tests
uv run pytest

# Format code
uv run ruff format agentfs_sdk tests

# Check code
uv run ruff check agentfs_sdk tests
```

## License

MIT License - see LICENSE file for details.

## Links

- [GitHub Repository](https://github.com/tursodatabase/agentfs)
- [TypeScript SDK](https://github.com/tursodatabase/agentfs/tree/main/sdk/typescript)
- [tursodb](https://github.com/tursodatabase/turso)
- [pyturso](https://pypi.org/project/pyturso/)




------------------------------------------
File: sdk/python/tests/__init__.py
------------------------------------------

"""Tests for AgentFS Python SDK"""




------------------------------------------
File: sdk/python/tests/test_agentfs.py
------------------------------------------

"""AgentFS Integration Tests"""

import os
import tempfile

import pytest

from agentfs_sdk import AgentFS, AgentFSOptions


@pytest.mark.asyncio
class TestAgentFSIntegration:
    """Integration tests for AgentFS"""

    async def test_initialize_with_id(self):
        """Should successfully initialize with an id"""
        with tempfile.TemporaryDirectory() as tmpdir:
            old_cwd = os.getcwd()
            os.chdir(tmpdir)

            try:
                agent = await AgentFS.open(AgentFSOptions(id="test-agent"))
                assert agent is not None
                assert isinstance(agent, AgentFS)
                await agent.close()
            finally:
                os.chdir(old_cwd)

    async def test_initialize_with_path(self):
        """Should initialize with explicit path"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            agent = await AgentFS.open(AgentFSOptions(path=db_path))
            assert agent is not None
            assert isinstance(agent, AgentFS)
            await agent.close()
            assert os.path.exists(db_path)

    async def test_require_id_or_path(self):
        """Should require at least id or path"""
        with pytest.raises(ValueError, match="requires at least 'id' or 'path'"):
            await AgentFS.open(AgentFSOptions())

    async def test_multiple_instances_different_ids(self):
        """Should allow multiple instances with different ids"""
        with tempfile.TemporaryDirectory() as tmpdir:
            old_cwd = os.getcwd()
            os.chdir(tmpdir)

            try:
                agent1 = await AgentFS.open(AgentFSOptions(id="test-agent-1"))
                agent2 = await AgentFS.open(AgentFSOptions(id="test-agent-2"))

                assert agent1 is not None
                assert agent2 is not None
                assert agent1 is not agent2

                await agent1.close()
                await agent2.close()
            finally:
                os.chdir(old_cwd)

    async def test_database_persistence_to_agentfs_directory(self):
        """Should persist database file to .agentfs directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            old_cwd = os.getcwd()
            os.chdir(tmpdir)

            try:
                agent = await AgentFS.open(AgentFSOptions(id="test-agent"))
                await agent.close()

                # Check that database file exists in .agentfs directory
                db_path = ".agentfs/test-agent.db"
                assert os.path.exists(db_path)
            finally:
                os.chdir(old_cwd)

    async def test_reuse_existing_database(self):
        """Should reuse existing database file with same id"""
        with tempfile.TemporaryDirectory() as tmpdir:
            old_cwd = os.getcwd()
            os.chdir(tmpdir)

            try:
                # Create first instance and write data
                agent1 = await AgentFS.open(AgentFSOptions(id="persistence-test"))
                await agent1.kv.set("test", "value1")
                await agent1.close()

                # Create second instance with same id - should be able to read the data
                agent2 = await AgentFS.open(AgentFSOptions(id="persistence-test"))
                value = await agent2.kv.get("test")

                assert agent1 is not None
                assert agent2 is not None
                assert value == "value1"

                await agent2.close()
            finally:
                os.chdir(old_cwd)

    async def test_context_manager(self):
        """Should work as a context manager"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")

            async with await AgentFS.open(AgentFSOptions(path=db_path)) as agent:
                await agent.kv.set("test", "value")
                value = await agent.kv.get("test")
                assert value == "value"

    async def test_get_database(self):
        """Should return the underlying database connection"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            agent = await AgentFS.open(AgentFSOptions(path=db_path))

            db = agent.get_database()
            assert db is not None

            await agent.close()

    async def test_validate_id_format(self):
        """Should validate agent ID format"""
        with tempfile.TemporaryDirectory() as tmpdir:
            old_cwd = os.getcwd()
            os.chdir(tmpdir)

            try:
                # Invalid characters in ID
                with pytest.raises(ValueError, match="alphanumeric characters"):
                    await AgentFS.open(AgentFSOptions(id="invalid id with spaces"))

                with pytest.raises(ValueError, match="alphanumeric characters"):
                    await AgentFS.open(AgentFSOptions(id="invalid@id"))
            finally:
                os.chdir(old_cwd)




------------------------------------------
File: sdk/python/tests/test_basic.py
------------------------------------------

"""Basic tests for AgentFS Python SDK"""

import os
import tempfile

import pytest

from agentfs_sdk import AgentFS, AgentFSOptions


@pytest.mark.asyncio
async def test_agentfs_open_with_id():
    """Test opening AgentFS with an ID"""
    with tempfile.TemporaryDirectory() as tmpdir:
        old_cwd = os.getcwd()
        os.chdir(tmpdir)

        try:
            agentfs = await AgentFS.open(AgentFSOptions(id="test-agent"))
            assert agentfs is not None
            assert agentfs.kv is not None
            assert agentfs.fs is not None
            assert agentfs.tools is not None

            await agentfs.close()
        finally:
            os.chdir(old_cwd)


@pytest.mark.asyncio
async def test_agentfs_open_with_path():
    """Test opening AgentFS with a custom path"""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = os.path.join(tmpdir, "test.db")

        agentfs = await AgentFS.open(AgentFSOptions(path=db_path))
        assert agentfs is not None

        await agentfs.close()
        assert os.path.exists(db_path)


@pytest.mark.asyncio
async def test_kvstore_basic():
    """Test basic key-value operations"""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = os.path.join(tmpdir, "test.db")

        agentfs = await AgentFS.open(AgentFSOptions(path=db_path))

        # Set and get
        await agentfs.kv.set("test_key", "test_value")
        value = await agentfs.kv.get("test_key")
        assert value == "test_value"

        # Set complex object
        obj = {"name": "Alice", "age": 30}
        await agentfs.kv.set("user", obj)
        retrieved = await agentfs.kv.get("user")
        assert retrieved == obj

        # Delete
        await agentfs.kv.delete("test_key")
        value = await agentfs.kv.get("test_key")
        assert value is None

        await agentfs.close()


@pytest.mark.asyncio
async def test_filesystem_basic():
    """Test basic filesystem operations"""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = os.path.join(tmpdir, "test.db")

        agentfs = await AgentFS.open(AgentFSOptions(path=db_path))

        # Write and read file
        await agentfs.fs.write_file("/test.txt", "Hello, World!")
        content = await agentfs.fs.read_file("/test.txt")
        assert content == "Hello, World!"

        # Create nested file (auto-create parent dirs)
        await agentfs.fs.write_file("/dir1/dir2/file.txt", "nested")
        content = await agentfs.fs.read_file("/dir1/dir2/file.txt")
        assert content == "nested"

        # List directory
        files = await agentfs.fs.readdir("/dir1")
        assert "dir2" in files

        # Get stats
        stats = await agentfs.fs.stat("/test.txt")
        assert stats.is_file()
        assert stats.size == len("Hello, World!")

        await agentfs.close()


@pytest.mark.asyncio
async def test_toolcalls_basic():
    """Test basic tool call tracking"""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = os.path.join(tmpdir, "test.db")

        agentfs = await AgentFS.open(AgentFSOptions(path=db_path))

        # Record a tool call
        import time

        start = int(time.time())
        end = start + 1

        call_id = await agentfs.tools.record(
            "test_tool", start, end, parameters={"param": "value"}, result={"result": "success"}
        )

        assert call_id > 0

        # Get the tool call
        call = await agentfs.tools.get(call_id)
        assert call is not None
        assert call.name == "test_tool"
        assert call.parameters == {"param": "value"}
        assert call.result == {"result": "success"}
        assert call.status == "success"

        # Get stats
        stats = await agentfs.tools.get_stats()
        assert len(stats) == 1
        assert stats[0].name == "test_tool"
        assert stats[0].total_calls == 1

        await agentfs.close()


@pytest.mark.asyncio
async def test_context_manager():
    """Test using AgentFS as a context manager"""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = os.path.join(tmpdir, "test.db")

        async with await AgentFS.open(AgentFSOptions(path=db_path)) as agentfs:
            await agentfs.kv.set("test", "value")
            value = await agentfs.kv.get("test")
            assert value == "value"

        # Database should be closed after exiting context




------------------------------------------
File: sdk/python/tests/test_filesystem.py
------------------------------------------

"""Filesystem Integration Tests"""

import os
import tempfile

import pytest
from turso.aio import connect

from agentfs_sdk import ErrnoException, Filesystem


@pytest.mark.asyncio
class TestFilesystemWriteOperations:
    """Filesystem write operations"""

    async def test_write_and_read_simple_text_file(self):
        """Should write and read a simple text file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/test.txt", "Hello, World!")
            content = await fs.read_file("/test.txt")
            assert content == "Hello, World!"
            await db.close()

    async def test_write_files_in_subdirectories(self):
        """Should write and read files in subdirectories"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/subdir/file.txt", "nested content")
            content = await fs.read_file("/dir/subdir/file.txt")
            assert content == "nested content"
            await db.close()

    async def test_overwrite_existing_file(self):
        """Should overwrite existing file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/overwrite.txt", "original content")
            await fs.write_file("/overwrite.txt", "new content")
            content = await fs.read_file("/overwrite.txt")
            assert content == "new content"
            await db.close()

    async def test_handle_empty_file_content(self):
        """Should handle empty file content"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/empty.txt", "")
            content = await fs.read_file("/empty.txt")
            assert content == ""
            await db.close()

    async def test_handle_large_file_content(self):
        """Should handle large file content"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            large_content = "x" * 100000
            await fs.write_file("/large.txt", large_content)
            content = await fs.read_file("/large.txt")
            assert content == large_content
            await db.close()

    async def test_special_characters_in_content(self):
        """Should handle files with special characters in content"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            special_content = "Special chars: \n\t\r\"'\\"
            await fs.write_file("/special.txt", special_content)
            content = await fs.read_file("/special.txt")
            assert content == special_content
            await db.close()


@pytest.mark.asyncio
class TestFilesystemReadOperations:
    """Filesystem read operations"""

    async def test_error_reading_nonexistent_file(self):
        """Should throw error when reading non-existent file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException):
                await fs.read_file("/non-existent.txt")
            await db.close()

    async def test_read_multiple_files(self):
        """Should read multiple different files"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/file1.txt", "content 1")
            await fs.write_file("/file2.txt", "content 2")
            await fs.write_file("/file3.txt", "content 3")

            assert await fs.read_file("/file1.txt") == "content 1"
            assert await fs.read_file("/file2.txt") == "content 2"
            assert await fs.read_file("/file3.txt") == "content 3"
            await db.close()


@pytest.mark.asyncio
class TestFilesystemDirectoryOperations:
    """Filesystem directory operations"""

    async def test_list_files_in_root(self):
        """Should list files in root directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/file1.txt", "content 1")
            await fs.write_file("/file2.txt", "content 2")
            await fs.write_file("/file3.txt", "content 3")

            files = await fs.readdir("/")
            assert "file1.txt" in files
            assert "file2.txt" in files
            assert "file3.txt" in files
            assert len(files) == 3
            await db.close()

    async def test_list_files_in_subdirectory(self):
        """Should list files in subdirectory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file1.txt", "content 1")
            await fs.write_file("/dir/file2.txt", "content 2")
            await fs.write_file("/other/file3.txt", "content 3")

            files = await fs.readdir("/dir")
            assert "file1.txt" in files
            assert "file2.txt" in files
            assert "file3.txt" not in files
            assert len(files) == 2
            await db.close()

    async def test_distinguish_files_in_different_directories(self):
        """Should distinguish between files in different directories"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir1/file.txt", "content 1")
            await fs.write_file("/dir2/file.txt", "content 2")

            files1 = await fs.readdir("/dir1")
            files2 = await fs.readdir("/dir2")

            assert "file.txt" in files1
            assert "file.txt" in files2
            assert len(files1) == 1
            assert len(files2) == 1
            await db.close()

    async def test_list_subdirectories(self):
        """Should list subdirectories within a directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/parent/child1/file.txt", "content")
            await fs.write_file("/parent/child2/file.txt", "content")
            await fs.write_file("/parent/file.txt", "content")

            entries = await fs.readdir("/parent")
            assert "file.txt" in entries
            assert "child1" in entries
            assert "child2" in entries
            await db.close()

    async def test_nested_directory_structures(self):
        """Should handle nested directory structures"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/a/b/c/d/file.txt", "deep content")
            files = await fs.readdir("/a/b/c/d")
            assert "file.txt" in files
            await db.close()


@pytest.mark.asyncio
class TestFilesystemDeleteOperations:
    """Filesystem delete operations"""

    async def test_delete_existing_file(self):
        """Should delete an existing file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/delete-me.txt", "content")
            await fs.delete_file("/delete-me.txt")
            with pytest.raises(ErrnoException):
                await fs.read_file("/delete-me.txt")
            await db.close()

    async def test_delete_nonexistent_file(self):
        """Should handle deleting non-existent file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.delete_file("/non-existent.txt")
            await db.close()

    async def test_delete_and_update_directory_listing(self):
        """Should delete file and update directory listing"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file1.txt", "content 1")
            await fs.write_file("/dir/file2.txt", "content 2")

            await fs.delete_file("/dir/file1.txt")

            files = await fs.readdir("/dir")
            assert "file1.txt" not in files
            assert "file2.txt" in files
            assert len(files) == 1
            await db.close()

    async def test_recreate_deleted_file(self):
        """Should allow recreating deleted file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/recreate.txt", "original")
            await fs.delete_file("/recreate.txt")
            await fs.write_file("/recreate.txt", "new content")
            content = await fs.read_file("/recreate.txt")
            assert content == "new content"
            await db.close()


@pytest.mark.asyncio
class TestFilesystemPathHandling:
    """Filesystem path handling"""

    async def test_paths_with_trailing_slashes(self):
        """Should handle paths with trailing slashes"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file.txt", "content")
            files1 = await fs.readdir("/dir")
            files2 = await fs.readdir("/dir/")
            assert files1 == files2
            await db.close()

    async def test_paths_with_special_characters(self):
        """Should handle paths with special characters"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            special_path = "/dir-with-dash/file_with_underscore.txt"
            await fs.write_file(special_path, "content")
            content = await fs.read_file(special_path)
            assert content == "content"
            await db.close()


@pytest.mark.asyncio
class TestFilesystemIntegrity:
    """Filesystem integrity tests"""

    async def test_maintain_file_hierarchy_integrity(self):
        """Should maintain file hierarchy integrity"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/root.txt", "root")
            await fs.write_file("/dir1/file.txt", "dir1")
            await fs.write_file("/dir2/file.txt", "dir2")
            await fs.write_file("/dir1/subdir/file.txt", "subdir")

            assert await fs.read_file("/root.txt") == "root"
            assert await fs.read_file("/dir1/file.txt") == "dir1"
            assert await fs.read_file("/dir2/file.txt") == "dir2"
            assert await fs.read_file("/dir1/subdir/file.txt") == "subdir"

            root_files = await fs.readdir("/")
            assert "root.txt" in root_files
            assert "dir1" in root_files
            assert "dir2" in root_files
            await db.close()

    async def test_multiple_files_same_name_different_directories(self):
        """Should support multiple files with same name in different directories"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir1/config.json", '{"version": 1}')
            await fs.write_file("/dir2/config.json", '{"version": 2}')

            assert await fs.read_file("/dir1/config.json") == '{"version": 1}'
            assert await fs.read_file("/dir2/config.json") == '{"version": 2}'
            await db.close()


@pytest.mark.asyncio
class TestFilesystemStandaloneUsage:
    """Filesystem standalone usage tests"""

    async def test_work_with_in_memory_database(self):
        """Should work with in-memory database"""
        db = await connect(":memory:")
        await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
        fs = await Filesystem.from_database(db)

        await fs.write_file("/test.txt", "standalone content")
        content = await fs.read_file("/test.txt")
        assert content == "standalone content"
        await db.close()

    async def test_maintain_isolation_between_instances(self):
        """Should maintain isolation between instances"""
        db1 = await connect(":memory:")
        await db1.execute("PRAGMA unstable_capture_data_changes_conn('full')")
        fs1 = await Filesystem.from_database(db1)

        db2 = await connect(":memory:")
        await db2.execute("PRAGMA unstable_capture_data_changes_conn('full')")
        fs2 = await Filesystem.from_database(db2)

        await fs1.write_file("/test.txt", "fs1 content")
        await fs2.write_file("/test.txt", "fs2 content")

        assert await fs1.read_file("/test.txt") == "fs1 content"
        assert await fs2.read_file("/test.txt") == "fs2 content"

        await db1.close()
        await db2.close()


@pytest.mark.asyncio
class TestFilesystemPersistence:
    """Filesystem persistence tests"""

    async def test_persist_across_instances(self):
        """Should persist data across Filesystem instances"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/persist.txt", "persistent content")

            new_fs = await Filesystem.from_database(db)
            content = await new_fs.read_file("/persist.txt")
            assert content == "persistent content"
            await db.close()


@pytest.mark.asyncio
class TestFilesystemChunkSize:
    """Filesystem chunk size tests"""

    async def test_default_chunk_size(self):
        """Should have default chunk size of 4096"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            assert fs.get_chunk_size() == 4096
            await db.close()

    async def test_write_file_smaller_than_chunk_size(self):
        """Should write file smaller than chunk size"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            # Write a file smaller than chunk_size (100 bytes)
            data = "x" * 100
            await fs.write_file("/small.txt", data)

            # Read it back
            read_data = await fs.read_file("/small.txt")
            assert len(read_data) == 100
            assert read_data == data
            await db.close()

    async def test_write_file_exact_chunk_size(self):
        """Should write file exactly chunk size"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Write exactly chunk_size bytes
            data = bytes(i % 256 for i in range(chunk_size))
            await fs.write_file("/exact.txt", data)

            # Read it back
            read_data = await fs.read_file("/exact.txt", encoding=None)
            assert len(read_data) == chunk_size
            await db.close()

    async def test_write_file_over_chunk_size(self):
        """Should write file one byte over chunk size"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Write chunk_size + 1 bytes
            data = bytes(i % 256 for i in range(chunk_size + 1))
            await fs.write_file("/overflow.txt", data)

            # Read it back
            read_data = await fs.read_file("/overflow.txt", encoding=None)
            assert len(read_data) == chunk_size + 1
            await db.close()

    async def test_write_file_spanning_multiple_chunks(self):
        """Should write file spanning multiple chunks"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Write ~2.5 chunks worth of data
            data_size = int(chunk_size * 2.5)
            data = bytes(i % 256 for i in range(data_size))
            await fs.write_file("/multi.txt", data)

            # Read it back
            read_data = await fs.read_file("/multi.txt", encoding=None)
            assert len(read_data) == data_size
            await db.close()


@pytest.mark.asyncio
class TestFilesystemDataIntegrity:
    """Filesystem data integrity tests"""

    async def test_roundtrip_data_byte_for_byte(self):
        """Should roundtrip data byte-for-byte"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Create data that spans chunk boundaries with identifiable patterns
            data_size = chunk_size * 3 + 123  # Odd size spanning 4 chunks

            data = bytes(i % 256 for i in range(data_size))
            await fs.write_file("/roundtrip.bin", data)

            read_data = await fs.read_file("/roundtrip.bin", encoding=None)
            assert len(read_data) == data_size
            assert read_data == data
            await db.close()

    async def test_handle_binary_data_with_null_bytes(self):
        """Should handle binary data with null bytes"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Create data with null bytes at chunk boundaries
            data = bytearray(chunk_size * 2 + 100)
            # Put nulls at the chunk boundary
            data[chunk_size - 1] = 0
            data[chunk_size] = 0
            data[chunk_size + 1] = 0
            # Put some non-null bytes around
            data[chunk_size - 2] = 0xFF
            data[chunk_size + 2] = 0xFF

            await fs.write_file("/nulls.bin", bytes(data))
            read_data = await fs.read_file("/nulls.bin", encoding=None)

            assert read_data[chunk_size - 2] == 0xFF
            assert read_data[chunk_size - 1] == 0
            assert read_data[chunk_size] == 0
            assert read_data[chunk_size + 1] == 0
            assert read_data[chunk_size + 2] == 0xFF
            await db.close()

    async def test_preserve_chunk_ordering(self):
        """Should preserve chunk ordering"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()
            # Create sequential bytes spanning multiple chunks
            data_size = chunk_size * 5
            data = bytes(i % 256 for i in range(data_size))
            await fs.write_file("/sequential.bin", data)

            read_data = await fs.read_file("/sequential.bin", encoding=None)

            # Verify every byte is in the correct position
            for i in range(data_size):
                assert read_data[i] == i % 256
            await db.close()


@pytest.mark.asyncio
class TestFilesystemEdgeCases:
    """Filesystem edge case tests"""

    async def test_empty_file_with_zero_chunks(self):
        """Should handle empty file with zero chunks"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            # Write empty file
            await fs.write_file("/empty.txt", "")

            # Read it back
            read_data = await fs.read_file("/empty.txt")
            assert read_data == ""

            # Verify size is 0
            stats = await fs.stat("/empty.txt")
            assert stats.size == 0
            await db.close()

    async def test_overwrite_large_file_with_smaller(self):
        """Should overwrite large file with smaller file and clean up chunks"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()

            # Write initial large file (3 chunks)
            initial_data = bytes(i % 256 for i in range(chunk_size * 3))
            await fs.write_file("/overwrite.txt", initial_data)

            # Overwrite with smaller file (1 chunk)
            new_data = "x" * 100
            await fs.write_file("/overwrite.txt", new_data)

            # Verify old chunks are gone and new data is correct
            read_data = await fs.read_file("/overwrite.txt")
            assert read_data == new_data

            # Verify size is updated
            stats = await fs.stat("/overwrite.txt")
            assert stats.size == 100
            await db.close()

    async def test_overwrite_small_file_with_larger(self):
        """Should overwrite small file with larger file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            chunk_size = fs.get_chunk_size()

            # Write initial small file (1 chunk)
            initial_data = "x" * 100
            await fs.write_file("/grow.txt", initial_data)

            # Overwrite with larger file (3 chunks)
            new_data = bytes(i % 256 for i in range(chunk_size * 3))
            await fs.write_file("/grow.txt", new_data)

            # Verify data is correct
            read_data = await fs.read_file("/grow.txt", encoding=None)
            assert len(read_data) == chunk_size * 3
            await db.close()

    async def test_very_large_file(self):
        """Should handle very large file (1MB)"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            # Write 1MB file
            data_size = 1024 * 1024
            data = bytes(i % 256 for i in range(data_size))
            await fs.write_file("/large.bin", data)

            read_data = await fs.read_file("/large.bin", encoding=None)
            assert len(read_data) == data_size
            await db.close()


@pytest.mark.asyncio
class TestFilesystemStats:
    """Filesystem stats tests"""

    async def test_stat_file(self):
        """Should get file statistics"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            content = "Hello, World!"
            await fs.write_file("/test.txt", content)

            stats = await fs.stat("/test.txt")
            assert stats.is_file()
            assert not stats.is_directory()
            assert stats.size == len(content)
            assert stats.ino > 0
            await db.close()

    async def test_stat_directory(self):
        """Should get directory statistics"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file.txt", "content")

            stats = await fs.stat("/dir")
            assert stats.is_directory()
            assert not stats.is_file()
            await db.close()

    async def test_stat_nonexistent_path(self):
        """Should throw error for non-existent path"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.stat("/nonexistent")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemMkdir:
    """Tests for mkdir() operation"""

    async def test_create_directory(self):
        """Should create a directory with mkdir()"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/newdir")
            entries = await fs.readdir("/")
            assert "newdir" in entries
            await db.close()

    async def test_mkdir_throws_eexist_for_existing_directory(self):
        """Should throw EEXIST when mkdir() is called on an existing directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/exists")
            with pytest.raises(ErrnoException, match="EEXIST"):
                await fs.mkdir("/exists")
            await db.close()

    async def test_mkdir_throws_enoent_for_missing_parent(self):
        """Should throw ENOENT when parent directory does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.mkdir("/missing-parent/child")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemRm:
    """Tests for rm() operation"""

    async def test_remove_file(self):
        """Should remove a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/rmfile.txt", "content")
            await fs.rm("/rmfile.txt")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.read_file("/rmfile.txt")
            await db.close()

    async def test_rm_force_does_not_throw_for_missing_file(self):
        """Should not throw when force=True and path does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            # Should not raise
            await fs.rm("/does-not-exist", force=True)
            await db.close()

    async def test_rm_throws_enoent_without_force(self):
        """Should throw ENOENT when force=False and path does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.rm("/does-not-exist")
            await db.close()

    async def test_rm_throws_eisdir_for_directory_without_recursive(self):
        """Should throw EISDIR when trying to rm a directory without recursive"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/rmdir")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.rm("/rmdir")
            await db.close()

    async def test_rm_recursive_removes_directory_tree(self):
        """Should remove a directory recursively"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/tree/a/b/c.txt", "content")
            await fs.rm("/tree", recursive=True)
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.readdir("/tree")
            root = await fs.readdir("/")
            assert "tree" not in root
            await db.close()


@pytest.mark.asyncio
class TestFilesystemRmdir:
    """Tests for rmdir() operation"""

    async def test_remove_empty_directory(self):
        """Should remove an empty directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/emptydir")
            await fs.rmdir("/emptydir")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.readdir("/emptydir")
            root = await fs.readdir("/")
            assert "emptydir" not in root
            await db.close()

    async def test_rmdir_throws_enotempty_for_non_empty_directory(self):
        """Should throw ENOTEMPTY when directory is not empty"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/nonempty/file.txt", "content")
            with pytest.raises(ErrnoException, match="ENOTEMPTY"):
                await fs.rmdir("/nonempty")
            await db.close()

    async def test_rmdir_throws_enotdir_for_file(self):
        """Should throw ENOTDIR when path is a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/afile", "content")
            with pytest.raises(ErrnoException, match="ENOTDIR"):
                await fs.rmdir("/afile")
            await db.close()

    async def test_rmdir_throws_eperm_for_root(self):
        """Should throw EPERM when attempting to remove root"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="EPERM"):
                await fs.rmdir("/")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemRename:
    """Tests for rename() operation"""

    async def test_rename_file(self):
        """Should rename a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/a.txt", "hello")
            await fs.rename("/a.txt", "/b.txt")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.read_file("/a.txt")
            content = await fs.read_file("/b.txt", "utf-8")
            assert content == "hello"
            await db.close()

    async def test_rename_directory_preserves_contents(self):
        """Should rename a directory and preserve its contents"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/olddir/sub/file.txt", "content")
            await fs.rename("/olddir", "/newdir")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.readdir("/olddir")
            content = await fs.read_file("/newdir/sub/file.txt", "utf-8")
            assert content == "content"
            await db.close()

    async def test_rename_overwrites_destination_file(self):
        """Should overwrite destination file if it exists"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/src.txt", "src")
            await fs.write_file("/dst.txt", "dst")
            await fs.rename("/src.txt", "/dst.txt")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.read_file("/src.txt")
            content = await fs.read_file("/dst.txt", "utf-8")
            assert content == "src"
            await db.close()

    async def test_rename_throws_eisdir_for_file_to_directory(self):
        """Should throw EISDIR when renaming a file onto a directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file.txt", "content")
            await fs.write_file("/file.txt", "content")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.rename("/file.txt", "/dir")
            await db.close()

    async def test_rename_throws_enotdir_for_directory_to_file(self):
        """Should throw ENOTDIR when renaming a directory onto a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/somedir")
            await fs.write_file("/somefile", "content")
            with pytest.raises(ErrnoException, match="ENOTDIR"):
                await fs.rename("/somedir", "/somefile")
            await db.close()

    async def test_rename_replaces_empty_directory(self):
        """Should replace an existing empty directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/fromdir")
            await fs.mkdir("/todir")
            await fs.rename("/fromdir", "/todir")
            root = await fs.readdir("/")
            assert "todir" in root
            assert "fromdir" not in root
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.readdir("/fromdir")
            await db.close()

    async def test_rename_throws_enotempty_for_non_empty_destination(self):
        """Should throw ENOTEMPTY when replacing a non-empty directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/fromdir")
            await fs.write_file("/todir/file.txt", "content")
            with pytest.raises(ErrnoException, match="ENOTEMPTY"):
                await fs.rename("/fromdir", "/todir")
            await db.close()

    async def test_rename_throws_eperm_for_root(self):
        """Should throw EPERM when attempting to rename root"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="EPERM"):
                await fs.rename("/", "/x")
            await db.close()

    async def test_rename_throws_einval_for_directory_into_subdirectory(self):
        """Should throw EINVAL when renaming a directory into its own subdirectory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/cycle/sub/file.txt", "content")
            with pytest.raises(ErrnoException, match="EINVAL"):
                await fs.rename("/cycle", "/cycle/sub/moved")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemCopyFile:
    """Tests for copy_file() operation"""

    async def test_copy_file(self):
        """Should copy a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/src.txt", "hello")
            await fs.copy_file("/src.txt", "/dst.txt")
            src_content = await fs.read_file("/src.txt", "utf-8")
            dst_content = await fs.read_file("/dst.txt", "utf-8")
            assert src_content == "hello"
            assert dst_content == "hello"
            await db.close()

    async def test_copy_file_overwrites_destination(self):
        """Should overwrite destination if it exists"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/src.txt", "src")
            await fs.write_file("/dst.txt", "dst")
            await fs.copy_file("/src.txt", "/dst.txt")
            dst_content = await fs.read_file("/dst.txt", "utf-8")
            assert dst_content == "src"
            await db.close()

    async def test_copy_file_throws_enoent_for_missing_source(self):
        """Should throw ENOENT when source does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.copy_file("/nope.txt", "/out.txt")
            await db.close()

    async def test_copy_file_throws_enoent_for_missing_destination_parent(self):
        """Should throw ENOENT when destination parent does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/src3.txt", "content")
            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.copy_file("/src3.txt", "/missing/child.txt")
            await db.close()

    async def test_copy_file_throws_eisdir_for_directory_source(self):
        """Should throw EISDIR when source is a directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/asrcdir")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.copy_file("/asrcdir", "/out2.txt")
            await db.close()

    async def test_copy_file_throws_eisdir_for_directory_destination(self):
        """Should throw EISDIR when destination is a directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/src4.txt", "content")
            await fs.mkdir("/adstdir")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.copy_file("/src4.txt", "/adstdir")
            await db.close()

    async def test_copy_file_throws_einval_for_same_source_and_destination(self):
        """Should throw EINVAL when source and destination are the same"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/same.txt", "content")
            with pytest.raises(ErrnoException, match="EINVAL"):
                await fs.copy_file("/same.txt", "/same.txt")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemAccess:
    """Tests for access() operation"""

    async def test_access_existing_file(self):
        """Should resolve when a file exists"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/exists.txt", "content")
            # Should not raise
            await fs.access("/exists.txt")
            await db.close()

    async def test_access_existing_directory(self):
        """Should resolve when a directory exists"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.mkdir("/existsdir")
            # Should not raise
            await fs.access("/existsdir")
            await db.close()

    async def test_access_throws_enoent_for_nonexistent_path(self):
        """Should throw ENOENT when path does not exist"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            with pytest.raises(ErrnoException, match="ENOENT"):
                await fs.access("/does-not-exist")
            await db.close()


@pytest.mark.asyncio
class TestFilesystemErrorCodes:
    """Tests for error code validation on existing methods"""

    async def test_write_file_throws_eisdir_for_directory(self):
        """Should throw EISDIR when attempting to write to a directory path"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file.txt", "content")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.write_file("/dir", "nope")
            await db.close()

    async def test_write_file_throws_enotdir_for_file_in_path(self):
        """Should throw ENOTDIR when a parent path component is a file"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/a", "file-content")
            with pytest.raises(ErrnoException, match="ENOTDIR"):
                await fs.write_file("/a/b.txt", "child")
            await db.close()

    async def test_read_file_throws_eisdir_for_directory(self):
        """Should throw EISDIR when attempting to read a directory path"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/dir/file.txt", "content")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.read_file("/dir")
            await db.close()

    async def test_readdir_throws_enotdir_for_file(self):
        """Should throw ENOTDIR when attempting to readdir a file path"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/notadir.txt", "content")
            with pytest.raises(ErrnoException, match="ENOTDIR"):
                await fs.readdir("/notadir.txt")
            await db.close()

    async def test_unlink_throws_eisdir_for_directory(self):
        """Should throw EISDIR when attempting to unlink a directory"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            fs = await Filesystem.from_database(db)

            await fs.write_file("/adir/file.txt", "content")
            with pytest.raises(ErrnoException, match="EISDIR"):
                await fs.unlink("/adir")
            await db.close()




------------------------------------------
File: sdk/python/tests/test_kvstore.py
------------------------------------------

"""KvStore Integration Tests"""

import os
import tempfile

import pytest
from turso.aio import connect

from agentfs_sdk import KvStore


@pytest.mark.asyncio
class TestKvStoreBasicOperations:
    """Basic KvStore operations"""

    async def test_set_and_get_string(self):
        """Should set and get a string value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("test-key", "test-value")
            value = await kv.get("test-key")
            assert value == "test-value"
            await db.close()

    async def test_set_and_get_object(self):
        """Should set and get an object value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            test_object = {"name": "test", "count": 42, "nested": {"value": True}}
            await kv.set("object-key", test_object)
            value = await kv.get("object-key")
            assert value == test_object
            await db.close()

    async def test_set_and_get_number(self):
        """Should set and get a number value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("number-key", 12345)
            value = await kv.get("number-key")
            assert value == 12345
            await db.close()

    async def test_set_and_get_boolean(self):
        """Should set and get a boolean value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("bool-key", True)
            value = await kv.get("bool-key")
            assert value is True
            await db.close()

    async def test_set_and_get_array(self):
        """Should set and get an array value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            test_array = [1, 2, "three", {"four": 4}]
            await kv.set("array-key", test_array)
            value = await kv.get("array-key")
            assert value == test_array
            await db.close()

    async def test_set_and_list_values(self):
        """Should set and list values"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("g1:k1", 1)
            await kv.set("g1:k2", 2)
            await kv.set("g2:k1", 3)
            await kv.set("g2:k2", 4)

            result1 = await kv.list("g1:")
            assert result1 == [{"key": "g1:k1", "value": 1}, {"key": "g1:k2", "value": 2}]

            result2 = await kv.list("g1:k1")
            assert result2 == [{"key": "g1:k1", "value": 1}]

            result3 = await kv.list("g1:k3")
            assert result3 == []

            result4 = await kv.list("g2:")
            assert result4 == [{"key": "g2:k1", "value": 3}, {"key": "g2:k2", "value": 4}]

            await db.close()


@pytest.mark.asyncio
class TestKvStoreUpdateOperations:
    """KvStore update operations"""

    async def test_update_existing_value(self):
        """Should update an existing value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("update-key", "initial-value")
            await kv.set("update-key", "updated-value")
            value = await kv.get("update-key")
            assert value == "updated-value"
            await db.close()

    async def test_update_value_type(self):
        """Should update value type"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("type-key", "string-value")
            await kv.set("type-key", {"object": "value"})
            value = await kv.get("type-key")
            assert value == {"object": "value"}
            await db.close()


@pytest.mark.asyncio
class TestKvStoreDeleteOperations:
    """KvStore delete operations"""

    async def test_delete_existing_key(self):
        """Should delete an existing key"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("delete-key", "value-to-delete")
            await kv.delete("delete-key")
            value = await kv.get("delete-key")
            assert value is None
            await db.close()

    async def test_delete_nonexistent_key(self):
        """Should handle deleting non-existent key"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            # Should not throw an error when deleting a non-existent key
            await kv.delete("non-existent-key")
            await db.close()


@pytest.mark.asyncio
class TestKvStoreEdgeCases:
    """KvStore edge cases"""

    async def test_get_nonexistent_key(self):
        """Should return None for non-existent key"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            value = await kv.get("non-existent-key")
            assert value is None
            await db.close()

    async def test_get_with_default(self):
        """Should return default value for non-existent key"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            value = await kv.get("non-existent-key", default="default-value")
            assert value == "default-value"
            await db.close()

    async def test_handle_null_values(self):
        """Should handle null values"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("null-key", None)
            value = await kv.get("null-key")
            assert value is None
            await db.close()

    async def test_handle_empty_string(self):
        """Should handle empty string"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("empty-key", "")
            value = await kv.get("empty-key")
            assert value == ""
            await db.close()

    async def test_handle_zero_value(self):
        """Should handle zero value"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("zero-key", 0)
            value = await kv.get("zero-key")
            assert value == 0
            await db.close()

    async def test_keys_with_special_characters(self):
        """Should handle keys with special characters"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            special_key = "key:with/special.chars@123"
            await kv.set(special_key, "value")
            value = await kv.get(special_key)
            assert value == "value"
            await db.close()


@pytest.mark.asyncio
class TestKvStoreLargeData:
    """KvStore large data tests"""

    async def test_large_string_values(self):
        """Should handle large string values"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            large_string = "x" * 10000
            await kv.set("large-string", large_string)
            value = await kv.get("large-string")
            assert value == large_string
            await db.close()

    async def test_large_object_values(self):
        """Should handle large object values"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            large_object = {
                "items": [
                    {"id": i, "name": f"Item {i}", "data": f"Data for item {i}"}
                    for i in range(1000)
                ]
            }
            await kv.set("large-object", large_object)
            value = await kv.get("large-object")
            assert value == large_object
            await db.close()


@pytest.mark.asyncio
class TestKvStorePersistence:
    """KvStore persistence tests"""

    async def test_persist_across_instances(self):
        """Should persist data across KvStore instances"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            kv = await KvStore.from_database(db)

            await kv.set("persist-key", "persist-value")

            # Create new KvStore instance with same database
            new_kv = await KvStore.from_database(db)
            value = await new_kv.get("persist-key")
            assert value == "persist-value"

            await db.close()




------------------------------------------
File: sdk/python/tests/test_toolcalls.py
------------------------------------------

"""ToolCalls Integration Tests"""

import asyncio
import os
import tempfile
import time

import pytest
from turso.aio import connect

from agentfs_sdk import ToolCalls


@pytest.mark.asyncio
class TestToolCallsBasicOperations:
    """Basic ToolCalls operations"""

    async def test_start_tool_call_returns_id(self):
        """Should start a tool call and return an ID"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("test_tool", {"arg1": "value1"})
            assert call_id > 0
            await db.close()

    async def test_start_without_parameters(self):
        """Should start a tool call without parameters"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("simple_tool")
            assert call_id > 0

            tool_call = await tools.get(call_id)
            assert tool_call is not None
            assert tool_call.name == "simple_tool"
            assert tool_call.parameters is None
            assert tool_call.status == "pending"
            await db.close()

    async def test_mark_call_as_successful(self):
        """Should mark a tool call as successful"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("test_tool", {"input": "test"})
            await tools.success(call_id, {"output": "result"})

            tool_call = await tools.get(call_id)
            assert tool_call is not None
            assert tool_call.status == "success"
            assert tool_call.result == {"output": "result"}
            assert tool_call.completed_at is not None and tool_call.completed_at > 0
            assert tool_call.duration_ms is not None and tool_call.duration_ms >= 0
            await db.close()

    async def test_mark_successful_without_result(self):
        """Should mark a tool call as successful without result"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("test_tool", {"input": "test"})
            await tools.success(call_id)

            tool_call = await tools.get(call_id)
            assert tool_call is not None
            assert tool_call.status == "success"
            assert tool_call.result is None
            await db.close()

    async def test_mark_call_as_failed(self):
        """Should mark a tool call as failed"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("test_tool", {"input": "test"})
            await tools.error(call_id, "Something went wrong")

            tool_call = await tools.get(call_id)
            assert tool_call is not None
            assert tool_call.status == "error"
            assert tool_call.error == "Something went wrong"
            assert tool_call.completed_at is not None and tool_call.completed_at > 0
            assert tool_call.duration_ms is not None and tool_call.duration_ms >= 0
            await db.close()

    async def test_get_tool_call_by_id(self):
        """Should get a tool call by ID"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("test_tool", {"arg": "value"})
            tool_call = await tools.get(call_id)

            assert tool_call is not None
            assert tool_call.id == call_id
            assert tool_call.name == "test_tool"
            assert tool_call.parameters == {"arg": "value"}
            assert tool_call.status == "pending"
            assert tool_call.started_at > 0
            await db.close()

    async def test_get_nonexistent_id(self):
        """Should return None for non-existent ID"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            tool_call = await tools.get(99999)
            assert tool_call is None
            await db.close()


@pytest.mark.asyncio
class TestToolCallsQueryOperations:
    """ToolCalls query operations"""

    async def test_get_by_name(self):
        """Should get tool calls by name"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            await tools.start("tool_a", {"test": 1})
            await tools.start("tool_b", {"test": 2})
            await tools.start("tool_a", {"test": 3})

            results = await tools.get_by_name("tool_a")
            assert len(results) == 2
            assert all(tc.name == "tool_a" for tc in results)
            await db.close()

    async def test_limit_results_by_name(self):
        """Should limit results when querying by name"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            await tools.start("tool_a", {"test": 1})
            await tools.start("tool_a", {"test": 2})
            await tools.start("tool_a", {"test": 3})

            results = await tools.get_by_name("tool_a", limit=2)
            assert len(results) == 2
            await db.close()

    async def test_get_recent_calls(self):
        """Should get recent tool calls"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            await tools.start("tool_old")
            # Wait to ensure different timestamps
            await asyncio.sleep(1.1)
            midpoint = int(time.time())
            await asyncio.sleep(1.1)
            await tools.start("tool_new")

            results = await tools.get_recent(midpoint)
            assert len(results) >= 1
            assert all(tc.started_at >= midpoint for tc in results)
            await db.close()

    async def test_limit_recent_calls(self):
        """Should limit recent tool calls"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            now = int(time.time())

            await tools.start("tool_1")
            await tools.start("tool_2")
            await tools.start("tool_3")

            results = await tools.get_recent(now - 10, limit=2)
            assert len(results) <= 2
            await db.close()

    async def test_empty_results_for_nonexistent_name(self):
        """Should return empty array when no matching tool calls by name"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            results = await tools.get_by_name("non_existent_tool")
            assert results == []
            await db.close()


@pytest.mark.asyncio
class TestToolCallsStatistics:
    """ToolCalls statistics"""

    async def test_calculate_statistics(self):
        """Should calculate tool call statistics"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            # Create some tool calls
            id1 = await tools.start("tool_a", {"test": 1})
            await tools.success(id1, {"result": "ok"})

            id2 = await tools.start("tool_a", {"test": 2})
            await tools.error(id2, "failed")

            id3 = await tools.start("tool_a", {"test": 3})
            await tools.success(id3, {"result": "ok"})

            id4 = await tools.start("tool_b", {"test": 4})
            await tools.success(id4, {"result": "ok"})

            stats = await tools.get_stats()

            assert len(stats) == 2

            tool_a_stats = next((s for s in stats if s.name == "tool_a"), None)
            assert tool_a_stats is not None
            assert tool_a_stats.total_calls == 3
            assert tool_a_stats.successful == 2
            assert tool_a_stats.failed == 1
            assert tool_a_stats.avg_duration_ms >= 0

            tool_b_stats = next((s for s in stats if s.name == "tool_b"), None)
            assert tool_b_stats is not None
            assert tool_b_stats.total_calls == 1
            assert tool_b_stats.successful == 1
            assert tool_b_stats.failed == 0
            await db.close()

    async def test_exclude_pending_from_stats(self):
        """Should exclude pending calls from statistics"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            id1 = await tools.start("tool_a", {"test": 1})
            await tools.success(id1, {"result": "ok"})

            # This one stays pending
            await tools.start("tool_a", {"test": 2})

            stats = await tools.get_stats()
            tool_a_stats = next((s for s in stats if s.name == "tool_a"), None)

            assert tool_a_stats is not None
            assert tool_a_stats.total_calls == 1  # Only completed calls
            await db.close()

    async def test_empty_stats_no_completed_calls(self):
        """Should return empty array when no completed calls"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            await tools.start("tool_a", {"test": 1})
            stats = await tools.get_stats()
            assert stats == []
            await db.close()


@pytest.mark.asyncio
class TestToolCallsComplexData:
    """ToolCalls complex parameters and results"""

    async def test_complex_nested_parameters(self):
        """Should handle complex nested parameters"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            complex_params = {
                "user": {"id": 123, "name": "Test User"},
                "options": {"timeout": 5000, "retry": True},
                "data": [1, 2, 3, 4, 5],
            }

            call_id = await tools.start("complex_tool", complex_params)
            tool_call = await tools.get(call_id)

            assert tool_call is not None
            assert tool_call.parameters == complex_params
            await db.close()

    async def test_complex_nested_results(self):
        """Should handle complex nested results"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            complex_result = {
                "data": {"items": [{"id": 1, "value": "a"}, {"id": 2, "value": "b"}]},
                "metadata": {"count": 2, "hasMore": False},
            }

            call_id = await tools.start("complex_tool")
            await tools.success(call_id, complex_result)
            tool_call = await tools.get(call_id)

            assert tool_call is not None
            assert tool_call.result == complex_result
            await db.close()

    async def test_large_parameters(self):
        """Should handle large parameters"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            large_params = {"items": [{"id": i, "data": f"Data for item {i}"} for i in range(100)]}

            call_id = await tools.start("large_tool", large_params)
            tool_call = await tools.get(call_id)

            assert tool_call is not None
            assert tool_call.parameters == large_params
            await db.close()


@pytest.mark.asyncio
class TestToolCallsPersistence:
    """ToolCalls persistence"""

    async def test_persist_across_instances(self):
        """Should persist tool calls across instances"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            call_id = await tools.start("persist_tool", {"test": "value"})
            await tools.success(call_id, {"result": "ok"})

            # Create new ToolCalls instance with same database
            new_tools = await ToolCalls.from_database(db)
            tool_call = await new_tools.get(call_id)

            assert tool_call is not None
            assert tool_call.name == "persist_tool"
            assert tool_call.status == "success"
            await db.close()


@pytest.mark.asyncio
class TestToolCallsOrdering:
    """ToolCalls ordering"""

    async def test_order_by_started_at_desc(self):
        """Should return tool calls ordered by started_at desc"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            await tools.start("tool_first")
            await asyncio.sleep(0.1)
            await tools.start("tool_second")
            await asyncio.sleep(0.1)
            await tools.start("tool_third")

            recent = await tools.get_recent(0)

            assert len(recent) >= 3
            # Most recent first
            for i in range(len(recent) - 1):
                assert recent[i].started_at >= recent[i + 1].started_at
            await db.close()


@pytest.mark.asyncio
class TestToolCallsRecord:
    """ToolCalls record method"""

    async def test_record_completed_call(self):
        """Should record a completed tool call"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            start = int(time.time())
            end = start + 1

            call_id = await tools.record(
                "test_tool",
                start,
                end,
                parameters={"param": "value"},
                result={"result": "success"},
            )

            assert call_id > 0

            # Get the tool call
            call = await tools.get(call_id)
            assert call is not None
            assert call.name == "test_tool"
            assert call.parameters == {"param": "value"}
            assert call.result == {"result": "success"}
            assert call.status == "success"
            await db.close()

    async def test_record_failed_call(self):
        """Should record a failed tool call"""
        with tempfile.TemporaryDirectory() as tmpdir:
            db_path = os.path.join(tmpdir, "test.db")
            db = await connect(db_path)
            await db.execute("PRAGMA unstable_capture_data_changes_conn('full')")
            tools = await ToolCalls.from_database(db)

            start = int(time.time())
            end = start + 1

            call_id = await tools.record(
                "test_tool", start, end, parameters={"param": "value"}, error="Failed"
            )

            assert call_id > 0

            # Get the tool call
            call = await tools.get(call_id)
            assert call is not None
            assert call.name == "test_tool"
            assert call.error == "Failed"
            assert call.status == "error"
            await db.close()




------------------------------------------
File: SPEC.md
------------------------------------------

# Agent Filesystem Specification

**Version:** 0.4

## Introduction

The Agent Filesystem Specification defines a SQLite schema for representing agent filesystem state. The specification consists of three main components:

1. **Tool Call Audit Trail**: Captures tool invocations, parameters, and results for debugging, auditing, and performance analysis
2. **Virtual Filesystem**: Stores agent artifacts (files, documents, outputs) using a Unix-like inode design with support for hard links, proper metadata, and efficient file operations
3. **Key-Value Store**: Provides simple get/set operations for agent context, preferences, and structured state that doesn't fit into the filesystem model

All timestamps in this specification use Unix epoch format (seconds since 1970-01-01 00:00:00 UTC) with optional nanosecond precision via separate `_nsec` columns.

## Tool Calls

The tool call tracking schema captures tool invocations for debugging, auditing, and analysis.

### Schema

#### Table: `tool_calls`

Stores individual tool invocations with parameters and results. This is an insert-only audit log.

```sql
CREATE TABLE tool_calls (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  name TEXT NOT NULL,
  parameters TEXT,
  result TEXT,
  error TEXT,
  started_at INTEGER NOT NULL,
  completed_at INTEGER NOT NULL,
  duration_ms INTEGER NOT NULL
)

CREATE INDEX idx_tool_calls_name ON tool_calls(name)
CREATE INDEX idx_tool_calls_started_at ON tool_calls(started_at)
```

**Fields:**

- `id` - Unique tool call identifier
- `name` - Tool name (e.g., 'read_file', 'web_search', 'execute_code')
- `parameters` - JSON-serialized input parameters (NULL if no parameters)
- `result` - JSON-serialized result (NULL if error)
- `error` - Error message (NULL if success)
- `started_at` - Invocation timestamp (Unix timestamp, seconds)
- `completed_at` - Completion timestamp (Unix timestamp, seconds)
- `duration_ms` - Execution duration in milliseconds

### Operations

#### Record Tool Call

```sql
INSERT INTO tool_calls (name, parameters, result, error, started_at, completed_at, duration_ms)
VALUES (?, ?, ?, ?, ?, ?, ?)
```

**Note:** Insert once when the tool call completes. Either `result` or `error` should be set, not both.

#### Query Tool Calls by Name

```sql
SELECT * FROM tool_calls
WHERE name = ?
ORDER BY started_at DESC
```

#### Query Recent Tool Calls

```sql
SELECT * FROM tool_calls
WHERE started_at > ?
ORDER BY started_at DESC
```

#### Analyze Tool Performance

```sql
SELECT
  name,
  COUNT(*) as total_calls,
  SUM(CASE WHEN error IS NULL THEN 1 ELSE 0 END) as successful,
  SUM(CASE WHEN error IS NOT NULL THEN 1 ELSE 0 END) as failed,
  AVG(duration_ms) as avg_duration_ms
FROM tool_calls
GROUP BY name
ORDER BY total_calls DESC
```

### Consistency Rules

1. Exactly one of `result` or `error` SHOULD be non-NULL (mutual exclusion)
2. `completed_at` MUST always be set (no NULL values)
3. `duration_ms` MUST always be set and equal to `(completed_at - started_at) * 1000`
4. Parameters and results MUST be valid JSON strings when present
5. Records MUST NOT be updated or deleted (insert-only audit log)

### Implementation Notes

- This is an insert-only audit log - no updates or deletes
- Insert the record once when the tool call completes
- Set either `result` (on success) or `error` (on failure), but not both
- `parameters`, `result`, and `error` are stored as JSON-serialized strings
- `duration_ms` should be computed as `(completed_at - started_at) * 1000`
- Use indexes for efficient queries by name or time
- Consider periodic archival of old tool call records to a separate table

### Extension Points

Implementations MAY extend the tool call schema with additional functionality:

- Session/conversation grouping (add `session_id` field)
- User attribution (add `user_id` field)
- Cost tracking (add `cost` field for API calls)
- Parent/child relationships for nested tool calls
- Token usage tracking
- Input/output size metrics

Such extensions SHOULD use separate tables to maintain referential integrity.

## Virtual Filesystem

The virtual filesystem provides POSIX-like file operations for agent artifacts. The filesystem separates namespace (paths and names) from data (file content and metadata) using a Unix-like inode design. This enables hard links (multiple paths to the same file), efficient file operations, proper file metadata (permissions, timestamps), and chunked content storage.

### Schema

#### Table: `fs_config`

Stores filesystem-level configuration. This table is initialized once when the filesystem is created and MUST NOT be modified afterward.

```sql
CREATE TABLE fs_config (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL
)
```

**Fields:**

- `key` - Configuration key
- `value` - Configuration value (stored as text)

**Required Configuration:**

| Key | Description | Default |
|-----|-------------|---------|
| `chunk_size` | Size of data chunks in bytes | `4096` |

**Notes:**

- `chunk_size` determines the fixed size of data chunks in `fs_data`
- All chunks except the last chunk of a file are exactly `chunk_size` bytes
- Configuration is immutable after filesystem initialization
- Implementations MAY define additional configuration keys

#### Table: `fs_inode`

Stores file and directory metadata.

```sql
CREATE TABLE fs_inode (
  ino INTEGER PRIMARY KEY AUTOINCREMENT,
  mode INTEGER NOT NULL,
  nlink INTEGER NOT NULL DEFAULT 0,
  uid INTEGER NOT NULL DEFAULT 0,
  gid INTEGER NOT NULL DEFAULT 0,
  size INTEGER NOT NULL DEFAULT 0,
  atime INTEGER NOT NULL,
  mtime INTEGER NOT NULL,
  ctime INTEGER NOT NULL,
  rdev INTEGER NOT NULL DEFAULT 0,
  atime_nsec INTEGER NOT NULL DEFAULT 0,
  mtime_nsec INTEGER NOT NULL DEFAULT 0,
  ctime_nsec INTEGER NOT NULL DEFAULT 0
)
```

**Fields:**

- `ino` - Inode number (unique identifier)
- `mode` - File type and permissions (Unix mode bits)
- `nlink` - Number of hard links pointing to this inode
- `uid` - Owner user ID
- `gid` - Owner group ID
- `size` - Total file size in bytes
- `atime` - Last access time (Unix timestamp, seconds)
- `mtime` - Last modification time (Unix timestamp, seconds)
- `ctime` - Creation/change time (Unix timestamp, seconds)
- `rdev` - Device number for character and block devices (major/minor encoded)
- `atime_nsec` - Nanosecond component of last access time (0–999999999)
- `mtime_nsec` - Nanosecond component of last modification time (0–999999999)
- `ctime_nsec` - Nanosecond component of creation/change time (0–999999999)

**Mode Encoding:**

The `mode` field combines file type and permissions:

```
File type (upper bits):
  0o170000 - File type mask (S_IFMT)
  0o100000 - Regular file (S_IFREG)
  0o040000 - Directory (S_IFDIR)
  0o120000 - Symbolic link (S_IFLNK)
  0o010000 - FIFO/named pipe (S_IFIFO)
  0o020000 - Character device (S_IFCHR)
  0o060000 - Block device (S_IFBLK)
  0o140000 - Socket (S_IFSOCK)

Permissions (lower 12 bits):
  0o000777 - Permission bits (rwxrwxrwx)

Example:
  0o100644 - Regular file, rw-r--r--
  0o040755 - Directory, rwxr-xr-x
```

**Special Inodes:**

- Inode 1 MUST be the root directory

#### Table: `fs_dentry`

Maps names to inodes (directory entries).

```sql
CREATE TABLE fs_dentry (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  name TEXT NOT NULL,
  parent_ino INTEGER NOT NULL,
  ino INTEGER NOT NULL,
  UNIQUE(parent_ino, name)
)

CREATE INDEX idx_fs_dentry_parent ON fs_dentry(parent_ino, name)
```

**Fields:**

- `id` - Internal entry ID
- `name` - Basename (filename or directory name)
- `parent_ino` - Parent directory inode number
- `ino` - Inode this entry points to

**Constraints:**

- `UNIQUE(parent_ino, name)` - No duplicate names in a directory

**Notes:**

- Root directory (ino=1) has no dentry (no parent)
- Multiple dentries MAY point to the same inode (hard links)
- Link count is stored in `fs_inode.nlink` and must be incremented/decremented when dentries are added/removed

#### Table: `fs_data`

Stores file content in fixed-size chunks. Chunk size is configured at filesystem level via `fs_config`.

```sql
CREATE TABLE fs_data (
  ino INTEGER NOT NULL,
  chunk_index INTEGER NOT NULL,
  data BLOB NOT NULL,
  PRIMARY KEY (ino, chunk_index)
)
```

**Fields:**

- `ino` - Inode number
- `chunk_index` - Zero-based chunk index (chunk 0 contains bytes 0 to chunk_size-1)
- `data` - Binary content (BLOB), exactly `chunk_size` bytes except for the last chunk

**Notes:**

- Directories MUST NOT have data chunks
- Chunk size is determined by the `chunk_size` value in `fs_config`
- All chunks except the last chunk of a file MUST be exactly `chunk_size` bytes
- The last chunk MAY be smaller than `chunk_size`
- Byte offset for a chunk = `chunk_index * chunk_size`
- To read at byte offset `N`: `chunk_index = N / chunk_size`, `offset_in_chunk = N % chunk_size`

#### Table: `fs_symlink`

Stores symbolic link targets.

```sql
CREATE TABLE fs_symlink (
  ino INTEGER PRIMARY KEY,
  target TEXT NOT NULL
)
```

**Fields:**

- `ino` - Inode number of the symlink
- `target` - Target path (may be absolute or relative)

### Operations

#### Path Resolution

To resolve a path to an inode:

1. Start at root inode (ino=1)
2. Split path by `/` and filter empty components
3. For each component:
   ```sql
   SELECT ino FROM fs_dentry WHERE parent_ino = ? AND name = ?
   ```
4. Return final inode or NULL if any component not found

#### Creating a File

1. Resolve parent directory path to inode
2. Get chunk size from config:
   ```sql
   SELECT value FROM fs_config WHERE key = 'chunk_size'
   ```
3. Insert inode:
   ```sql
   INSERT INTO fs_inode (mode, uid, gid, size, atime, mtime, ctime)
   VALUES (?, ?, ?, 0, ?, ?, ?)
   RETURNING ino
   ```
4. Insert directory entry:
   ```sql
   INSERT INTO fs_dentry (name, parent_ino, ino)
   VALUES (?, ?, ?)
   ```
5. Increment link count:
   ```sql
   UPDATE fs_inode SET nlink = nlink + 1 WHERE ino = ?
   ```
6. Split data into chunks and insert each:
   ```sql
   INSERT INTO fs_data (ino, chunk_index, data)
   VALUES (?, ?, ?)
   ```
   Where `chunk_index` starts at 0 and increments for each chunk.
7. Update inode size:
   ```sql
   UPDATE fs_inode SET size = ?, mtime = ? WHERE ino = ?
   ```

#### Reading a File

1. Resolve path to inode
2. Fetch all chunks in order:
   ```sql
   SELECT data FROM fs_data WHERE ino = ? ORDER BY chunk_index ASC
   ```
3. Concatenate chunks in order
4. Update access time:
   ```sql
   UPDATE fs_inode SET atime = ? WHERE ino = ?
   ```

#### Reading a File at Offset

To read `length` bytes starting at byte offset `offset`:

1. Resolve path to inode
2. Get chunk size from config:
   ```sql
   SELECT value FROM fs_config WHERE key = 'chunk_size'
   ```
3. Calculate chunk range:
   - `start_chunk = offset / chunk_size`
   - `end_chunk = (offset + length - 1) / chunk_size`
4. Fetch required chunks:
   ```sql
   SELECT chunk_index, data FROM fs_data
   WHERE ino = ? AND chunk_index >= ? AND chunk_index <= ?
   ORDER BY chunk_index ASC
   ```
5. Extract the requested byte range from the chunks:
   - `offset_in_first_chunk = offset % chunk_size`
   - Skip first `offset_in_first_chunk` bytes of first chunk
   - Take `length` total bytes across chunks

#### Listing a Directory

1. Resolve directory path to inode
2. Query entries:
   ```sql
   SELECT name FROM fs_dentry WHERE parent_ino = ? ORDER BY name ASC
   ```

#### Deleting a File

1. Resolve path to get inode and parent
2. Delete directory entry:
   ```sql
   DELETE FROM fs_dentry WHERE parent_ino = ? AND name = ?
   ```
3. Decrement link count:
   ```sql
   UPDATE fs_inode SET nlink = nlink - 1 WHERE ino = ?
   ```
4. Check if last link:
   ```sql
   SELECT nlink FROM fs_inode WHERE ino = ?
   ```
5. If nlink = 0, delete inode and data:
   ```sql
   DELETE FROM fs_inode WHERE ino = ?
   DELETE FROM fs_data WHERE ino = ?
   ```

#### Creating a Hard Link

1. Resolve source path to get inode
2. Resolve destination parent to get parent_ino
3. Insert new directory entry:
   ```sql
   INSERT INTO fs_dentry (name, parent_ino, ino)
   VALUES (?, ?, ?)
   ```
4. Increment link count:
   ```sql
   UPDATE fs_inode SET nlink = nlink + 1 WHERE ino = ?
   ```

#### Reading File Metadata (stat)

1. Resolve path to inode
2. Query inode (includes link count):
   ```sql
   SELECT ino, mode, nlink, uid, gid, size, atime, mtime, ctime, rdev,
          atime_nsec, mtime_nsec, ctime_nsec
   FROM fs_inode WHERE ino = ?
   ```

### Initialization

When creating a new agent database, initialize the filesystem configuration and root directory:

```sql
-- Initialize filesystem configuration
INSERT INTO fs_config (key, value) VALUES ('chunk_size', '4096');

-- Initialize root directory
INSERT INTO fs_inode (ino, mode, nlink, uid, gid, size, atime, mtime, ctime)
VALUES (1, 16877, 1, 0, 0, 0, unixepoch(), unixepoch(), unixepoch());
```

Where `16877` = `0o040755` (directory with rwxr-xr-x permissions)

**Note:** The `chunk_size` value can be customized at filesystem creation time but MUST NOT be changed afterward. The root directory has `nlink=1` as it has no parent directory entry.

### Consistency Rules

1. Root inode (ino=1) MUST always exist
2. Every dentry MUST reference a valid inode
3. Every dentry MUST reference a valid parent inode
4. No directory MAY contain duplicate names
5. Directories MUST have mode with S_IFDIR bit set
6. Regular files MUST have mode with S_IFREG bit set
7. File size MUST match total size of all data chunks
8. Every inode MUST have at least one dentry (except root)

### Implementation Notes

- Use `RETURNING` clause to safely get auto-generated inode numbers
- Parent directories are created implicitly as needed
- Empty files have an inode but no data chunks
- Symlink resolution is implementation-defined (not part of schema)
- Use transactions for multi-step operations to maintain consistency

### Extension Points

Implementations MAY extend the filesystem schema with additional functionality:

- Extended attributes table
- File ACLs and advanced permissions
- Quota tracking per user/group
- Version history and snapshots
- Content deduplication
- Compression metadata
- File checksums/hashes

Such extensions SHOULD use separate tables to maintain referential integrity.

## Overlay Filesystem

The overlay filesystem provides copy-on-write semantics by layering a writable delta filesystem on top of a read-only base filesystem. Changes are written to the delta layer while the base layer remains unmodified. This enables sandboxed execution where modifications can be discarded or committed independently.

### Whiteouts

When a file is deleted from an overlay filesystem, the deletion must be recorded so that lookups do not fall through to the base layer. This is accomplished using "whiteouts" - markers that indicate a path has been explicitly deleted.

#### Table: `fs_whiteout`

Tracks deleted paths in the overlay to prevent base layer visibility.

```sql
CREATE TABLE fs_whiteout (
  path TEXT PRIMARY KEY,
  parent_path TEXT NOT NULL,
  created_at INTEGER NOT NULL
)

CREATE INDEX idx_fs_whiteout_parent ON fs_whiteout(parent_path)
```

**Fields:**

- `path` - Normalized absolute path that has been deleted
- `parent_path` - Parent directory path (for efficient child lookups)
- `created_at` - Deletion timestamp (Unix timestamp, seconds)

**Notes:**

- The `parent_path` column enables O(1) lookups of whiteouts within a directory, avoiding expensive `LIKE` pattern matching
- For the root directory `/`, `parent_path` is `/`
- For other paths, `parent_path` is the path with the final component removed (e.g., `/foo/bar` has parent `/foo`)

### Operations

#### Create Whiteout

When deleting a file that exists in the base layer:

```sql
INSERT INTO fs_whiteout (path, parent_path, created_at)
VALUES (?, ?, ?)
ON CONFLICT(path) DO UPDATE SET created_at = excluded.created_at
```

#### Check for Whiteout

Before falling through to the base layer during lookup:

```sql
SELECT 1 FROM fs_whiteout WHERE path = ?
```

#### Remove Whiteout

When creating a file at a previously deleted path:

```sql
DELETE FROM fs_whiteout WHERE path = ?
```

#### List Child Whiteouts

When listing a directory, get whiteouts to exclude from base layer results:

```sql
SELECT path FROM fs_whiteout WHERE parent_path = ?
```

### Overlay Lookup Semantics

1. Check if path exists in delta layer → return delta entry
2. Check if path has a whiteout → return "not found"
3. Check if path exists in base layer → return base entry
4. Return "not found"

### Inode Origin Tracking

When a file is copied from the base layer to the delta layer during a copy-up operation (e.g., when creating a hard link to a base file), the original base inode number must be preserved. This is necessary because the kernel caches inode numbers, and returning a different inode after copy-up causes ENOENT errors or cache inconsistencies.

This mechanism is similar to Linux overlayfs's `trusted.overlay.origin` extended attribute, which stores a file handle to the lower inode.

#### Table: `fs_origin`

Maps delta layer inodes to their original base layer inodes.

```sql
CREATE TABLE fs_origin (
  delta_ino INTEGER PRIMARY KEY,
  base_ino INTEGER NOT NULL
)
```

**Fields:**

- `delta_ino` - Inode number in the delta layer
- `base_ino` - Original inode number from the base layer

#### Operations

##### Store Origin Mapping

When copying a file from base to delta during copy-up:

```sql
INSERT OR REPLACE INTO fs_origin (delta_ino, base_ino)
VALUES (?, ?)
```

##### Get Origin Inode

When stat'ing a file that exists in delta, check if it has an origin:

```sql
SELECT base_ino FROM fs_origin WHERE delta_ino = ?
```

If a mapping exists, return `base_ino` instead of `delta_ino` in stat results.

### Consistency Rules

1. A whiteout MUST be removed when a new file is created at that path
2. A whiteout MUST be created when deleting a file that exists in the base layer
3. The `parent_path` MUST be correctly derived from `path`
4. Whiteouts only affect overlay lookups, not the underlying base filesystem
5. When copying a file from base to delta, the origin mapping MUST be stored
6. When stat'ing a delta file with an origin mapping, the base inode MUST be returned

## Key-Value Data

The key-value store provides simple get/set operations for agent context and state.

### Schema

#### Table: `kv_store`

Stores arbitrary key-value pairs with automatic timestamping.

```sql
CREATE TABLE kv_store (
  key TEXT PRIMARY KEY,
  value TEXT NOT NULL,
  created_at INTEGER DEFAULT (unixepoch()),
  updated_at INTEGER DEFAULT (unixepoch())
)

CREATE INDEX idx_kv_store_created_at ON kv_store(created_at)
```

**Fields:**

- `key` - Unique key identifier
- `value` - JSON-serialized value
- `created_at` - Creation timestamp (Unix timestamp, seconds)
- `updated_at` - Last update timestamp (Unix timestamp, seconds)

### Operations

#### Set a Value

```sql
INSERT INTO kv_store (key, value, updated_at)
VALUES (?, ?, unixepoch())
ON CONFLICT(key) DO UPDATE SET
  value = excluded.value,
  updated_at = unixepoch()
```

#### Get a Value

```sql
SELECT value FROM kv_store WHERE key = ?
```

#### Delete a Value

```sql
DELETE FROM kv_store WHERE key = ?
```

#### List All Keys

```sql
SELECT key, created_at, updated_at FROM kv_store ORDER BY key ASC
```

### Consistency Rules

1. Keys MUST be unique (enforced by PRIMARY KEY)
2. Values MUST be valid JSON strings
3. Timestamps MUST use Unix epoch format (seconds)

### Implementation Notes

- Values are stored as JSON strings; serialize before storing, deserialize after retrieving
- Use `ON CONFLICT` clause for upsert operations
- Indexes on `created_at` support temporal queries
- Updates automatically refresh the `updated_at` timestamp
- Keys can use any naming convention (e.g., namespaced: `user:preferences`, `session:state`)

### Extension Points

Implementations MAY extend the key-value store schema with additional functionality:

- Namespaced keys with hierarchy support
- Value versioning/history
- TTL (time-to-live) for automatic expiration
- Value size limits and quotas

Such extensions SHOULD use separate tables to maintain referential integrity.

## Revision History

### Version 0.4

- Added nanosecond timestamp precision for `atime`, `mtime`, and `ctime`
- Added `atime_nsec`, `mtime_nsec`, `ctime_nsec` columns to `fs_inode` table (DEFAULT 0 for backward compatibility)
- Nanosecond precision enables correct NFS `wcc_data` cache invalidation when multiple operations occur within the same second
- Added POSIX special file support (FIFOs, character devices, block devices, sockets)
- Added `rdev` column to `fs_inode` table for device major/minor numbers
- Added `S_IFIFO`, `S_IFCHR`, `S_IFBLK`, `S_IFSOCK` file type constants to Mode Encoding
- Updated stat query to include `rdev` field

### Version 0.3

- Added `fs_origin` table to Overlay Filesystem for tracking copy-up origin inodes
- Origin tracking ensures consistent inode numbers after copy-up (similar to Linux overlayfs `trusted.overlay.origin`)

### Version 0.2

- Added Overlay Filesystem section with `fs_whiteout` table for copy-on-write semantics
- Whiteout table includes `parent_path` column with index for efficient O(1) child lookups
- Added `nlink` column to `fs_inode` table to store link count directly
- Link count is now maintained in the inode rather than computed via COUNT(*) on `fs_dentry`

### Version 0.1

- Added `fs_config` table for filesystem-level configuration
- Changed `fs_data` table to use fixed-size chunks with `chunk_index` instead of variable-size chunks with `offset` and `size`
- Added `chunk_size` configuration option (default: 4096 bytes)
- Added "Reading a File at Offset" operation for efficient partial reads
- Chunk-based storage enables efficient random access reads without loading entire files

### Version 0.0

- Initial specification
- Tool call audit trail (`tool_calls` table)
- Virtual filesystem (`fs_inode`, `fs_dentry`, `fs_data`, `fs_symlink` tables)
- Key-value store (`kv_store` table)
